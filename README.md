{
  "query": "How have organizations like the IEEE, UNESCO, and the European Commission influenced the development of AI ethics guidelines, particularly regarding facial recognition systems in China, the U.S., and the EU?",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 20754,
      "chunk": "Ethics\nThis section is an excerpt from Ethics of artificial intelligence.[edit]"
    },
    {
      "id": 20755,
      "chunk": "The ethics of artificial intelligence covers a broad range of topics within AI that are considered to have particular ethical stakes.[147] This includes algorithmic biases, fairness,[148] automated decision-making,[149] accountability, privacy, and regulation. It also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment,"
    },
    {
      "id": 20871,
      "chunk": "Krištofík, Andrej (28 April 2025). \"Bias in AI (Supported) Decision Making: Old Problems, New Technologies\". International Journal for Court Administration. 16 (1). doi:10.36745/ijca.598. ISSN 2156-7964.\nGarcia, Megan (2016). \"Racist in the Machine\". World Policy Journal. 33 (4): 111–117. doi:10.1215/07402775-3813015. ISSN 0740-2775. S2CID 151595343.\nBostrom, Nick (2011). \"The Ethics of Artificial Intelligence\" (PDF). Archived from the original (PDF) on 4 March 2016. Retrieved 11 April 2016."
    },
    {
      "chunk": "\"A Google DeepMind Algorithm Uses Deep Learning and More to Master the Game of Go | MIT Technology Review\". MIT Technology Review. Archived from the original on 1 February 2016. Retrieved 30 January 2016.\nMetz, Cade (6 November 2017). \"A.I. Researchers Leave Elon Musk Lab to Begin Robotics Start-Up\". The New York Times. Archived from the original on 7 July 2019. Retrieved 5 July 2019."
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\"A Deep Dive Into the Transformer Architecture – The Development of Transformer Models\". KDnuggets. 2020-08-24. Retrieved 2025-06-29.\nAllamar, Jay. \"Illustrated transformer\". Archived from the original on 2023-07-25. Retrieved 2023-07-29.\nAllamar, Jay. \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\". Retrieved 2023-08-01.\n\"Our next-generation model: Gemini 1.5\". Google. 15 February 2024. Archived from the original on 18 February 2024. Retrieved 18 February 2024."
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\"ChatGPT is more like an 'alien intelligence' than a human brain, says futurist\". ZDNET. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\nNewport, Cal (13 April 2023). \"What Kind of Mind Does ChatGPT Have?\". The New Yorker. Archived from the original on 12 June 2023. Retrieved 12 June 2023."
    },
    {
      "chunk": "\"Data Augmentation - deeplearning.ai | Coursera\". Coursera. Archived from the original on 1 December 2017. Retrieved 30 November 2017.\nHinton, G. E. (2010). \"A Practical Guide to Training Restricted Boltzmann Machines\". Tech. Rep. UTML TR 2010-003. Archived from the original on 2021-05-09. Retrieved 2017-06-13."
    },
    {
      "chunk": "\"137 emergent abilities of large language models\". Jason Wei. Retrieved 2023-06-24.\nBowman, Samuel R. (2023). \"Eight Things to Know about Large Language Models\". arXiv:2304.00612 [cs.CL].\nMukherjee, Anirban; Chang, Hannah (2024). \"Heuristic Reasoning in AI: Instrumental Use and Mimetic Absorption\". arXiv:2403.09404 [cs.AI].\nHahn, Michael; Goyal, Navin (2023-03-14). \"A Theory of Emergent In-Context Learning as Implicit Structure Induction\". arXiv:2303.07971 [cs.LG]."
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    },
    {
      "chunk": "\"Adversarial Machine Learning – CLTC UC Berkeley Center for Long-Term Cybersecurity\". CLTC. Archived from the original on 17 May 2022. Retrieved 25 May 2022.\n\"Machine-learning models vulnerable to undetectable backdoors\". The Register. Archived from the original on 13 May 2022. Retrieved 13 May 2022.\n\"Undetectable Backdoors Plantable In Any Machine-Learning Algorithm\". IEEE Spectrum. 10 May 2022. Archived from the original on 11 May 2022. Retrieved 13 May 2022."
    }
  ]
}

{
  "query": "\"Compare the performance of ResNet-50, EfficientNet, and Vision Transformers when trained using TensorFlow on NVIDIA A100 GPUs versus Apple M1 chips for image classification tasks on ImageNet and CIFAR-10.\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 5710,
      "chunk": "resnet_block(256, 2),\nresnet_block(512, 2))\nnet.add(nn.GlobalAvgPool2D(), nn.Dense(num_classes))\nreturn net\nThe CIFAR-10 image classification challenge uses 10 categories. We will perform Xavier random\ninitialization on the model before training begins.\ndef get_net(devices):\nnum_classes = 10\nnet = resnet18(num_classes)\n(continues on next page)\n618\nChapter 13. Computer Vision"
    },
    {
      "id": 2982,
      "chunk": "atively small minibatches of data to be processed. At the same time, small batches limit the\nefficiency of GPUs. Hence, training on 1024 GPUs with a minibatch size of, say 32 images\nper batch amounts to an aggregate minibatch of 32k images. Recent work, first by Li (Li,\n2017), and subsequently by (You et al., 2017) and (Jia et al., 2018) pushed the size up to 64k\nobservations, reducing training time for ResNet50 on ImageNet to less than 7 minutes. For"
    },
    {
      "id": 4138,
      "chunk": "7.1.3 Reading the Dataset\nAlthough AlexNet is trained on ImageNet in the paper, we use Fashion-MNIST here since training\nan ImageNet model to convergence could take hours or days even on a modern GPU. One of the\nproblems with applying AlexNet directly on Fashion-MNIST is that its images have lower resolu-\ntion (28 × 28 pixels) than ImageNet images. To make things work, we upsample them to 224 × 224\n(generally not a smart practice, but we do it here to be faithful to the AlexNet architecture). We"
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

{
  "query": "\"How does the ReAct prompting framework improve the reasoning capabilities of LLMs like GPT-4, Claude 3, and Gemini 1.5 when integrated with external tools such as Wolfram Alpha and Python REPLs in the context of academic research from Stanford and DeepMind?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 21477,
      "chunk": "Yao, Shunyu; Zhao, Jeffrey; Yu, Dian; Du, Nan; Shafran, Izhak; Narasimhan, Karthik; Cao, Yuan (2022-10-01). \"ReAct: Synergizing Reasoning and Acting in Language Models\". arXiv:2210.03629 [cs.CL].\nWang, Zihao; Cai, Shaofei; Liu, Anji; Ma, Xiaojian; Liang, Yitao (2023-02-03). \"Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents\". arXiv:2302.01560 [cs.AI]."
    },
    {
      "id": 21335,
      "chunk": "The ReAct pattern, a portmanteau of \"Reason + Act\", constructs an agent out of an LLM, using the LLM as a planner. The LLM is prompted to \"think out loud\". Specifically, the language model is prompted with a textual description of the environment, a goal, a list of possible actions, and a record of the actions and observations so far. It generates one or more thoughts before generating an action, which is then executed in the environment.[73]"
    },
    {
      "id": 21268,
      "chunk": "The largest and most capable LLMs are generative pretrained transformers (GPTs), which are largely used in generative chatbots such as ChatGPT, Gemini or Claude. LLMs can be fine-tuned for specific tasks or guided by prompt engineering.[1] These models acquire predictive power regarding syntax, semantics, and ontologies[2] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[3]\nHistory"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

{
  "query": "\"What role do support vector machines, random forests, and XGBoost play in predictive analytics for oncology diagnostics at institutions like Mayo Clinic and Johns Hopkins, and how do HIPAA and GDPR affect data handling in these models?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 7741,
      "chunk": "dom forest forces the algorithm to consider many possible explanations, the result\nbeing that the random forest captures a much broader picture of the data than a sin‐\ngle tree.\nStrengths, weaknesses, and parameters.    Random forests for regression and classifica‐\ntion are currently among the most widely used machine learning methods. They are\nvery powerful, often work well without heavy tuning of the parameters, and don’t\nrequire scaling of the data."
    },
    {
      "id": 20259,
      "chunk": "12\nSUPPORT VECTOR MACHINES\nDeveloped inside the computer science community in the 1990s, support vector\nmachines (SVM) was initially designed for predicting numeric and categorical\noutcomes as a double-barrel prediction technique. Today, though, SVM is mostly\nused as a classification technique for predicting categorical outcomes.\nAs a classification technique, SVM is similar to logistic regression, in that it’s\nused to filter data into a binary or multiclass target variable. But, as seen in"
    },
    {
      "id": 20770,
      "chunk": "for machine learning in health care to provide professionals an additional tool to diagnose, medicate, and plan recovery paths for patients, but this requires these biases to be mitigated.[166]"
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

{
  "query": "\"How have multilingual models like mBERT, XLM-RoBERTa, and M2M-100 improved machine translation quality between low-resource languages such as Swahili, Tagalog, and Uyghur, especially in humanitarian settings supported by UNHCR and Translators without Borders?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 21538,
      "chunk": "Anwar, U.; Saparov, A.; Rando, J.; Paleka, D.; Turpin, M.; Hase, P.; Lubana, E. S.; Jenner, E.; Casper, S.; Sourbut, O.; Edelman, B. L.; Zhang, Z.; Günther, M.; Korinek, A.; Hernandez-Orallo, J.; Hammond, L.; Bigelow, E.; Pan, A.; Langosco, L.; Krueger, D. (2024). \"Foundational Challenges in Assuring Alignment and Safety of Large Language Models\". arXiv:2404.09932 [cs.LG]."
    },
    {
      "id": 21210,
      "chunk": "Machine Translation System: Bridging the Gap between Human and Machine Translation\". arXiv:1609.08144 [cs.CL]."
    },
    {
      "id": 21447,
      "chunk": "Movva, Rajiv; Balachandar, Sidhika; Peng, Kenny; Agostini, Gabriel; Garg, Nikhil; Pierson, Emma (2024). \"Topics, Authors, and Institutions in Large Language Model Research: Trends from 17K arXiv Papers\". Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers). pp. 1223–1243. arXiv:2307.10700. doi:10.18653/v1/2024.naacl-long.67. Retrieved 2024-12-08."
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\"ChatGPT is more like an 'alien intelligence' than a human brain, says futurist\". ZDNET. 2023. Archived from the original on 12 June 2023. Retrieved 12 June 2023.\nNewport, Cal (13 April 2023). \"What Kind of Mind Does ChatGPT Have?\". The New Yorker. Archived from the original on 12 June 2023. Retrieved 12 June 2023."
    },
    {
      "chunk": "\"Data Augmentation - deeplearning.ai | Coursera\". Coursera. Archived from the original on 1 December 2017. Retrieved 30 November 2017.\nHinton, G. E. (2010). \"A Practical Guide to Training Restricted Boltzmann Machines\". Tech. Rep. UTML TR 2010-003. Archived from the original on 2021-05-09. Retrieved 2017-06-13."
    },
    {
      "chunk": "\"Burns, Benureau, Tani (2018) A Bergson-Inspired Adaptive Time Constant for the Multiple Timescales Recurrent Neural Network Model. JNNS\".\nBarkan, Oren; Benchimol, Jonathan; Caspi, Itamar; Cohen, Eliya; Hammer, Allon; Koenigstein, Noam (2023). \"Forecasting CPI inflation components with Hierarchical Recurrent Neural Networks\". International Journal of Forecasting. 39 (3): 1145–1162. arXiv:2011.07920. doi:10.1016/j.ijforecast.2022.04.009."
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    },
    {
      "chunk": "\"Adversarial Machine Learning – CLTC UC Berkeley Center for Long-Term Cybersecurity\". CLTC. Archived from the original on 17 May 2022. Retrieved 25 May 2022.\n\"Machine-learning models vulnerable to undetectable backdoors\". The Register. Archived from the original on 13 May 2022. Retrieved 13 May 2022.\n\"Undetectable Backdoors Plantable In Any Machine-Learning Algorithm\". IEEE Spectrum. 10 May 2022. Archived from the original on 11 May 2022. Retrieved 13 May 2022."
    }
  ]
}

{
  "query": "\"What alignment techniques are being developed by OpenAI, Anthropic, and Google DeepMind to prevent reward hacking and specification gaming in reinforcement learning agents deployed in financial systems like Nasdaq and the London Stock Exchange?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 20882,
      "chunk": "\"AI and Compute\". OpenAI. 16 May 2018. Archived from the original on 17 June 2020. Retrieved 11 June 2020."
    },
    {
      "id": 21256,
      "chunk": "\"A Google DeepMind Algorithm Uses Deep Learning and More to Master the Game of Go | MIT Technology Review\". MIT Technology Review. Archived from the original on 1 February 2016. Retrieved 30 January 2016.\nMetz, Cade (6 November 2017). \"A.I. Researchers Leave Elon Musk Lab to Begin Robotics Start-Up\". The New York Times. Archived from the original on 7 July 2019. Retrieved 5 July 2019."
    },
    {
      "id": 16305,
      "chunk": "ing approach. Google DeepMind used deep learning to train its \n“AlphaGo” ­program and defeat Lee Sedol, one of the strongest \nhuman Go players."
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

{
  "query": "\"How do generative AI tools like DALL·E 3, Midjourney, and Stable Diffusion handle copyright concerns when trained on datasets scraped from DeviantArt, Getty Images, and Flickr, and what legal actions have been initiated by the U.S. Copyright Office?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 16632,
      "chunk": "28\nCHAPTER 2\nPretrained networks\n While this scenario is a bit farcical, the underlying technology is sound and will\nlikely have a profound impact on the perceived veracity of digital data in the years to\ncome. The entire concept of “photographic evidence” is likely to become entirely sus-\npect, given how easy it will be to automate the production of convincing, yet fake,\nimages and video. The only key ingredient is data. Let’s see how this process works.\n2.2.1\nThe GAN game"
    },
    {
      "id": 21416,
      "chunk": "Memorization and copyright\nFurther information: Artificial intelligence and copyright"
    },
    {
      "id": 21422,
      "chunk": "was strategically placing web content through mass publication and duplication with the intention of biasing LLM outputs. The American Sunlight Project coined this technique \"LLM grooming\", and pointed to it as a new tool of weaponizing AI to spread disinformation and harmful content.[161][162] Similarly, Yongge Wang[163] illustrated in 2024 how a potential criminal could potentially bypass ChatGPT 4o's safety controls to obtain information on establishing a drug trafficking operation. External"
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

{
  "query": "\"What are the limitations of Reinforcement Learning from Human Feedback (RLHF) when used to train conversational models like ChatGPT and Claude on feedback collected via Reddit, StackOverflow, and Mechanical Turk?\"",
  "top_k": 3,
  "mode": "union",
  "results": [
    {
      "id": 21438,
      "chunk": "Foundation models\n    List of large language models\n    List of chatbots\n    Language model benchmark\n    Reinforcement learning\n    Small language model\n\nReferences"
    },
    {
      "id": 21303,
      "chunk": "Reinforcement learning from human feedback"
    },
    {
      "id": 21304,
      "chunk": "RLHF involves training a reward model to predict which text humans prefer. Then, the LLM can be fine-tuned through reinforcement learning to better satisfy this reward model. Since humans typically prefer truthful, helpful and harmless answers, RLHF favors such answers.[38]\nArchitecture"
    },
    {
      "chunk": "\u0002\n∇2\nθg(θ)\n\u0003−1. Applying Slutsky’s theorem (Theorem 2.4) proves the claim.\nNow that we established the asymptotic properties of the MLE for exponen-\ntial families it is only natural to ask how much variation one may expect in\nˆθ[X] when performing estimation. The Cramer-Rao bound governs this.\nTheorem 2.19 (Cram´er and Rao [Rao73]) Assume that X is drawn from\np(X; θ) and let ˆθ[X] be an asymptotically unbiased estimator. Denote by I"
    },
    {
      "chunk": "\u0000H \\ S\nn∈N Hn\n\u0001\n.\n4. Construct a class H1 of functions from the unit interval [0, 1] to {0, 1} that\nis nonuniformly learnable but not PAC learnable.\n5. Construct a class H2 of functions from the unit interval [0, 1] to {0, 1} that\nis not nonuniformly learnable.\n6. In this question we wish to show that the algorithm Memorize is a consistent\nlearner for every class of (binary-valued) functions over any countable domain.\nLet X be a countable domain and let D be a probability distribution over X."
    },
    {
      "chunk": "\u0010e m\nd\n\u0011d\n.\nDenote A = {(1[h(x1)̸=y1], . . . , 1[h(xm)̸=ym]) : h ∈H}. This clearly implies that\n|A| ≤\n\u0010e m\nd\n\u0011d\n.\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0003\n= min\nθ\n\u0002\n∥Vt−1θ −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n=\n\u0002\n∥Vt−1θt−1 −y∥2\u0003\n+ min\nα\n\u0002\nα2∥uj∥2 −2α⟨uj, y⟩\n\u0003\n= ∥Vt−1θt−1 −y∥2 −(⟨uj, y⟩)2\n∥uj∥2\n.\nIt follows that we should select the feature\njt = argmax\nj\n(⟨uj, y⟩)2\n∥uj∥2\n.\nThe rest of the update is to set\nVt =\n\u0014\nVt−1,\nujt\n∥ujt∥2\n\u0015\n,\nθt =\n\u0014\nθt−1 ; ⟨ujt, y⟩\n∥ujt∥2\n\u0015\n.\nThe OMP procedure maintains an orthonormal basis of the selected features,\nwhere in the preceding description, the orthonormalization property is obtained"
    },
    {
      "chunk": "\u0000LS1(f) −LS2(f)\n\u0001\n.\n(26.2)\nUnderstanding Machine Learning, c⃝2014 by Shai Shalev-Shwartz and Shai Ben-David\nPublished 2014 by Cambridge University Press.\nPersonal use only. Not for distribution. Do not post.\nPlease link to http://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning"
    },
    {
      "chunk": "\u0000n\n2\n\u0001\n·\n2\nn2+β <\n1/nβ as claimed in the Johnson Lindenstrauss Lemma. All that remains is\nto prove Lemma 1.2 and 1.3.\nProof (Lemma 1.2). Since qi =\n1\n√\nk\nP\nj Rijαj is a linear combination of stan-\ndard normal random variables Rij it follows that qi is normally distributed.\nTo compute the mean note that\nE [qi] =\n1\n√\nk\nX\nj\nαj E [Rij] = 0.\nSince Rij are independent zero mean unit variance random variables, E [RijRil] =\n1 if j = l and 0 otherwise. Using this\nE\n\u0002\nq2\ni\n\u0003\n= 1\nk E\n\n\nd\nX\nj=1\nRijαj\n\n"
    }
  ]
}

