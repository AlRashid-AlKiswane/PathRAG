# Work Summary - Markdown Document Chunker

## Overview

Today, I developed and enhanced a **Markdown Document Chunker** tool designed to process Markdown files or directories containing Markdown documents. The tool intelligently splits large Markdown documents into smaller, semantically meaningful chunks suitable for downstream processing such as indexing, semantic search, or integration with language models.

---

## Key Features and Technologies Used

### 1. Recursive Markdown Splitting Logic

- Implemented a robust recursive splitter that parses Markdown headers (`#`, `##`, `###`, etc.) to divide documents into hierarchical chunks preserving the document’s structure.
- Added recursive chunk splitting to ensure that no chunk exceeds a specified maximum size, which is critical for embedding models or other downstream tasks.
- The splitter respects the semantic meaning of sections, sub-sections, and paragraphs to avoid breaking context abruptly.

### 2. Directory and File Processing

- Added support for processing both individual Markdown files and recursively traversing directories containing multiple Markdown files.
- This enables bulk processing of entire document collections with a single command.

### 3. MongoDB Integration

- Integrated with MongoDB using `pymongo` to store processed chunks.
- Added the ability to reset the MongoDB collection to avoid duplicate or stale data on subsequent runs.
- Each chunk is stored with metadata including the source filename and chunk index for traceability.

### 4. Comprehensive Logging

- Configured Python’s built-in `logging` module to provide detailed logs for all processing steps.
- Logs include info-level status updates, debug messages for internal state, and error-level logs for exceptions.
- This logging setup ensures easy monitoring, debugging, and maintenance in production environments.

### 5. User Feedback via Progress Bars

- Incorporated `tqdm` progress bars for visual feedback during long-running operations.
- Progress bars wrap over file processing and database insertion loops, showing real-time progress and estimated time remaining.
- This improves usability and transparency when processing large datasets.

### 6. FastAPI Endpoint

- Exposed the chunking functionality as a REST API using FastAPI.
- The `/api/v1/chunker-md` POST endpoint accepts JSON input specifying the path to process and whether to reset the database.
- This facilitates easy integration with other systems or frontend interfaces.

---

## End-to-End Workflow

1. **Input:** Receive a path (file or directory) and reset flag via API or command line.
2. **File Discovery:** Recursively scan for `.md` files if a directory is provided.
3. **Chunking:** For each file:
   - Read full Markdown content.
   - Recursively split based on Markdown headers and chunk size limits.
4. **Database Storage:** Insert each chunk into MongoDB with metadata.
5. **Logging & Progress:** Log detailed processing steps and update progress bars.
6. **Output:** Return count of inserted chunks or processing status.

---

## Tools and Libraries Used

- **Python `logging`** — For structured, multi-level logs.
- **`tqdm`** — To show progress bars for batch file processing and DB insertion.
- **`pymongo`** — MongoDB driver for inserting and managing chunk documents.
- **`FastAPI`** — For building the chunking REST API endpoint.
- **Standard Python Libraries** — For file I/O, regex, and string operations.

---

## Summary

The chunker project today ensures reliable and maintainable processing of Markdown documents into manageable pieces with complete traceability and real-time feedback. Logging and progress bars improve operational visibility, while FastAPI provides a scalable interface for automation or UI integration. MongoDB persistence allows for long-term storage and retrieval of processed chunks.

---