{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ“š JAIS-13B Comprehensive Language Model Evaluation\n",
    "\n",
    "Welcome to the comprehensive evaluation notebook for the **JAIS-13B Chat Model** (`inceptionai/jais-13b-chat`).  \n",
    "This notebook is designed to **test, monitor, and benchmark** the performance of the model across 100 multilingual questions covering diverse categories.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  Purpose\n",
    "\n",
    "This notebook enables:\n",
    "- Automated response generation for structured input test cases.\n",
    "- Systematic tracking of system resource usage (CPU, RAM, GPU, and response time).\n",
    "- Output logging and summary statistics to evaluate:\n",
    "  - Model accuracy\n",
    "  - System efficiency\n",
    "  - Response consistency across categories and languages\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Features\n",
    "\n",
    "- âœ… Hugging Face model + tokenizer loading\n",
    "- âœ… Bilingual prompt creation (Arabic + English)\n",
    "- âœ… Token-level configuration using `GenerationConfig`\n",
    "- âœ… Full test suite runner with real-time system monitoring\n",
    "- âœ… Summary JSON output with success/failure rates and performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª Test Categories\n",
    "\n",
    "The test cases span multiple domains including:\n",
    "- General knowledge\n",
    "- Science & technology\n",
    "- Language understanding\n",
    "- Multilingual capabilities (English and Arabic)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ˆ Output Files\n",
    "\n",
    "The notebook produces:\n",
    "- `llm_test_results.json` â€“ detailed logs of each model response and system performance\n",
    "- `llm_test_results_summary.json` â€“ aggregate statistics and insights\n",
    "\n",
    "---\n",
    "\n",
    "> âš ï¸ **Note:** This notebook is optimized for GPUs with â‰¥16GB memory. Ensure your environment has sufficient resources before running all 100 test cases.\n",
    "\n",
    "Happy benchmarking! ğŸ’¡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ Install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:34.338141Z",
     "iopub.status.busy": "2025-07-31T09:29:34.337812Z",
     "iopub.status.idle": "2025-07-31T09:29:37.375246Z",
     "shell.execute_reply": "2025-07-31T09:29:37.374284Z",
     "shell.execute_reply.started": "2025-07-31T09:29:34.338116Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“‹ Define Test Cases List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:31:24.798354Z",
     "iopub.status.busy": "2025-07-31T09:31:24.798007Z",
     "iopub.status.idle": "2025-07-31T09:31:24.818159Z",
     "shell.execute_reply": "2025-07-31T09:31:24.817551Z",
     "shell.execute_reply.started": "2025-07-31T09:31:24.798333Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # Arabic General Knowledge (25 questions)\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ù…Ù† Ù‡Ùˆ Ø£ÙˆÙ„ Ø®Ù„ÙŠÙØ© ÙÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"history\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø£Ø·ÙˆÙ„ Ù†Ù‡Ø± ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ø§Ø°ÙƒØ± Ø®Ù…Ø³Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„ÙÙˆØ§ÙƒÙ‡\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ÙƒÙ… Ø¹Ø¯Ø¯ Ø£ÙŠØ§Ù… Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø£ÙƒØ¨Ø± Ù…Ø­ÙŠØ· ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ù…Ù† Ø§Ø®ØªØ±Ø¹ Ø§Ù„Ù…ØµØ¨Ø§Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø¹Ù…Ù„Ø© Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©ØŸ\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"ÙƒÙ… Ø¹Ø¯Ø¯ Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø­ÙŠÙˆØ§Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"nature\"},\n",
    "    {\"input\": \"ÙÙŠ Ø£ÙŠ Ø³Ù†Ø© ØªÙ… Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø£ÙƒØ¨Ø± Ø¯ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø­Ø©ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¸Ø§Ù… ÙÙŠ Ø¬Ø³Ù… Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø¨Ø§Ù„ØºØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø£Ø¹Ù…Ù‚ Ø®Ù†Ø¯Ù‚ ÙÙŠ Ø§Ù„Ù…Ø­ÙŠØ·ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ù…Ù† ÙƒØªØ¨ Ø±ÙˆØ§ÙŠØ© Ù…Ø¦Ø© Ø¹Ø§Ù… Ù…Ù† Ø§Ù„Ø¹Ø²Ù„Ø©ØŸ\", \"language\": \"arabic\", \"category\": \"literature\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø£ØµØºØ± Ø¯ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ÙƒÙ… Ø¹Ø¯Ø¯ Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø¨Ø§Ù„ØºØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø±Ù…Ø² Ø¹Ù†ØµØ± Ø§Ù„Ø°Ù‡Ø¨ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ÙÙŠ Ø£ÙŠ Ù‚Ø§Ø±Ø© ØªÙ‚Ø¹ Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¨Ø±Ø§Ø²ÙŠÙ„ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø¬ÙˆÙŠØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ÙƒÙ… Ø¹Ø¯Ø¯ Ø£Ù„ÙˆØ§Ù† Ù‚ÙˆØ³ Ù‚Ø²Ø­ØŸ\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø£Ø·ÙˆÙ„ Ø¬Ø¨Ù„ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ÙÙŠ Ø£ÙŠ Ø³Ù†Ø© Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"history\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø£ÙƒØ¨Ø± ØµØ­Ø±Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    \n",
    "    # Arabic Technical/AI (25 questions)\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙˆØµÙŠØ©ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¨Ù„ÙˆÙƒ ØªØ´ÙŠÙ†ØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø·Ø¨ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Python Ùˆ JavaØŸ\", \"language\": \"arabic\", \"category\": \"programming\"},\n",
    "    {\"input\": \"ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙŠ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŸ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØºÙŠØ± Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    \n",
    "    # English General Knowledge (25 questions)\n",
    "    {\"input\": \"What is artificial intelligence?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is the capital of France?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who invented the telephone?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    {\"input\": \"What is the largest planet in our solar system?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"How many continents are there?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"What is the speed of light?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"Who wrote Romeo and Juliet?\", \"language\": \"english\", \"category\": \"literature\"},\n",
    "    {\"input\": \"What is the chemical symbol for water?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"In which year did World War II end?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    {\"input\": \"What is the smallest country in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"How many bones are in the human body?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the longest river in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who painted the Mona Lisa?\", \"language\": \"english\", \"category\": \"art\"},\n",
    "    {\"input\": \"What is the currency of Japan?\", \"language\": \"english\", \"category\": \"general\"},\n",
    "    {\"input\": \"How many chambers does a human heart have?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the tallest mountain in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who discovered penicillin?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the largest ocean on Earth?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"In what year was the internet invented?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is the fastest animal on land?\", \"language\": \"english\", \"category\": \"nature\"},\n",
    "    {\"input\": \"How many days are in a leap year?\", \"language\": \"english\", \"category\": \"general\"},\n",
    "    {\"input\": \"What is the chemical symbol for gold?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"Which planet is known as the Red Planet?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the largest desert in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who was the first person to walk on the moon?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    \n",
    "    # English Technical/AI (25 questions)\n",
    "    {\"input\": \"Explain machine learning algorithms\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is deep learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How do neural networks work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is natural language processing?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain computer vision technology\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is reinforcement learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How does speech recognition work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What are recommendation algorithms?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain cloud computing\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is blockchain technology?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"How do search engines work?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is cybersecurity?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Explain big data analytics\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is the Internet of Things?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"How does encryption work?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is virtual reality?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Explain quantum computing\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What are clustering algorithms?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How does supervised learning work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is unsupervised learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain decision trees in machine learning\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is feature engineering?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How do convolutional neural networks work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is transfer learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain generative adversarial networks\", \"language\": \"english\", \"category\": \"ai\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› ï¸ Import Required Libraries and Setup Environment\n",
    "\n",
    "- `torch`: PyTorch library for tensor computations and model inference.  \n",
    "- `json`: For JSON serialization and deserialization.  \n",
    "- `time`: To measure time intervals.  \n",
    "- `psutil`: To monitor system resources like CPU and RAM usage.  \n",
    "- `GPUtil`: To track GPU usage and GPU memory consumption.  \n",
    "- `datetime`: For timestamping and time-related operations.  \n",
    "- `transformers`: Hugging Face Transformers for loading tokenizers and causal language models, and managing generation configurations.  \n",
    "- `warnings`: To suppress unwanted warning messages during execution.  \n",
    "- `huggingface_hub`: To authenticate and interact with Hugging Face Hub models.\n",
    "\n",
    "This setup is essential for running language model inference efficiently, while monitoring system performance and handling API authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:44.364190Z",
     "iopub.status.busy": "2025-07-31T09:29:44.363499Z",
     "iopub.status.idle": "2025-07-31T09:29:44.373960Z",
     "shell.execute_reply": "2025-07-31T09:29:44.373371Z",
     "shell.execute_reply.started": "2025-07-31T09:29:44.364158Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ–¥ï¸ System Monitoring Class\n",
    "\n",
    "This class tracks system resource usage during model inference or other processes:\n",
    "\n",
    "- **Attributes:**  \n",
    "  - `start_time`: Timestamp when monitoring begins.  \n",
    "  - `start_ram`: RAM usage in GB at start.  \n",
    "  - `start_cpu`: CPU usage % at start.  \n",
    "  - `start_gpu_memory`: GPU memory used at start (if GPU available).\n",
    "\n",
    "- **Methods:**  \n",
    "  - `start_monitoring()`: Records initial resource metrics (time, RAM, CPU, GPU).  \n",
    "  - `get_metrics()`: Calculates and returns resource usage since monitoring started, including:  \n",
    "    - Response time (seconds)  \n",
    "    - RAM usage (GB) delta  \n",
    "    - CPU usage (%)  \n",
    "    - GPU load (%) (if GPU detected)  \n",
    "    - GPU memory usage (MB) (if GPU detected)\n",
    "\n",
    "- **Usage:**  \n",
    "  Call `start_monitoring()` before starting the task, then `get_metrics()` after completion to gather resource stats.\n",
    "\n",
    "This helps track system performance and detect bottlenecks or resource constraints during model generation or heavy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:48.480096Z",
     "iopub.status.busy": "2025-07-31T09:29:48.479456Z",
     "iopub.status.idle": "2025-07-31T09:29:48.487161Z",
     "shell.execute_reply": "2025-07-31T09:29:48.486422Z",
     "shell.execute_reply.started": "2025-07-31T09:29:48.480068Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# System monitoring class\n",
    "class SystemMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.start_gpu_memory = None\n",
    "        self.start_ram = None\n",
    "        self.start_cpu = None\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_ram = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "        self.start_cpu = psutil.cpu_percent()\n",
    "        \n",
    "        # GPU monitoring\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                self.start_gpu_memory = gpus[0].memoryUsed\n",
    "        except:\n",
    "            self.start_gpu_memory = None\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - self.start_time\n",
    "        \n",
    "        current_ram = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "        current_cpu = psutil.cpu_percent()\n",
    "        \n",
    "        ram_usage = current_ram - self.start_ram\n",
    "        cpu_usage = current_cpu\n",
    "        \n",
    "        gpu_usage = None\n",
    "        gpu_memory_used = None\n",
    "        \n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]\n",
    "                gpu_usage = gpu.load * 100\n",
    "                gpu_memory_used = gpu.memoryUsed\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"response_time\": round(response_time, 3),\n",
    "            \"ram_usage_gb\": round(ram_usage, 3),\n",
    "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "            \"gpu_usage_percent\": round(gpu_usage, 2) if gpu_usage else None,\n",
    "            \"gpu_memory_mb\": gpu_memory_used if gpu_memory_used else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”‘ Authenticate Hugging Face Hub and Specify Model\n",
    "\n",
    "- Logs into Hugging Face Hub using your API token to access private or large models.  \n",
    "- Sets the variable `model_name` to specify the pretrained model to load (`inceptionai/jais-13b-chat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:11:25.202578Z",
     "iopub.status.busy": "2025-07-31T09:11:25.201944Z",
     "iopub.status.idle": "2025-07-31T09:11:25.337763Z",
     "shell.execute_reply": "2025-07-31T09:11:25.337058Z",
     "shell.execute_reply.started": "2025-07-31T09:11:25.202553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(\"hf___\")\n",
    "model_name = \"inceptionai/jais-13b-chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¥ Load Tokenizer and Model from Hugging Face Hub\n",
    "\n",
    "- **Tokenizer Loading:**  \n",
    "  Uses `AutoTokenizer.from_pretrained()` to load the tokenizer for the specified `model_name`.  \n",
    "  - `trust_remote_code=True` allows execution of custom tokenizer code if provided by the model repository.  \n",
    "  - `padding_side=\"left\"` pads sequences on the left, useful for causal language models that predict tokens on the right.\n",
    "\n",
    "- **Model Loading:**  \n",
    "  Uses `AutoModelForCausalLM.from_pretrained()` to load the causal language model weights with these options:  \n",
    "  - `torch_dtype=torch.float16`: Loads model weights in half precision (float16) to reduce GPU memory usage and improve speed.  \n",
    "  - `device_map=\"auto\"`: Automatically places model layers on available GPU(s) and CPU to optimize memory and performance.  \n",
    "  - `low_cpu_mem_usage=True`: Reduces CPU RAM usage during model loading by using efficient strategies.\n",
    "\n",
    "**Print statements** indicate progress to help debug loading delays, especially for large models like `inceptionai/jais-13b-chat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:12:46.872334Z",
     "iopub.status.busy": "2025-07-31T09:12:46.871566Z",
     "iopub.status.idle": "2025-07-31T09:21:41.009719Z",
     "shell.execute_reply": "2025-07-31T09:21:41.009081Z",
     "shell.execute_reply.started": "2025-07-31T09:12:46.872307Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d51024de72048dcb61c3b4e335a85be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c3d74ef95476fa27804c4844fb82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bc7016edf64246b66b5da9c9b48e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdb683471b143b5853064f1f4821f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcce18b8ab2449a9af16e4e2c8aaaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_jais.py:   0%|          | 0.00/6.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/inceptionai/jais-13b-chat:\n",
      "- configuration_jais.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03447fc86694c279811442c25eedfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_jais.py:   0%|          | 0.00/68.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/inceptionai/jais-13b-chat:\n",
      "- modeling_jais.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "2025-07-31 09:12:56.622551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753953176.872355      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753953176.960754      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e27b5b4c441d1bcadda6b6e03aacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/42.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dd7cf0c748446cb7222aac5f59b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee856cdb495b42bdb75a7bd611f53e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00006.bin:   0%|          | 0.00/9.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e4a6d356dc4075bbd2da41966f1bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00006.bin:   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbba68675f1b4c8a805e6f8179c5ff45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00006.bin:   0%|          | 0.00/9.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da036363e0b04704ada08282787f5844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00006.bin:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2819f5b4bcf545149790d2cccfd1a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00006.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba646269d04f918e27b3a72f598f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00006.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eb5979ad3d40fab51cb9c25078d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/44.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18094345c84a40588d354a1d525b14d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588c6f9b14547af9bfba7d4ec5a25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  # Use float16 for better memory efficiency\n",
    "    device_map=\"auto\",  # Automatically distribute across available GPUs\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âš™ï¸ Ensure Tokenizer Has a Padding Token\n",
    "\n",
    "- Checks if the tokenizer lacks a `pad_token`.  \n",
    "- If missing, assigns the `eos_token` (end-of-sequence token) as the `pad_token`.  \n",
    "- This prevents errors during batch encoding and padding, especially for causal language models that may not have an explicit pad token by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:21:52.353722Z",
     "iopub.status.busy": "2025-07-31T09:21:52.352890Z",
     "iopub.status.idle": "2025-07-31T09:21:52.356943Z",
     "shell.execute_reply": "2025-07-31T09:21:52.356392Z",
     "shell.execute_reply.started": "2025-07-31T09:21:52.353694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§  Define Text Generation Configuration\n",
    "\n",
    "Sets up a `GenerationConfig` object to control how the model generates responses:\n",
    "\n",
    "- **Core Generation Parameters:**\n",
    "  - `max_new_tokens=512`: Limits generated output to 512 tokens.\n",
    "  - `min_length=10`: Ensures the output has at least 10 tokens.\n",
    "\n",
    "- **Sampling & Decoding Strategies:**\n",
    "  - `do_sample=True`: Enables sampling instead of deterministic decoding.\n",
    "  - `temperature=0.7`: Adds randomness to predictions (lower = more focused).\n",
    "  - `top_k=50`: Limits sampling to the top 50 most likely tokens.\n",
    "  - `top_p=0.9`: Nucleus sampling (includes tokens until 90% cumulative probability).\n",
    "  - `typical_p=0.95`: Typical decoding based on entropy of tokens.\n",
    "\n",
    "- **Repetition & Coherence Control:**\n",
    "  - `repetition_penalty=1.1`: Penalizes repeated tokens.\n",
    "  - `no_repeat_ngram_size=3`: Prevents repeating sequences of 3 or more tokens.\n",
    "\n",
    "- **Token IDs:**\n",
    "  - `pad_token_id`, `eos_token_id`, `bos_token_id`: Set based on tokenizer to handle special tokens properly.\n",
    "\n",
    "- **Beam Search (optional here):**\n",
    "  - `num_beams=1`: Disables beam search (acts as greedy decoding).\n",
    "  - `early_stopping=True`: Stops generation when an EOS token is reached.\n",
    "\n",
    "- **Advanced Controls (mostly unused or left as default):**\n",
    "  Includes penalties, constraints, suppression lists, and other fine-tuning options for advanced decoding control.\n",
    "\n",
    "> This configuration is passed to `model.generate()` to guide and fine-tune the response behavior of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:21:55.948829Z",
     "iopub.status.busy": "2025-07-31T09:21:55.948559Z",
     "iopub.status.idle": "2025-07-31T09:21:55.955450Z",
     "shell.execute_reply": "2025-07-31T09:21:55.954718Z",
     "shell.execute_reply.started": "2025-07-31T09:21:55.948809Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    # Core parameters\n",
    "    max_new_tokens=512,           # Maximum number of tokens to generate\n",
    "    min_length=10,                # Minimum length of generated sequence\n",
    "    \n",
    "    # Sampling parameters\n",
    "    do_sample=True,               # Enable sampling\n",
    "    temperature=0.7,              # Controls randomness (0.1-2.0)\n",
    "    top_k=50,                     # Top-k sampling\n",
    "    top_p=0.9,                    # Nucleus sampling\n",
    "    typical_p=0.95,               # Typical sampling\n",
    "    \n",
    "    # Repetition control\n",
    "    repetition_penalty=1.1,       # Penalty for repetition\n",
    "    no_repeat_ngram_size=3,       # Prevent repeating n-grams\n",
    "    \n",
    "    # Special tokens\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    \n",
    "    # Decoding strategy\n",
    "    num_beams=1,                  # Number of beams for beam search (1 = greedy)\n",
    "    early_stopping=True,          # Stop when EOS is generated\n",
    "    \n",
    "    # Additional parameters\n",
    "    length_penalty=1.0,           # Length penalty for beam search\n",
    "    diversity_penalty=0.0,        # Diversity penalty for diverse beam search\n",
    "    encoder_no_repeat_ngram_size=0,\n",
    "    bad_words_ids=None,\n",
    "    force_words_ids=None,\n",
    "    renormalize_logits=False,\n",
    "    constraints=None,\n",
    "    forced_bos_token_id=None,\n",
    "    forced_eos_token_id=None,\n",
    "    remove_invalid_values=False,\n",
    "    exponential_decay_length_penalty=None,\n",
    "    suppress_tokens=None,\n",
    "    begin_suppress_tokens=None,\n",
    "    forced_decoder_ids=None,\n",
    "    sequence_bias=None,\n",
    "    guidance_scale=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ§ª Run Comprehensive Test Suite with System Monitoring\n",
    "\n",
    "This function runs a full set of LLM test cases with system performance tracking. It:\n",
    "\n",
    "- Iterates over a list of 100 predefined `test_cases`.\n",
    "- Logs input, category, and language for each question.\n",
    "- Monitors system resource usage (RAM, CPU, GPU, time) for each generation using `SystemMonitor`.\n",
    "- Calls a response generator (`generate_response_with_monitoring()`).\n",
    "- Captures and prints performance metrics for each question.\n",
    "- Appends results (response + metrics) to a list.\n",
    "- Catches and logs any exceptions per test case.\n",
    "- Saves final results to a `.json` file.\n",
    "- Generates a test summary JSON via `generate_test_summary()`.\n",
    "\n",
    "**Metrics Tracked Per Response:**\n",
    "- Response time in seconds\n",
    "- RAM usage in GB\n",
    "- CPU usage %\n",
    "- GPU usage % (if available)\n",
    "- GPU memory used in MB\n",
    "\n",
    "**Output:**\n",
    "- A detailed results file (`llm_test_results.json`)\n",
    "- A summary statistics file (`llm_test_results_summary.json`)\n",
    "\n",
    "This function helps benchmark LLM behavior across a variety of inputs while tracking compute efficiency and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:05.363322Z",
     "iopub.status.busy": "2025-07-31T09:32:05.362747Z",
     "iopub.status.idle": "2025-07-31T09:32:05.372148Z",
     "shell.execute_reply": "2025-07-31T09:32:05.371532Z",
     "shell.execute_reply.started": "2025-07-31T09:32:05.363300Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced test runner with system monitoring\n",
    "def run_comprehensive_test(model, tokenizer, generation_config, output_file=\"llm_test_results.json\"):\n",
    "    \"\"\"\n",
    "    Run comprehensive test suite with 100 questions and system monitoring\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_questions = len(test_cases)\n",
    "    \n",
    "    print(f\"Starting comprehensive test with {total_questions} questions...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nQuestion {i}/{total_questions}\")\n",
    "        print(f\"Category: {test_case['category']}\")\n",
    "        print(f\"Language: {test_case['language']}\")\n",
    "        print(f\"Input: {test_case['input']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Initialize system monitor\n",
    "        monitor = SystemMonitor()\n",
    "        monitor.start_monitoring()\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generate_response_with_monitoring(\n",
    "                test_case['input'],\n",
    "                language=test_case['language'],\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            \n",
    "            # Get system metrics\n",
    "            metrics = monitor.get_metrics()\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"category\": test_case['category'],\n",
    "                \"language\": test_case['language'],\n",
    "                \"input\": test_case['input'],\n",
    "                \"response\": response,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": metrics,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            print(f\"Response: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "            print(f\"Response Time: {metrics['response_time']}s\")\n",
    "            print(f\"RAM Usage: {metrics['ram_usage_gb']} GB\")\n",
    "            print(f\"CPU Usage: {metrics['cpu_usage_percent']}%\")\n",
    "            if metrics['gpu_usage_percent']:\n",
    "                print(f\"GPU Usage: {metrics['gpu_usage_percent']}%\")\n",
    "                print(f\"GPU Memory: {metrics['gpu_memory_mb']} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"category\": test_case['category'],\n",
    "                \"language\": test_case['language'],\n",
    "                \"input\": test_case['input'],\n",
    "                \"response\": None,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (i / total_questions) * 100\n",
    "        print(f\"Progress: {progress:.1f}%\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Generate summary\n",
    "    generate_test_summary(results, output_file.replace('.json', '_summary.json'))\n",
    "    \n",
    "    print(f\"\\nTest completed! Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ’¬ Generate LLM Response with Bilingual Prompt and Performance Tracking\n",
    "\n",
    "This function generates a response using the JAIS model, configured with Hugging Face Transformers:\n",
    "\n",
    "- **Inputs:**\n",
    "  - `user_input`: The raw input question or text.\n",
    "  - `language`: The target response language (`en`, `ar`, etc.).\n",
    "  - `model`: A causal language model instance.\n",
    "  - `tokenizer`: Corresponding tokenizer.\n",
    "  - `generation_config`: Hugging Face `GenerationConfig` controlling decoding behavior.\n",
    "\n",
    "- **Steps:**\n",
    "  1. Creates a **bilingual prompt** using `create_bilingual_prompt()` to condition the model.\n",
    "  2. Tokenizes the prompt with truncation and padding (max 2048 tokens).\n",
    "  3. Moves inputs to the correct device (`model.device`, usually GPU).\n",
    "  4. Runs the model in **inference-only mode** (`torch.no_grad()` with `use_cache=True`) to speed up decoding.\n",
    "  5. Decodes only the **newly generated tokens**, excluding the input part.\n",
    "  6. Returns the cleaned, final model response as a string.\n",
    "\n",
    "> This function is called per test case and used in conjunction with `SystemMonitor` to track performance per generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:16.583909Z",
     "iopub.status.busy": "2025-07-31T09:32:16.583656Z",
     "iopub.status.idle": "2025-07-31T09:32:16.589352Z",
     "shell.execute_reply": "2025-07-31T09:32:16.588502Z",
     "shell.execute_reply.started": "2025-07-31T09:32:16.583892Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response_with_monitoring(user_input, language, model, tokenizer, generation_config):\n",
    "    \"\"\"\n",
    "    Generate response with the JAIS model\n",
    "    \"\"\"\n",
    "    prompt = create_bilingual_prompt(user_input, language)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸŒ Create Bilingual Prompt for JAIS Model (English + Arabic)\n",
    "\n",
    "This function generates a language-aware prompt designed for the **JAIS multilingual LLM**, adapting to the user's input language:\n",
    "\n",
    "- **Inputs:**\n",
    "  - `user_message`: The user's raw question or message.\n",
    "  - `language`: Optional language override (`\"arabic\"` or `\"auto\"`). If `\"auto\"`, the function detects Arabic script based on Unicode ranges.\n",
    "\n",
    "- **Logic:**\n",
    "  - If the message is in Arabic (or explicitly specified), the prompt is constructed using Arabic system instructions.\n",
    "  - Otherwise, the prompt is constructed in English.\n",
    "  \n",
    "- **Prompt Format:**\n",
    "  - Starts with a role definition (`Instruction:` / `Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:`).\n",
    "  - Followed by the user's question (`Question:` / `Ø§Ù„Ø³Ø¤Ø§Ù„:`).\n",
    "  - Ends with an open-ended answer section (`Response:` / `Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:`) for the model to complete.\n",
    "\n",
    "> This prompt design ensures consistent structure and bilingual adaptability, improving the performance of multilingual models like `jais-13b-chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:41.028110Z",
     "iopub.status.busy": "2025-07-31T09:32:41.027493Z",
     "iopub.status.idle": "2025-07-31T09:32:41.032461Z",
     "shell.execute_reply": "2025-07-31T09:32:41.031773Z",
     "shell.execute_reply.started": "2025-07-31T09:32:41.028089Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_bilingual_prompt(user_message, language=\"auto\"):\n",
    "    \"\"\"\n",
    "    Create a bilingual prompt for JAIS model\n",
    "    \"\"\"\n",
    "    if language == \"arabic\" or (language == \"auto\" and any('\\u0600' <= char <= '\\u06FF' for char in user_message)):\n",
    "        system_prompt = \"\"\"Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© ÙˆØ§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ¯Ù‚Ø©.\"\"\"\n",
    "        prompt = f\"\"\"### Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n",
    "{system_prompt}\n",
    "\n",
    "### Ø§Ù„Ø³Ø¤Ø§Ù„:\n",
    "{user_message}\n",
    "\n",
    "### Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n",
    "\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful and intelligent assistant that speaks both Arabic and English. Answer questions clearly and accurately.\"\"\"\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "### Question:\n",
    "{user_message}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“Š Generate Test Summary and Performance Statistics\n",
    "\n",
    "This function analyzes the results from the comprehensive test run and produces a summary JSON report.\n",
    "\n",
    "---\n",
    "\n",
    "#### âœ… Key Outputs:\n",
    "- **Total questions tested**\n",
    "- **Number of successful and failed responses**\n",
    "- **Success rate (%)**\n",
    "- **Average system performance metrics:**\n",
    "  - Response time (s)\n",
    "  - RAM usage (GB)\n",
    "  - CPU usage (%)\n",
    "  - GPU usage (%) â€” if available\n",
    "  - GPU memory usage (MB) â€” if available\n",
    "- **Category-level success breakdown**\n",
    "- **Language-level success breakdown**\n",
    "- **Timestamp of test summary creation**\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ›  How It Works:\n",
    "1. Filters for successful responses only.\n",
    "2. Calculates mean values for all available performance metrics.\n",
    "3. Groups results by category and language for detailed analysis.\n",
    "4. Outputs the summary to a JSON file (e.g., `llm_test_results_summary.json`).\n",
    "5. Displays key metrics to console.\n",
    "\n",
    "> This is the final reporting step in your testing pipeline. It allows performance comparison across different model versions, prompts, or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:33:00.972081Z",
     "iopub.status.busy": "2025-07-31T09:33:00.971764Z",
     "iopub.status.idle": "2025-07-31T09:33:00.983027Z",
     "shell.execute_reply": "2025-07-31T09:33:00.982331Z",
     "shell.execute_reply.started": "2025-07-31T09:33:00.972032Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_summary(results, summary_file):\n",
    "    \"\"\"\n",
    "    Generate test summary statistics\n",
    "    \"\"\"\n",
    "    total_questions = len(results)\n",
    "    successful_responses = len([r for r in results if r['status'] == 'success'])\n",
    "    failed_responses = total_questions - successful_responses\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    successful_results = [r for r in results if r['status'] == 'success' and r['performance_metrics']]\n",
    "    \n",
    "    if successful_results:\n",
    "        avg_response_time = sum(r['performance_metrics']['response_time'] for r in successful_results) / len(successful_results)\n",
    "        avg_ram_usage = sum(r['performance_metrics']['ram_usage_gb'] for r in successful_results) / len(successful_results)\n",
    "        avg_cpu_usage = sum(r['performance_metrics']['cpu_usage_percent'] for r in successful_results) / len(successful_results)\n",
    "        \n",
    "        gpu_results = [r for r in successful_results if r['performance_metrics']['gpu_usage_percent']]\n",
    "        avg_gpu_usage = sum(r['performance_metrics']['gpu_usage_percent'] for r in gpu_results) / len(gpu_results) if gpu_results else None\n",
    "        avg_gpu_memory = sum(r['performance_metrics']['gpu_memory_mb'] for r in gpu_results) / len(gpu_results) if gpu_results else None\n",
    "    else:\n",
    "        avg_response_time = avg_ram_usage = avg_cpu_usage = avg_gpu_usage = avg_gpu_memory = None\n",
    "    \n",
    "    # Category breakdown\n",
    "    categories = {}\n",
    "    languages = {}\n",
    "    \n",
    "    for result in results:\n",
    "        cat = result['category']\n",
    "        lang = result['language']\n",
    "        \n",
    "        if cat not in categories:\n",
    "            categories[cat] = {'total': 0, 'success': 0}\n",
    "        if lang not in languages:\n",
    "            languages[lang] = {'total': 0, 'success': 0}\n",
    "        \n",
    "        categories[cat]['total'] += 1\n",
    "        languages[lang]['total'] += 1\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            categories[cat]['success'] += 1\n",
    "            languages[lang]['success'] += 1\n",
    "    \n",
    "    summary = {\n",
    "        \"test_overview\": {\n",
    "            \"total_questions\": total_questions,\n",
    "            \"successful_responses\": successful_responses,\n",
    "            \"failed_responses\": failed_responses,\n",
    "            \"success_rate\": round((successful_responses / total_questions) * 100, 2)\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"average_response_time\": round(avg_response_time, 3) if avg_response_time else None,\n",
    "            \"average_ram_usage_gb\": round(avg_ram_usage, 3) if avg_ram_usage else None,\n",
    "            \"average_cpu_usage_percent\": round(avg_cpu_usage, 2) if avg_cpu_usage else None,\n",
    "            \"average_gpu_usage_percent\": round(avg_gpu_usage, 2) if avg_gpu_usage else None,\n",
    "            \"average_gpu_memory_mb\": round(avg_gpu_memory, 2) if avg_gpu_memory else None\n",
    "        },\n",
    "        \"category_breakdown\": categories,\n",
    "        \"language_breakdown\": languages,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nTest Summary:\")\n",
    "    print(f\"Success Rate: {summary['test_overview']['success_rate']}%\")\n",
    "    print(f\"Average Response Time: {summary['performance_metrics']['average_response_time']}s\")\n",
    "    print(f\"Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸš€ Main Execution: Run the Comprehensive Test Suite\n",
    "\n",
    "This is the entry point that triggers the full benchmarking process when the script is run directly.\n",
    "\n",
    "---\n",
    "\n",
    "#### ğŸ”§ Steps Performed:\n",
    "\n",
    "1. **Define Generation Configuration:**  \n",
    "   Sets up generation parameters for the JAIS model:\n",
    "   - Sampling (`do_sample=True`) with temperature, top-k, and top-p.\n",
    "   - Repetition penalty and n-gram constraints to reduce redundancy.\n",
    "   - Special token IDs set from the tokenizer.\n",
    "   - `early_stopping=True` to stop on EOS token.\n",
    "\n",
    "2. **Run the Test Suite:**  \n",
    "   Calls `run_comprehensive_test()` with:\n",
    "   - The loaded model and tokenizer\n",
    "   - The configured generation settings\n",
    "   - An output file for saving detailed results with system metrics\n",
    "\n",
    "3. **Print Final Summary:**  \n",
    "   Displays the total number of processed questions and confirms that the results were saved successfully.\n",
    "\n",
    "---\n",
    "\n",
    "> This block should be placed at the end of the script to ensure proper standalone execution or when running inside a notebook with an `if __name__ == \"__main__\":` guard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:34:02.393948Z",
     "iopub.status.busy": "2025-07-31T09:34:02.392817Z",
     "iopub.status.idle": "2025-07-31T09:44:31.525160Z",
     "shell.execute_reply": "2025-07-31T09:44:31.524445Z",
     "shell.execute_reply.started": "2025-07-31T09:34:02.393913Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive test suite...\n",
      "Starting comprehensive test with 100 questions...\n",
      "================================================================================\n",
      "\n",
      "Question 1/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø±ÙŠØ§Ø¶\n",
      "Response Time: 0.498s\n",
      "RAM Usage: 0.018 GB\n",
      "CPU Usage: 28.4%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 1.0%\n",
      "================================================================================\n",
      "\n",
      "Question 2/100\n",
      "Category: history\n",
      "Language: arabic\n",
      "Input: Ù…Ù† Ù‡Ùˆ Ø£ÙˆÙ„ Ø®Ù„ÙŠÙØ© ÙÙŠ Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø®Ù„ÙŠÙØ© Ø§Ù„Ø£ÙˆÙ„ ÙƒØ§Ù† Ø£Ø¨Ùˆ Ø¨ÙƒØ± Ø§Ù„ØµØ¯ÙŠÙ‚ (Ø£Ø¨ÙˆØ­ÙØµ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ø«Ù…Ø§Ù† Ø§Ù„ØªÙŠÙ…ÙŠ Ø§Ù„Ù‚Ø±Ø´ÙŠ).\n",
      "Response Time: 3.22s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 2.0%\n",
      "================================================================================\n",
      "\n",
      "Question 3/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø£Ø·ÙˆÙ„ Ù†Ù‡Ø± ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù†Ù‡Ø± Ø§Ù„Ù†ÙŠÙ„.\n",
      "Response Time: 0.884s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 3.0%\n",
      "================================================================================\n",
      "\n",
      "Question 4/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: Ø§Ø°ÙƒØ± Ø®Ù…Ø³Ø© Ø£Ù†ÙˆØ§Ø¹ Ù…Ù† Ø§Ù„ÙÙˆØ§ÙƒÙ‡\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1. Ø§Ù„ØªÙØ§Ø­ \n",
      "2. Ø§Ù„Ù…ÙˆØ² \n",
      "3. Ø§Ù„Ø¨Ø±ØªÙ‚Ø§Ù„ \n",
      "4. Ø§Ù„ÙƒÙ…Ø«Ø±Ù‰ \n",
      "5. Ø§Ù„ÙØ±Ø§ÙˆÙ„Ø©\n",
      "Response Time: 4.325s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 4.0%\n",
      "================================================================================\n",
      "\n",
      "Question 5/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ù…ØµØ±ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ù‚Ø§Ù‡Ø±Ø©\n",
      "Response Time: 0.385s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 24.8%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 5.0%\n",
      "================================================================================\n",
      "\n",
      "Question 6/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: ÙƒÙ… Ø¹Ø¯Ø¯ Ø£ÙŠØ§Ù… Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù‡Ù†Ø§Ùƒ 366 ÙŠÙˆÙ… ÙÙŠ Ø§Ù„Ø³Ù†Ø© Ø§Ù„Ù…ÙŠÙ„Ø§Ø¯ÙŠØ© Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø³Ù†Ø© ÙƒØ¨ÙŠØ³Ø©ØŒ Ùˆ365 ÙŠÙˆÙ…Ù‹Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¹Ø§Ø¯ÙŠØ©.\n",
      "Response Time: 3.699s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 6.0%\n",
      "================================================================================\n",
      "\n",
      "Question 7/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø£ÙƒØ¨Ø± Ù…Ø­ÙŠØ· ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ù…Ø­ÙŠØ· Ø§Ù„Ù‡Ø§Ø¯Ø¦ Ù‡Ùˆ Ø£ÙƒØ¨Ø± Ø§Ù„Ù…Ø­ÙŠØ·Ø§ØªØŒ Ø¥Ø° ÙŠÙ…ØªØ¯ Ø¹Ù„Ù‰ Ù…Ø³Ø§Ø­Ø© 63.8 Ù…Ù„ÙŠÙˆÙ† Ù…ÙŠÙ„ Ù…Ø±Ø¨Ø¹ (166.0 Ù…Ù„ÙŠÙˆÙ† ÙƒÙŠÙ„ÙˆÙ…ØªØ± Ù…Ø±Ø¨Ø¹).\n",
      "Response Time: 4.378s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 7.0%\n",
      "================================================================================\n",
      "\n",
      "Question 8/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: Ù…Ù† Ø§Ø®ØªØ±Ø¹ Ø§Ù„Ù…ØµØ¨Ø§Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ØªÙ… Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ù…ØµØ¨Ø§Ø­ Ø§Ù„ÙƒÙ‡Ø±Ø¨Ø§Ø¦ÙŠ Ù…Ù† Ù‚Ø¨Ù„ ØªÙˆÙ…Ø§Ø³ Ø¥Ø¯ÙŠØ³ÙˆÙ† ÙÙŠ Ø¹Ø§Ù… 1879.\n",
      "Response Time: 2.376s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.8%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 8.0%\n",
      "================================================================================\n",
      "\n",
      "Question 9/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø¹Ù…Ù„Ø© Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ù…ØªØ­Ø¯Ø©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø¹Ù…Ù„Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù…Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù‡ÙŠ Ø§Ù„Ø¯Ø±Ù‡Ù… Ø§Ù„Ø¥Ù…Ø§Ø±Ø§ØªÙŠ (AED).\n",
      "Response Time: 2.378s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 9.0%\n",
      "================================================================================\n",
      "\n",
      "Question 10/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ÙƒÙ… Ø¹Ø¯Ø¯ Ù‚Ø§Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 7 Ù‚Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… ÙˆÙ‡ÙŠ Ø¢Ø³ÙŠØ§ ÙˆØ£ÙØ±ÙŠÙ‚ÙŠØ§ ÙˆØ£ÙˆØ±ÙˆØ¨Ø§ ÙˆØ£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø´Ù…Ø§Ù„ÙŠØ© ÙˆØ£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ© ÙˆØ£Ø³ØªØ±Ø§Ù„ÙŠØ§ ÙˆØ§Ù„Ù‚Ø§Ø±Ø© Ø§Ù„Ù‚Ø·Ø¨ÙŠØ© Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©.\n",
      "Response Time: 3.322s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 10.0%\n",
      "================================================================================\n",
      "\n",
      "Question 11/100\n",
      "Category: nature\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø­ÙŠÙˆØ§Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ÙÙ‡Ø¯ Ø£Ùˆ Ø§Ù„Ø´ÙŠØªØ§ (Ø¨Ø§Ù„Ø§Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Cheetah)ØŒ ÙˆÙŠØ¹Ø¯ Ø£Ø³Ø±Ø¹ Ø­ÙŠÙˆØ§Ù† Ø¨Ø±ÙŠ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ Ø­ÙŠØ« ØªØ¨Ù„Øº Ø³Ø±Ø¹ØªÙ‡ 112 Ù…ÙŠÙ„Ø§Ù‹ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø© (180 ÙƒÙŠÙ„ÙˆÙ…ØªØ±Ù‹Ø§ ÙÙŠ Ø§Ù„Ø³Ø§Ø¹Ø©).\n",
      "Response Time: 6.56s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 11.0%\n",
      "================================================================================\n",
      "\n",
      "Question 12/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ÙÙŠ Ø£ÙŠ Ø³Ù†Ø© ØªÙ… Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ØªÙ… Ø§Ø®ØªØ±Ø§Ø¹ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª Ø¹Ø§Ù… 1969Ù….\n",
      "Response Time: 1.345s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 12.0%\n",
      "================================================================================\n",
      "\n",
      "Question 13/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø£ÙƒØ¨Ø± Ø¯ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù… Ù…Ù† Ø­ÙŠØ« Ø§Ù„Ù…Ø³Ø§Ø­Ø©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø±ÙˆØ³ÙŠØ§\n",
      "Response Time: 0.381s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.2%\n",
      "GPU Usage: 48.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 13.0%\n",
      "================================================================================\n",
      "\n",
      "Question 14/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: ÙƒÙ… Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ø¸Ø§Ù… ÙÙŠ Ø¬Ø³Ù… Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø¨Ø§Ù„ØºØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø¨Ø´Ø± Ù„Ø¯ÙŠÙ‡Ù… 206 Ø¹Ø¸Ø§Ù… Ø¹Ù†Ø¯ Ø§Ù„ÙˆÙ„Ø§Ø¯Ø©, ÙˆÙ„ÙƒÙ† Ù‡Ø°Ø§ Ø§Ù„Ø±Ù‚Ù… ÙŠØªÙ†Ø§Ù‚Øµ Ø¥Ù„Ù‰ 206 Ø¹Ø¸Ø§Ù… Ù†Ø§Ø¶Ø¬Ø© ÙÙŠ Ø­ÙŠØ§Ø© Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ† Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø§Ù†Ø¯Ù…Ø§Ø¬\n",
      "Response Time: 3.918s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 14.0%\n",
      "================================================================================\n",
      "\n",
      "Question 15/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø£Ø¹Ù…Ù‚ Ø®Ù†Ø¯Ù‚ ÙÙŠ Ø§Ù„Ù…Ø­ÙŠØ·ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø®Ù†Ø¯Ù‚ Ù…Ø§Ø±ÙŠØ§Ù†Ø§ (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Mariana Trench) Ù‡Ùˆ Ø£Ø¹Ù…Ù‚ Ù†Ù‚Ø·Ø© ÙÙŠ Ø§Ù„Ù…Ø­ÙŠØ·Ø§ØªØŒ ÙˆÙÙŠ Ø°Ø§Øª Ø§Ù„ÙˆÙ‚Øª Ø£Ø¹Ù…Ù‚ Ù†Ù‚Ø·Ø© Ù…Ø¹Ø±ÙˆÙØ© ÙÙŠ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§; Ø¥Ø° ÙŠØ¨Ù„Øº Ø·ÙˆÙ„Ù‡ Ø­ÙˆØ§Ù„ÙŠ 36,070.9 ÙƒÙŠÙ„ÙˆÙ…ØªØ± (22,238 Ù…ÙŠÙ„).\n",
      "Response Time: 7.483s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 41.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 15.0%\n",
      "================================================================================\n",
      "\n",
      "Question 16/100\n",
      "Category: literature\n",
      "Language: arabic\n",
      "Input: Ù…Ù† ÙƒØªØ¨ Ø±ÙˆØ§ÙŠØ© Ù…Ø¦Ø© Ø¹Ø§Ù… Ù…Ù† Ø§Ù„Ø¹Ø²Ù„Ø©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ØºØ§Ø¨Ø±ÙŠÙŠÙ„ ØºØ§Ø±Ø«ÙŠØ§ Ù…Ø§Ø±ÙƒÙŠØ«\n",
      "Response Time: 1.508s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 16.0%\n",
      "================================================================================\n",
      "\n",
      "Question 17/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø£ØµØºØ± Ø¯ÙˆÙ„Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù…ÙˆÙ†ØªØ³Ø±Ø§Øª\n",
      "Response Time: 0.695s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 17.0%\n",
      "================================================================================\n",
      "\n",
      "Question 18/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: ÙƒÙ… Ø¹Ø¯Ø¯ Ø£Ø³Ù†Ø§Ù† Ø§Ù„Ø¥Ù†Ø³Ø§Ù† Ø§Ù„Ø¨Ø§Ù„ØºØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 32\n",
      "Response Time: 0.382s\n",
      "RAM Usage: -0.022 GB\n",
      "CPU Usage: 37.1%\n",
      "GPU Usage: 50.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 18.0%\n",
      "================================================================================\n",
      "\n",
      "Question 19/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø±Ù…Ø² Ø¹Ù†ØµØ± Ø§Ù„Ø°Ù‡Ø¨ ÙÙŠ Ø§Ù„Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ø¯ÙˆØ±ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø±Ù…Ø² Ø§Ù„ÙƒÙŠÙ…ÙŠØ§Ø¦ÙŠ Ù„Ù„Ø°Ù‡Ø¨ Ù‡Ùˆ Au.\n",
      "Response Time: 1.34s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 19.0%\n",
      "================================================================================\n",
      "\n",
      "Question 20/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ÙÙŠ Ø£ÙŠ Ù‚Ø§Ø±Ø© ØªÙ‚Ø¹ Ø¯ÙˆÙ„Ø© Ø§Ù„Ø¨Ø±Ø§Ø²ÙŠÙ„ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø£Ù…Ø±ÙŠÙƒØ§ Ø§Ù„Ø¬Ù†ÙˆØ¨ÙŠØ©\n",
      "Response Time: 0.536s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 20.0%\n",
      "================================================================================\n",
      "\n",
      "Question 21/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ ÙˆØ­Ø¯Ø© Ù‚ÙŠØ§Ø³ Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø¬ÙˆÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ÙŠØªÙ… Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ù„Ø¶ØºØ· Ø§Ù„Ø¬ÙˆÙŠ Ø¨ÙˆØ­Ø¯Ø§Øª Ø±Ø·Ù„ Ù„ÙƒÙ„ Ø¨ÙˆØµØ© Ù…Ø±Ø¨Ø¹Ø© Ø£Ùˆ ÙƒÙŠÙ„Ùˆ Ø¨Ø§Ø³ÙƒØ§Ù„.\n",
      "Response Time: 2.949s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 21.0%\n",
      "================================================================================\n",
      "\n",
      "Question 22/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: ÙƒÙ… Ø¹Ø¯Ø¯ Ø£Ù„ÙˆØ§Ù† Ù‚ÙˆØ³ Ù‚Ø²Ø­ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø³Ø¨Ø¹Ø©\n",
      "Response Time: 0.537s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.2%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 22.0%\n",
      "================================================================================\n",
      "\n",
      "Question 23/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø£Ø·ÙˆÙ„ Ø¬Ø¨Ù„ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø£Ø·ÙˆÙ„ Ø¬Ø¨Ù„ ÙÙŠ  Ø§Ù„Ø¹Ø§Ù„Ù… Ù‡Ùˆ Ø¬Ø¨Ù„ Ø¥ÙŠÙØ±Ø³Øª (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Mount Everest) ÙˆÙŠÙ†Ø·Ù‚ Ø¥ÙŠ-ÙÙŠØ±Ø³ØªØŒ ÙˆÙ‡Ùˆ Ø¬Ø¨Ù„ ÙŠÙ‚Ø¹ ÙÙŠ Ø³Ù„Ø³Ù„Ø© Ø¬Ø¨Ø§Ù„ Ø§Ù„Ù‡Ù…Ù„Ø§ÙŠØ§ Ø¹Ù„Ù‰ Ø­Ø¯ÙˆØ¯ Ø§Ù„ØµÙŠÙ† (Ø§Ù„Ø¥Ù‚Ù„ÙŠÙ… Ø§Ù„ØµÙŠÙ†ÙŠ: Ø§Ù„ØªØ¨Øª) ÙˆÙ†ÙŠØ¨Ø§Ù„ (Ø¯ÙˆØ§Ø±Ø§) ØŒ ÙˆÙŠØ¨Ù„Øº Ø§Ø±ØªÙØ§Ø¹Ù‡ 8,848 Ù…ØªØ±Ø§Ù‹ ÙÙˆÙ‚ Ø³...\n",
      "Response Time: 10.392s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 44.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 23.0%\n",
      "================================================================================\n",
      "\n",
      "Question 24/100\n",
      "Category: history\n",
      "Language: arabic\n",
      "Input: ÙÙŠ Ø£ÙŠ Ø³Ù†Ø© Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø§Ù„Ø«Ø§Ù†ÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø­Ø±Ø¨ Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© ÙÙŠ Ø¹Ø§Ù… 1945.\n",
      "Response Time: 1.517s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 24.0%\n",
      "================================================================================\n",
      "\n",
      "Question 25/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø£ÙƒØ¨Ø± ØµØ­Ø±Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØµØ­Ø±Ø§Ø¡ Ø§Ù„ÙƒØ¨Ø±Ù‰ Ù‡ÙŠ Ø£ÙƒØ¨Ø± Ø§Ù„ØµØ­Ø§Ø±Ù‰ Ø§Ù„Ø­Ø§Ø±Ø© ÙÙŠ Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ Ø­ÙŠØ« ØªØ­ØªÙ„ Ù…Ø³Ø§Ø­Ø© ØªÙ‚Ø¯Ø± Ø¨Ù†Ø­Ùˆ 3.6 Ù…Ù„ÙŠÙˆÙ† ÙƒÙŠÙ„ÙˆÙ…ØªØ± Ù…Ø±Ø¨Ø¹ ÙÙŠ Ø´Ù…Ø§Ù„ Ø£ÙØ±ÙŠÙ‚ÙŠØ§.\n",
      "Response Time: 4.411s\n",
      "RAM Usage: 0.006 GB\n",
      "CPU Usage: 26.3%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 25.0%\n",
      "================================================================================\n",
      "\n",
      "Question 26/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù„ÙŠ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø¹Ù„ÙˆÙ… Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ù‡Ø§ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© ØªÙ‚Ù„ÙŠØ¯ÙŠÙ‹Ø§ Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¨Ø´Ø±ÙŠ ØŒ Ù…Ø«Ù„ Ø§Ù„Ø¥Ø¯Ø±Ø§Ùƒ Ø§Ù„Ø¨ØµØ±ÙŠ ØŒ ÙˆØ§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… ØŒ ÙˆØ§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ØŒ ÙˆØªØ±Ø¬Ù…Ø© Ø§Ù„Ù„ØºØ© ØŒ ÙˆØ§Ù„ØªØ±Ø¬Ù…Ø©...\n",
      "Response Time: 15.032s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 26.0%\n",
      "================================================================================\n",
      "\n",
      "Question 27/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù‡Ùˆ ÙØ±Ø¹ Ù…Ù† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù„Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±Ø§Øª ÙˆØ§Ù„ØªÙ†Ø¨Ø¤Ø§Øª.\n",
      "Response Time: 5.129s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.3%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 27.0%\n",
      "================================================================================\n",
      "\n",
      "Question 28/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© ÙƒØªØ§Ø¨Ø© ØªØ¹Ù„ÙŠÙ…Ø§Øª ÙˆØªÙˆØ¬ÙŠÙ‡ Ø£ÙˆØ§Ù…Ø± Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ Ø£Ùˆ Ø§Ù„Ø±ÙˆØ¨ÙˆØª Ù…Ù† Ø£Ø¬Ù„ Ø¥ØªÙ…Ø§Ù… Ù…Ù‡Ù…Ø© Ù…Ø¹ÙŠÙ†Ø©. Ø¨ÙŠÙ†Ù…Ø§ Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡ÙŠ Ø¹Ù…Ù„ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ø±Ø§Ù…Ø¬ ÙˆØªØ·Ø¨ÙŠÙ‚Ø§Øª ØªØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ø¥ØªÙ…Ø§Ù… Ù…Ù‡Ø§Ù… Ù…Ø­Ø¯Ø¯Ø©ØŒ Ù…...\n",
      "Response Time: 7.908s\n",
      "RAM Usage: -0.019 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 28.0%\n",
      "================================================================================\n",
      "\n",
      "Question 29/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ©\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø´Ø¨ÙƒØ© Ø§Ù„Ø¹ØµØ¨ÙŠØ© Ù‡ÙŠ Ø¨Ù†ÙŠØ© ØªÙ‡Ø¯Ù Ø¥Ù„Ù‰ Ù…Ø­Ø§ÙƒØ§Ø© ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø¨Ø´Ø±ÙŠ. ØªØªÙƒÙˆÙ† Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¹Ù‚Ø¯ (Ø§Ù„Ø¹ØµØ¨ÙˆÙ†Ø§Øª) ÙˆØ§Ù„ØªÙŠ ØªØªØµÙ„ Ø¨Ø¨Ø¹Ø¶Ù‡Ø§ Ø¹Ø¨Ø± Ø£ÙˆØ²Ø§Ù† Ø§ØªØµØ§Ù„ ÙŠÙ…ÙƒÙ† ØªØ¹Ø¯ÙŠÙ„Ù‡Ø§. ØªØ³ØªØ®Ø¯Ù… Ø§Ù„Ø´Ø¨ÙƒØ§Øª Ø§Ù„Ø¹ØµØ¨ÙŠØ© ÙÙŠ Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø«Ù„ Ø§Ù„ØªØµÙ†...\n",
      "Response Time: 8.786s\n",
      "RAM Usage: 0.009 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 41.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 29.0%\n",
      "================================================================================\n",
      "\n",
      "Question 30/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù‡ÙŠ Ù†Ù…Ø§Ø°Ø¬ Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† Ø§Ù„Ø¯Ù…Ø§Øº Ø§Ù„Ø¨Ø´Ø±ÙŠ ØŒ ØªÙØ³ØªØ®Ø¯Ù… Ù„Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø©.\n",
      "Response Time: 3.287s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 27.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 30.0%\n",
      "================================================================================\n",
      "\n",
      "Question 31/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ÙŠÙ…ÙƒÙ† Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ØµÙˆØª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„ØªØ­Ù„ÙŠÙ„ Ø£ØµÙˆØ§Øª Ø§Ù„ÙƒÙ„Ù…Ø§Øª ØŒ Ø«Ù… ØªÙˆÙ„ÙŠØ¯ ØªÙ…Ø«ÙŠÙ„ Ø±Ù‚Ù…ÙŠ Ù„ØªÙ„Ùƒ Ø§Ù„ÙƒÙ„Ù…Ø§Øª. ØªØ³ØªØ®Ø¯Ù… Ù‡Ø°Ù‡ Ø§Ù„ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„Ø±Ù‚Ù…ÙŠØ© Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ø­ÙˆØ§Ø³ÙŠØ¨ Ù„ØªÙØ³ÙŠØ± ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© Ø¨Ù†ÙØ³ Ø§Ù„Ø·Ø±ÙŠ...\n",
      "Response Time: 6.678s\n",
      "RAM Usage: 0.012 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 31.0%\n",
      "================================================================================\n",
      "\n",
      "Question 32/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© (Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Natural Language Processing) ÙˆØªØ®ØªØµØ± Ø¨NLP Ù‡ÙŠ Ù…Ø¬Ø§Ù„ ÙŠÙ‡ØªÙ… Ø¨Ø§Ù„Ù‡Ù†Ø¯Ø³Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© ÙÙŠ ÙƒÙŠÙÙŠØ© ØªØµÙ…ÙŠÙ… Ø¨Ø±Ø§Ù…Ø¬Ø§ Ù„ÙÙ‡Ù…, ØªØ­Ù„ÙŠÙ„, ØªÙ„Ø®ÙŠØµ, ÙˆØªØ±Ø´ÙŠØ­ Ø§Ù„Ù†ØµÙˆØµ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.\n",
      "Response Time: 6.521s\n",
      "RAM Usage: -0.018 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 32.0%\n",
      "================================================================================\n",
      "\n",
      "Question 33/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø©\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¶Ø®Ù…Ø© Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø°Ø§Øª Ø§Ù„Ø­Ø¬Ù… Ø§Ù„ÙƒØ¨ÙŠØ± Ø¬Ø¯Ù‹Ø§ ÙˆØ§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ù„Ø¯Ø±Ø¬Ø© Ø£Ù†Ù‡ ÙŠØµØ¨Ø­ Ù…Ù† Ø§Ù„ØµØ¹Ø¨ Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯Ø§Ø© ÙˆØ§Ø­Ø¯Ø© ÙÙ‚Ø· Ù…Ù† Ø£Ø¯ÙˆØ§Øª Ø¥Ø¯Ø§Ø±Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ...\n",
      "Response Time: 6.183s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 33.0%\n",
      "================================================================================\n",
      "\n",
      "Question 34/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø­ÙˆØ³Ø¨Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ© Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…ÙÙ‡ÙˆÙ… Ø¹Ø§Ù… ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙŠ ÙŠØªÙ… Ù…Ù† Ø®Ù„Ø§Ù„Ù‡Ø§ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© (Software & Hardware) ÙƒØ®Ø¯Ù…Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø¨Ù…Ø§ ÙÙŠ Ø°Ù„Ùƒ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„ØªØ®Ø²ÙŠÙ†ÙŠØ© ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø§Øª Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙˆØ®Ø¯Ù…Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ§...\n",
      "Response Time: 7.491s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 34.0%\n",
      "================================================================================\n",
      "\n",
      "Question 35/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ÙƒÙŠÙ ØªØ¹Ù…Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙˆØµÙŠØ©ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ØªØ¹Ù…Ù„ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙˆØµÙŠÙ‡ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ù„ÙŠÙ„ ØªØ§Ø±ÙŠØ® ØªØµÙØ­ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„Ø§Ù†ØªØ±Ù†Øª Ùˆ Ø§Ø³ØªÙ†Ø§Ø¯Ø§Ù‹ Ø§Ù„Ù‰ Ù‡Ø°Ø§ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙŠÙ‚ØªØ±Ø­ Ø§Ù„Ù…ØªØµÙØ­ Ù…ÙˆØ§Ù‚Ø¹ Ø°Ø§Øª ØµÙ„Ù‡ Ø¨Ù…Ø§ Ø§Ù‡ØªÙ… Ø¨Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø³Ø§Ø¨Ù‚Ø§Ù‹ Ø¨Ø§Ù„Ø§Ø¶Ø§ÙÙ‡ Ø§Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙŠ ØªÙ‡Ù…Ù„ Ù…Ù† Ù‚Ø¨Ù„ Ø§Ù„Ù…ØªØµÙØ­ Ùˆ Ù„Ø§ ...\n",
      "Response Time: 8.786s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 35.0%\n",
      "================================================================================\n",
      "\n",
      "Question 36/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙˆØ§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ (AI) Ù‡Ùˆ Ù†Ù‡Ø¬ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„ØªØ®ØµØµØ§Øª Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ù‡Ø§ ØªÙ†ÙÙŠØ° Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ ØªØªØ·Ù„Ø¨ Ø§Ù„Ø¥Ø¯Ø±Ø§Ùƒ ÙˆØ§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ§Ù„Ø§ØªØµØ§Ù„. Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ (ML) Ù‡Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ© Ø§Ù„ØªÙŠ ØªÙ…ÙƒÙ†...\n",
      "Response Time: 12.717s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 36.0%\n",
      "================================================================================\n",
      "\n",
      "Question 37/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ©\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ÙŠØ© Ù‡ÙŠ Ù…Ø¬Ø§Ù„ Ù…Ù† Ù…Ø¬Ø§Ù„Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± ÙŠØ­Ø§ÙˆÙ„ ØªØ·ÙˆÙŠØ± Ø·Ø±Ù‚ Ù…Ø­ÙˆØ³Ø¨Ø© ØªØ³Ù…Ø­ Ù„Ø¬Ù‡Ø§Ø² Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ± Ø¨Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒØ§Ø¦Ù†Ø§Øª ÙˆÙÙ‡Ù… Ø¨ÙŠØ¦ØªÙ‡Ø§ 3D. \n",
      "\n",
      "ØªØªØ¶Ù…Ù† Ø¨Ø¹Ø¶ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø±Ø¤ÙŠØ© Ø§Ù„Ø­Ø§Ø³Ø¨ÙŠØ© Ø§Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„Ø§ØµØ·Ø¯Ø§Ù…Ø§Øª, ÙˆØ§Ù„Øª...\n",
      "Response Time: 11.092s\n",
      "RAM Usage: 0.006 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 37.0%\n",
      "================================================================================\n",
      "\n",
      "Question 38/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø¨Ù„ÙˆÙƒ ØªØ´ÙŠÙ†ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ø¨Ù„ÙˆÙƒ ØªØ´ÙŠÙ† Ù‡Ùˆ Ø¯ÙØªØ± Ø£Ø³ØªØ§Ø° Ø±Ù‚Ù…ÙŠ ØºÙŠØ± Ù‚Ø§Ø¨Ù„ Ù„Ù„ØªØºÙŠÙŠØ± Ù„Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª.\n",
      "Response Time: 2.475s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 38.0%\n",
      "================================================================================\n",
      "\n",
      "Question 39/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙÙŠ Ø§Ù„Ø·Ø¨ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ÙŠØ¹Ù…Ù„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø´ÙƒÙ„ Ù…ØªØ²Ø§ÙŠØ¯ ÙÙŠ Ù…Ø¬Ø§Ù„ Ø§Ù„Ø·Ø¨ ØŒ Ø¨Ø¯Ø¡Ù‹Ø§ Ù…Ù† ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø·Ø¨ÙŠØ© Ø¥Ù„Ù‰ Ø§ÙƒØªØ´Ø§Ù Ø§Ù„Ø£Ø¯ÙˆÙŠØ©. ÙŠÙ…ÙƒÙ† Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø£Ù† ÙŠØ³Ø§Ø¹Ø¯ Ø§Ù„Ø£Ø·Ø¨Ø§Ø¡ Ø¹Ù„Ù‰ ØªØ´Ø®ÙŠØµ ÙˆØ¹Ù„Ø§Ø¬ Ø§Ù„Ù…Ø±Ø¶Ù‰ Ø¨Ø´ÙƒÙ„ Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙˆÙƒÙØ§Ø¡Ø© Ù…Ù† Ø°ÙŠ Ù‚Ø¨Ù„.\n",
      "Response Time: 6.538s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 39.0%\n",
      "================================================================================\n",
      "\n",
      "Question 40/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØ¬Ù…ÙŠØ¹ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØªØ¬Ù…ÙŠØ¹ Ù‡Ùˆ Ø£Ø­Ø¯ Ø£Ù‡Ù… Ø£Ø³Ø§Ù„ÙŠØ¨ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠØŒ Ø­ÙŠØ« ÙŠØªÙ… ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù…Ø¹Ù‹Ø§ ÙˆÙØµÙ„Ù‡Ø§ Ø¹Ù† Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©. ÙˆÙŠØ¹ØªÙ…Ø¯ Ø°Ù„Ùƒ Ø¹Ù„Ù‰ Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª Ø¨ÙŠÙ† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ§Ø³ØªØ®Ù„Ø§ØµÙ‡Ø§ØŒ Ù…Ù…Ø§ ÙŠØ³Ø§Ø¹Ø¯ Ø¹Ù„Ù‰ ÙÙ‡Ù… ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ ...\n",
      "Response Time: 7.505s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 40.0%\n",
      "================================================================================\n",
      "\n",
      "Question 41/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø² Ù‡Ùˆ Ø¹Ù…Ù„ÙŠØ© ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø³Ù„ÙˆÙƒ Ø£Ùˆ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ù…Ù† Ø®Ù„Ø§Ù„ ØªÙ‚Ø¯ÙŠÙ… Ø§Ù„ØªØ¹Ø²ÙŠØ² Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ Ù…Ø«Ù„ Ø§Ù„Ù…ÙƒØ§ÙØ¢ØªØŒ Ø¨Ø¹Ø¯ Ø¥ØªÙ…Ø§Ù… Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ø¨Ù†Ø¬Ø§Ø­. ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ… ÙˆØ§Ù„ØªØ­ÙÙŠØ² Ø¨Ø´ÙƒÙ„ Ø´Ø§Ø¦Ø¹.\n",
      "Response Time: 6.032s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 41.0%\n",
      "================================================================================\n",
      "\n",
      "Question 42/100\n",
      "Category: programming\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Python Ùˆ JavaØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Python Ù‡ÙŠ Ù„ØºØ© Ø¨Ø±Ù…Ø¬Ø© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙ‰ ØªØ³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ÙƒØ§Ø¦Ù†ÙŠØ©ØŒ Ø¨ÙŠÙ†Ù…Ø§ Java Ù‡ÙŠ Ù„ØºØ© Ø¨Ø±Ù…Ø¬ÙŠØ© Ø°Ø§Øª Ù…Ø³ØªÙˆÙ‰ Ø¹Ø§Ù„ÙŠ ØªØ³ØªØ®Ø¯Ù… Ø£Ø³Ù„ÙˆØ¨ Ø¨Ø±Ù…Ø¬Ø© ØªØ¬Ø±ÙŠØ¯ÙŠ.\n",
      "\n",
      "\n",
      "Python Ø£Ø³Ù‡Ù„ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„Ø¨Ø±Ù…Ø¬Ø© Ù…Ù† JavaØŒ ÙˆÙ„ÙƒÙ† Java Ø£Ø³Ø±Ø¹ ÙˆØ£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©...\n",
      "Response Time: 7.975s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 42.0%\n",
      "================================================================================\n",
      "\n",
      "Question 43/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ù†Ø¸Ø§Ù… Ø¥Ø¯Ø§Ø±Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª (DBMS)  Ù‡Ùˆ Ø¹Ø¨Ø§Ø±Ø© Ø¹Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØªØ³Ø§Ø¹Ø¯ ÙÙŠ ØªÙ†ÙÙŠØ° Ø¹Ù…Ù„ÙŠØ§Øª ØªØ­ØªØ§Ø¬Ù‡Ø§ ÙÙŠ Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ Ù…Ø«Ù„ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ« ÙˆØ§Ù„Ø­Ø°Ù ÙˆØ§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªØµÙÙŠØ© ÙˆØ§Ù„Ø¥Ø­ØµØ§...\n",
      "Response Time: 8.299s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 43.0%\n",
      "================================================================================\n",
      "\n",
      "Question 44/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø³ÙŠØ¨Ø±Ø§Ù†ÙŠ Ø£Ùˆ Ø§Ù„Ø£Ù…Ù† Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ  Ù‡ÙŠ Ø§Ù„ØªÙ‚Ù†ÙŠØ§Øª ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø© Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø´Ø¨ÙƒØ§Øª ÙˆØ§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© Ù…Ù† Ø§Ù„Ù‡Ø¬Ù…Ø§Øª Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠØ© ÙˆØ§Ù„Ù‚Ø±ØµÙ†Ø©.\n",
      "Response Time: 3.767s\n",
      "RAM Usage: -0.025 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 44.0%\n",
      "================================================================================\n",
      "\n",
      "Question 45/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø¥Ù†ØªØ±Ù†Øª Ø§Ù„Ø£Ø´ÙŠØ§Ø¡\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„Ù€\"Ø£Ø´ÙŠØ§Ø¡Ù Ø§Ù„Ø¥Ù„ÙƒØªÙØ±ÙˆÙ†ÙŠØ©\" Ø§Ù„Ù…ÙØªØµÙ„Ø© Ø¨Ø§Ù„Ø§Ù†ØªØ±Ù†Øª ØªÙÙ…ÙƒÙÙ‘Ù† Ø§Ù„Ø¬Ù…Ø§Ø¯Ø§Øª Ù…Ù† Ø§Ù„ØªØ­ÙƒÙÙ„Ù‘ÙÙ… Ø¨Ø§Ù„Ø£Ø¬Ù‡Ø²Ø© Ø§Ù„Ø£Ø®Ø±Ù‰ ÙˆØ¥Ù†Ø²Ø§Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ù‡Ø§ Ø£Ùˆ Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø§Ù„Ø£ÙˆØ§Ù…Ø± Ù…Ù†Ù‡Ø§ Ø¹Ø¨Ø± Ø§Ù„Ø´Ø¨ÙƒØ©ØŒ Ù…Ø«Ø§Ù„ Ø°Ù„Ùƒ: Ø­Ø³Ø§Ø³Ø§Øª Ø¯Ø±Ø¬Ø© Ø§Ù„Ø­Ø±Ø§Ø±Ø© Ø§Ù„ØªÙŠ ØªÙÙ…ÙƒÙ† Ø§Ù„Ù…Ø¨Ø±Ø¯ Ø§Ù„Ø°Ùƒ...\n",
      "Response Time: 16.886s\n",
      "RAM Usage: 0.019 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 45.0%\n",
      "================================================================================\n",
      "\n",
      "Question 46/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªØµÙ†ÙŠÙ Ù‡ÙŠ Ø£Ø­Ø¯ ÙØ±ÙˆØ¹ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ ÙØ¦Ø§Øª Ù…Ø­Ø¯Ø¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒ Ù…Ø«Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙƒÙ€(Ø³Ø¨Ø§Ù…) Ø£Ùˆ (Ù„ÙŠØ³ Ø³Ø¨Ø§Ù…).\n",
      "Response Time: 6.029s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 46.0%\n",
      "================================================================================\n",
      "\n",
      "Question 47/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ÙƒÙŠÙ ÙŠØ¹Ù…Ù„ Ø§Ù„ØªØ´ÙÙŠØ± ÙÙŠ Ø§Ù„Ø­Ø§Ø³ÙˆØ¨ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØªØ´ÙÙŠØ± Ù‡Ùˆ Ø¹Ù…Ù„ÙŠØ© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø´ÙƒÙ„Ù‡Ø§ Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠ Ø§Ù„Ù…ÙÙ‡ÙˆÙ… Ù„Ù†Ø§ Ø¥Ù„Ù‰ Ø´ÙƒÙ„ ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ… Ø¨Ø­ÙŠØ« ÙŠØªØ¹Ø°Ø± Ø¹Ù„Ù‰ Ù…Ù† Ù„Ø§ ÙŠÙ…Ù„Ùƒ Ù…ÙØªØ§Ø­ Ø§Ù„Ø´ÙØ±Ø© ÙÙ‡Ù…Ù‡Ø§. ÙŠØ³ØªØ®Ø¯Ù… Ø§Ù„ØªØ´ÙÙŠØ± Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØªÙ… Ø¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª ÙˆØ§Ù„ØªÙŠ ØªÙƒÙˆÙ† Ø­Ø³...\n",
      "Response Time: 20.561s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 47.0%\n",
      "================================================================================\n",
      "\n",
      "Question 48/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡Ùˆ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„ØºÙŠØ± Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨ØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ØªØ¹Ù„Ù… ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹ Ù„Ù„Ø¥Ø´Ø±Ø§Ù Ù‡Ùˆ Ù†ÙˆØ¹ Ù…Ù† Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„Ø© Ø§Ù„ØªÙŠ ØªØ³ØªØ®Ø¯Ù… Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¥Ø¯Ø®Ø§Ù„ ÙÙŠ Ø´ÙƒÙ„ Ù…Ø³ØªÙ‚Ù„ Ø¹Ù† ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ùˆ Ø§Ù„Ø¥Ø´Ø±Ø§Ù Ø§Ù„Ø¨Ø´Ø±ÙŠ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±ØŒ ÙˆØªØ³ØªØ®Ø¯Ù… Ø¹Ø§Ø¯Ø©Ù‹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ø­Ù„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯...\n",
      "Response Time: 7.005s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 50.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 48.0%\n",
      "================================================================================\n",
      "\n",
      "Question 49/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: Ø§Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù‡Ùˆ Ù…Ø­Ø§ÙƒØ§Ø© ÙˆØ§Ù‚Ø¹ÙŠØ© Ù„Ø¨ÙŠØ¦Ø© Ø£Ùˆ ÙˆØ¶Ø¹ Ù…Ø¹ÙŠÙ†ØŒ ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„ÙƒÙ…Ø¨ÙŠÙˆØªØ±ØŒ ÙˆÙŠØªØ¶Ù…Ù† Ø¹Ø§Ø¯Ø©Ù‹ Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù„Ù„Ø¨ÙŠØ¦Ø§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ© ÙˆØ§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØ© Ù…Ø¹ Ø§Ù„Ø£Ø´ÙƒØ§Ù„ ÙˆØ§Ù„Ø£ØµÙˆØ§Øª ÙˆØ§Ù„Ø±Ø§Ø¦Ø­Ø©. ÙˆÙŠÙ‡Ø¯Ù Ø§Ù„ÙˆØ§Ù‚Ø¹ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¥Ù„Ù‰ ØªÙˆ...\n",
      "Response Time: 12.033s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 49.0%\n",
      "================================================================================\n",
      "\n",
      "Question 50/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: Ù…Ø§ Ù‡ÙŠ Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŸ\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„Ø¨Ø­Ø« Ù‡ÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø§Ù„ØªÙŠ ÙŠØ³ØªØ®Ø¯Ù…Ù‡Ø§ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø­Ù„ÙˆÙ„ Ø£Ùˆ Ø£Ù†Ù…Ø§Ø· ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©. ØªØ´Ù…Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª Ù…Ø«Ù„ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø«Ù†Ø§Ø¦ÙŠ ÙˆØ§Ù„Ø¨Ø­Ø« Ø¨ÙŠÙ† Ø§Ù„Ø¹Ù…Ù‚ ÙˆØ§Ù„Ø£ÙˆÙ„ (breadth-fi...\n",
      "Response Time: 8.319s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 50.0%\n",
      "================================================================================\n",
      "\n",
      "Question 51/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is artificial intelligence?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The field of computer science that gives computers the ability to perform tasks that require human intelligence, such as visual perception, speech recognition, decision-making, and language translatio...\n",
      "Response Time: 5.88s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.5%\n",
      "GPU Usage: 53.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 51.0%\n",
      "================================================================================\n",
      "\n",
      "Question 52/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the capital of France?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Paris\n",
      "Response Time: 0.548s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 52.0%\n",
      "================================================================================\n",
      "\n",
      "Question 53/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: Who invented the telephone?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Alexander Graham Bell\n",
      "Response Time: 0.869s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 53.0%\n",
      "================================================================================\n",
      "\n",
      "Question 54/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the largest planet in our solar system?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Answer: Jupiter is the biggest planet in the solar system. It's 1,320 kilometers wide.\n",
      "Response Time: 3.467s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 54.0%\n",
      "================================================================================\n",
      "\n",
      "Question 55/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: How many continents are there?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There are seven continents.\n",
      "Response Time: 1.2s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 55.0%\n",
      "================================================================================\n",
      "\n",
      "Question 56/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the speed of light?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The speed of the light in vacuum, approximately 299,792,458 metres per second (m/s), is a fundamental physical constant important in many areas of science.\n",
      "Response Time: 5.899s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 56.0%\n",
      "================================================================================\n",
      "\n",
      "Question 57/100\n",
      "Category: literature\n",
      "Language: english\n",
      "Input: Who wrote Romeo and Juliet?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: William Shakespeare\n",
      "Response Time: 0.712s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 57.0%\n",
      "================================================================================\n",
      "\n",
      "Question 58/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the chemical symbol for water?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: H2O\n",
      "Response Time: 0.872s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 58.0%\n",
      "================================================================================\n",
      "\n",
      "Question 59/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: In which year did World War II end?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1945\n",
      "Response Time: 0.553s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.3%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 59.0%\n",
      "================================================================================\n",
      "\n",
      "Question 60/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the smallest country in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Vatican City.\n",
      "Response Time: 0.868s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 60.0%\n",
      "================================================================================\n",
      "\n",
      "Question 61/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: How many bones are in the human body?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The adult human body contains 206 bones.\n",
      "Response Time: 1.687s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 61.0%\n",
      "================================================================================\n",
      "\n",
      "Question 62/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the longest river in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Nile River, which runs through Egypt, Sudan, Uganda, Democratic Republic of the Congo, Ethiopia, Eritrea, and other countries, is the world's longest river, measuring about 6,853 kilometers (4,258...\n",
      "Response Time: 8.314s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 62.0%\n",
      "================================================================================\n",
      "\n",
      "Question 63/100\n",
      "Category: art\n",
      "Language: english\n",
      "Input: Who painted the Mona Lisa?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Leonardo da Vinci is believed to have painted the painting.\n",
      "Response Time: 2.651s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 63.0%\n",
      "================================================================================\n",
      "\n",
      "Question 64/100\n",
      "Category: general\n",
      "Language: english\n",
      "Input: What is the currency of Japan?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The currency of the Japan is Japanese yen (Â¥)\n",
      "Response Time: 2.337s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 27.1%\n",
      "GPU Usage: 42.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 64.0%\n",
      "================================================================================\n",
      "\n",
      "Question 65/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: How many chambers does a human heart have?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 4\n",
      "Response Time: 0.548s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 65.0%\n",
      "================================================================================\n",
      "\n",
      "Question 66/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the tallest mountain in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The tallest peak on Earth is Mount Everest, standing at 8,848 meters (29,029 feet) above sea level.\n",
      "Response Time: 4.933s\n",
      "RAM Usage: -0.024 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 66.0%\n",
      "================================================================================\n",
      "\n",
      "Question 67/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: Who discovered penicillin?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Alexander Fleming\n",
      "Response Time: 0.873s\n",
      "RAM Usage: 0.012 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 67.0%\n",
      "================================================================================\n",
      "\n",
      "Question 68/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the largest ocean on Earth?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The largest ocean in the world is the Pacific Ocean, covering approximately 46% of the Earth's water surface area.\n",
      "Response Time: 4.117s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 68.0%\n",
      "================================================================================\n",
      "\n",
      "Question 69/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: In what year was the internet invented?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1991\n",
      "Response Time: 0.551s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 25.3%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 69.0%\n",
      "================================================================================\n",
      "\n",
      "Question 70/100\n",
      "Category: nature\n",
      "Language: english\n",
      "Input: What is the fastest animal on land?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The cheetah.\n",
      "Response Time: 1.04s\n",
      "RAM Usage: -0.026 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 70.0%\n",
      "================================================================================\n",
      "\n",
      "Question 71/100\n",
      "Category: general\n",
      "Language: english\n",
      "Input: How many days are in a leap year?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There are 366 days in a regular year, but there are only 365 days in an ordinary leap year. This is because one day is subtracted from February every four years.\n",
      "Response Time: 5.893s\n",
      "RAM Usage: -0.017 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 71.0%\n",
      "================================================================================\n",
      "\n",
      "Question 72/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the chemical symbol for gold?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Au\n",
      "Response Time: 0.549s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 72.0%\n",
      "================================================================================\n",
      "\n",
      "Question 73/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: Which planet is known as the Red Planet?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Mars is the red planet.\n",
      "Response Time: 1.523s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 73.0%\n",
      "================================================================================\n",
      "\n",
      "Question 74/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the largest desert in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The largest desert by size is the Antarctica, which covers 14 million square kilometers (5.4 million square miles). However, the Sahara Desert is the hottest desert on Earth with an average surface te...\n",
      "Response Time: 9.804s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 74.0%\n",
      "================================================================================\n",
      "\n",
      "Question 75/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: Who was the first person to walk on the moon?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Apollo 11 astronaut Neil Armstrong became the first human to step foot onto the lunar surface.\n",
      "Response Time: 3.311s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 75.0%\n",
      "================================================================================\n",
      "\n",
      "Question 76/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain machine learning algorithms\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Machine learning is a branch of artificial intelligence in which machines can learn without being explicitly programmed. Machine learning algorithms use data to train themselves, find patterns, and ma...\n",
      "Response Time: 10.256s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 76.0%\n",
      "================================================================================\n",
      "\n",
      "Question 77/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is deep learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Deep Learning is an application of artificial intelligence (AI) and machine learning which uses neural networks with multiple layers to model high level abstractions in data with the aim of making acc...\n",
      "Response Time: 6.864s\n",
      "RAM Usage: -0.018 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 77.0%\n",
      "================================================================================\n",
      "\n",
      "Question 78/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How do neural networks work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A neural network is a computer program based on the structure of the human brain, which has many interconnected areas called neurons.\n",
      "Response Time: 4.266s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 78.0%\n",
      "================================================================================\n",
      "\n",
      "Question 79/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is natural language processing?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Natural Language Processing (NLP) is the branch of Artificial Intelligence that gives computers ability to understand, interpret and generate human languages.\n",
      "Response Time: 4.751s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 79.0%\n",
      "================================================================================\n",
      "\n",
      "Question 80/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain computer vision technology\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Computer Vision is an interdisciplinary field concerned with the study of software and algorithms for acquiring, processing, analyzing and understanding visual information. The goal of this field is t...\n",
      "Response Time: 7.822s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 80.0%\n",
      "================================================================================\n",
      "\n",
      "Question 81/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is reinforcement learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Reinforcement Learning (RL) is an area of machine learning concerned with enabling agents to take actions based on rewards or punishments they receive over time. RL is the study of how to program an a...\n",
      "Response Time: 25.942s\n",
      "RAM Usage: -0.014 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 81.0%\n",
      "================================================================================\n",
      "\n",
      "Question 82/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How does speech recognition work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Speech recognition is the technology that enables devices to understand and process human speech into text. The process begins when sound waves from your voice reach the microphone, which converts tho...\n",
      "Response Time: 22.651s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 53.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 82.0%\n",
      "================================================================================\n",
      "\n",
      "Question 83/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What are recommendation algorithms?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Recommendation algorithms are used to find the best possible match for your search query.\n",
      "Response Time: 3.127s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.8%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 83.0%\n",
      "================================================================================\n",
      "\n",
      "Question 84/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain cloud computing\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Cloud computing is the on-demand availability of computer system resources without direct active management by the user.\n",
      "Response Time: 3.614s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 84.0%\n",
      "================================================================================\n",
      "\n",
      "Question 85/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is blockchain technology?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Blockchain is a type of distributed ledger technology (DLT) that was originally created to support the cryptocurrency bitcoin. It stores data in blocks, which are linked using cryptography. Once a blo...\n",
      "Response Time: 16.752s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 85.0%\n",
      "================================================================================\n",
      "\n",
      "Question 86/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: How do search engines work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Search engine algorithms use a variety of techniques to determine which websites are the best answer to a given query, and rank them accordingly. These include analyzing content, links, and other aspe...\n",
      "Response Time: 6.711s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 86.0%\n",
      "================================================================================\n",
      "\n",
      "Question 87/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is cybersecurity?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Cybersecurity is the protection of information technology systems, including hardware, software, and data from unauthorized access, use, disclosure, disruption, modification, or destruction while ensu...\n",
      "Response Time: 8.654s\n",
      "RAM Usage: -0.022 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 87.0%\n",
      "================================================================================\n",
      "\n",
      "Question 88/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain big data analytics\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Big data analytics is the process of collecting, organizing, analyzing, and interpreting large amounts of data to discover insights and patterns that can help businesses make informed decisions. This ...\n",
      "Response Time: 18.396s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 88.0%\n",
      "================================================================================\n",
      "\n",
      "Question 89/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is the Internet of Things?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: It's a network of physical objects, such as appliances and sensors, which can connect to the internet and exchange data.\n",
      "Response Time: 4.116s\n",
      "RAM Usage: 0.004 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 35.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 89.0%\n",
      "================================================================================\n",
      "\n",
      "Question 90/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: How does encryption work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Encryption is the process of converting plaintext into ciphertext, which is unreadable unless decrypted using an encryption key or passphrase. The encrypted data can then be transmitted across an inse...\n",
      "Response Time: 11.567s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 90.0%\n",
      "================================================================================\n",
      "\n",
      "Question 91/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is virtual reality?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Virtual Reality (VR) is an artificial, computer-generated simulation environment in which users can interact in a seemingly real or physical way. VR technology is used to create simulations of three d...\n",
      "Response Time: 13.515s\n",
      "RAM Usage: -0.017 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 91.0%\n",
      "================================================================================\n",
      "\n",
      "Question 92/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain quantum computing\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Quantum computing is a new field of computer science based on the idea that you can encode much more information into quantum mechanical systems such as atoms or photons, than classical physics allows...\n",
      "Response Time: 10.277s\n",
      "RAM Usage: 0.008 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 92.0%\n",
      "================================================================================\n",
      "\n",
      "Question 93/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What are clustering algorithms?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Clustering algorithms are unsupervised learning algorithms used for grouping data into different clusters based on their similarity to each other. The main goal of these algorithms is to find patterns...\n",
      "Response Time: 14.48s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 93.0%\n",
      "================================================================================\n",
      "\n",
      "Question 94/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How does supervised learning work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Answer:Supervised learning is a type of machine learning algorithm where the system is given a set of examples with known outputs (the supervision) and the algorithm learns to map inputs to their corr...\n",
      "Response Time: 6.872s\n",
      "RAM Usage: 0.011 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 94.0%\n",
      "================================================================================\n",
      "\n",
      "Question 95/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is unsupervised learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: unsupervised learning is a machine learning algorithm that finds patterns in data without being explicitly programmed to do so, using algorithms such as clustering.\n",
      "Response Time: 4.782s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 27.5%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 95.0%\n",
      "================================================================================\n",
      "\n",
      "Question 96/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain decision trees in machine learning\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A decision tree is a type of supervised learning algorithm used for classification or regression problems. It works by splitting the dataset into subsets using a series of conditions (or \"tests\") whic...\n",
      "Response Time: 22.149s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 96.0%\n",
      "================================================================================\n",
      "\n",
      "Question 97/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is feature engineering?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: It means to create new features from existing data to improve model performance.\n",
      "Response Time: 2.661s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 97.0%\n",
      "================================================================================\n",
      "\n",
      "Question 98/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How do convolutional neural networks work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A convolutional layer applies a set of learnable filters to volumes of the input data, usually 3x3 or 5x5 in size, and produces a single value as output. These filter responses are then passed through...\n",
      "Response Time: 11.091s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 98.0%\n",
      "================================================================================\n",
      "\n",
      "Question 99/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is transfer learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Transfer Learning is a very useful technique in which knowledge gained by training one model on an existing dataset can be used to train another model on a different but related task or dataset, signi...\n",
      "Response Time: 14.165s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 99.0%\n",
      "================================================================================\n",
      "\n",
      "Question 100/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain generative adversarial networks\n",
      "------------------------------------------------------------\n",
      "Response: generative adversarial network (GAN) is an artificial neural network used to generate new data, based on previously generated data. It consists of two competing neural networks called the generator an...\n",
      "Response Time: 19.192s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 100.0%\n",
      "================================================================================\n",
      "\n",
      "Test Summary:\n",
      "Success Rate: 100.0%\n",
      "Average Response Time: 6.261s\n",
      "Summary saved to: jais_13b_comprehensive_test_results_summary.json\n",
      "\n",
      "Test completed! Results saved to jais_13b_comprehensive_test_results.json\n",
      "\n",
      "Test completed successfully!\n",
      "Total questions processed: 100\n",
      "Results saved to JSON file with performance metrics.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generation configuration\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    print(\"Starting comprehensive test suite...\")\n",
    "    results = run_comprehensive_test(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=generation_config,\n",
    "        output_file=\"jais_13b_comprehensive_test_results.json\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTest completed successfully!\")\n",
    "    print(f\"Total questions processed: {len(results)}\")\n",
    "    print(\"Results saved to JSON file with performance metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
