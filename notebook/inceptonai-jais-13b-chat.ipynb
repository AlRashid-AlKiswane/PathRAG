{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📚 JAIS-13B Comprehensive Language Model Evaluation\n",
    "\n",
    "Welcome to the comprehensive evaluation notebook for the **JAIS-13B Chat Model** (`inceptionai/jais-13b-chat`).  \n",
    "This notebook is designed to **test, monitor, and benchmark** the performance of the model across 100 multilingual questions covering diverse categories.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 Purpose\n",
    "\n",
    "This notebook enables:\n",
    "- Automated response generation for structured input test cases.\n",
    "- Systematic tracking of system resource usage (CPU, RAM, GPU, and response time).\n",
    "- Output logging and summary statistics to evaluate:\n",
    "  - Model accuracy\n",
    "  - System efficiency\n",
    "  - Response consistency across categories and languages\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 Features\n",
    "\n",
    "- ✅ Hugging Face model + tokenizer loading\n",
    "- ✅ Bilingual prompt creation (Arabic + English)\n",
    "- ✅ Token-level configuration using `GenerationConfig`\n",
    "- ✅ Full test suite runner with real-time system monitoring\n",
    "- ✅ Summary JSON output with success/failure rates and performance metrics\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Test Categories\n",
    "\n",
    "The test cases span multiple domains including:\n",
    "- General knowledge\n",
    "- Science & technology\n",
    "- Language understanding\n",
    "- Multilingual capabilities (English and Arabic)\n",
    "\n",
    "---\n",
    "\n",
    "## 📈 Output Files\n",
    "\n",
    "The notebook produces:\n",
    "- `llm_test_results.json` – detailed logs of each model response and system performance\n",
    "- `llm_test_results_summary.json` – aggregate statistics and insights\n",
    "\n",
    "---\n",
    "\n",
    "> ⚠️ **Note:** This notebook is optimized for GPUs with ≥16GB memory. Ensure your environment has sufficient resources before running all 100 test cases.\n",
    "\n",
    "Happy benchmarking! 💡"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📦 Install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:34.338141Z",
     "iopub.status.busy": "2025-07-31T09:29:34.337812Z",
     "iopub.status.idle": "2025-07-31T09:29:37.375246Z",
     "shell.execute_reply": "2025-07-31T09:29:37.374284Z",
     "shell.execute_reply.started": "2025-07-31T09:29:34.338116Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -q GPUtil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📋 Define Test Cases List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:31:24.798354Z",
     "iopub.status.busy": "2025-07-31T09:31:24.798007Z",
     "iopub.status.idle": "2025-07-31T09:31:24.818159Z",
     "shell.execute_reply": "2025-07-31T09:31:24.817551Z",
     "shell.execute_reply.started": "2025-07-31T09:31:24.798333Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    # Arabic General Knowledge (25 questions)\n",
    "    {\"input\": \"ما هي عاصمة المملكة العربية السعودية؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"من هو أول خليفة في الإسلام؟\", \"language\": \"arabic\", \"category\": \"history\"},\n",
    "    {\"input\": \"ما هو أطول نهر في العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"اذكر خمسة أنواع من الفواكه\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"ما هي عاصمة مصر؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"كم عدد أيام السنة الميلادية؟\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"ما هو أكبر محيط في العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"من اخترع المصباح الكهربائي؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ما هي عملة دولة الإمارات العربية المتحدة؟\", \"language\": \"arabic\", \"category\": \"general\"},\n",
    "    {\"input\": \"كم عدد قارات العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ما هو الحيوان الأسرع في العالم؟\", \"language\": \"arabic\", \"category\": \"nature\"},\n",
    "    {\"input\": \"في أي سنة تم اختراع الإنترنت؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هي أكبر دولة في العالم من حيث المساحة؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"كم عدد العظام في جسم الإنسان البالغ؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ما هو أعمق خندق في المحيط؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"من كتب رواية مئة عام من العزلة؟\", \"language\": \"arabic\", \"category\": \"literature\"},\n",
    "    {\"input\": \"ما هي أصغر دولة في العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"كم عدد أسنان الإنسان البالغ؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ما هو رمز عنصر الذهب في الجدول الدوري؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"في أي قارة تقع دولة البرازيل؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"ما هي وحدة قياس الضغط الجوي؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"كم عدد ألوان قوس قزح؟\", \"language\": \"arabic\", \"category\": \"science\"},\n",
    "    {\"input\": \"ما هو أطول جبل في العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    {\"input\": \"في أي سنة انتهت الحرب العالمية الثانية؟\", \"language\": \"arabic\", \"category\": \"history\"},\n",
    "    {\"input\": \"ما هي أكبر صحراء في العالم؟\", \"language\": \"arabic\", \"category\": \"geography\"},\n",
    "    \n",
    "    # Arabic Technical/AI (25 questions)\n",
    "    {\"input\": \"اشرح لي مفهوم الذكاء الاصطناعي\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هو التعلم الآلي؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما الفرق بين البرمجة والبرمجة بالذكاء الاصطناعي؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"اشرح مفهوم الشبكات العصبية\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هي خوارزميات التعلم العميق؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"كيف يعمل نظام التعرف على الكلام؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هو معالجة اللغة الطبيعية؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"اشرح مفهوم البيانات الضخمة\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هي الحوسبة السحابية؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"كيف تعمل خوارزميات التوصية؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هو الفرق بين الذكاء الاصطناعي والتعلم الآلي؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"اشرح مفهوم الرؤية الحاسوبية\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هي تقنية البلوك تشين؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"كيف يعمل الذكاء الاصطناعي في الطب؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هي خوارزميات التجميع في التعلم الآلي؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"اشرح مفهوم التعلم المعزز\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"ما هو الفرق بين Python و Java؟\", \"language\": \"arabic\", \"category\": \"programming\"},\n",
    "    {\"input\": \"كيف يعمل نظام إدارة قواعد البيانات؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هي تقنيات الأمن السيبراني؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"اشرح مفهوم إنترنت الأشياء\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هي خوارزميات التصنيف؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"كيف يعمل التشفير في الحاسوب؟\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هو التعلم الغير المراقب؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    {\"input\": \"اشرح مفهوم الواقع الافتراضي\", \"language\": \"arabic\", \"category\": \"technology\"},\n",
    "    {\"input\": \"ما هي خوارزميات البحث في الذكاء الاصطناعي؟\", \"language\": \"arabic\", \"category\": \"ai\"},\n",
    "    \n",
    "    # English General Knowledge (25 questions)\n",
    "    {\"input\": \"What is artificial intelligence?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is the capital of France?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who invented the telephone?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    {\"input\": \"What is the largest planet in our solar system?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"How many continents are there?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"What is the speed of light?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"Who wrote Romeo and Juliet?\", \"language\": \"english\", \"category\": \"literature\"},\n",
    "    {\"input\": \"What is the chemical symbol for water?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"In which year did World War II end?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    {\"input\": \"What is the smallest country in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"How many bones are in the human body?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the longest river in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who painted the Mona Lisa?\", \"language\": \"english\", \"category\": \"art\"},\n",
    "    {\"input\": \"What is the currency of Japan?\", \"language\": \"english\", \"category\": \"general\"},\n",
    "    {\"input\": \"How many chambers does a human heart have?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the tallest mountain in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who discovered penicillin?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the largest ocean on Earth?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"In what year was the internet invented?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is the fastest animal on land?\", \"language\": \"english\", \"category\": \"nature\"},\n",
    "    {\"input\": \"How many days are in a leap year?\", \"language\": \"english\", \"category\": \"general\"},\n",
    "    {\"input\": \"What is the chemical symbol for gold?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"Which planet is known as the Red Planet?\", \"language\": \"english\", \"category\": \"science\"},\n",
    "    {\"input\": \"What is the largest desert in the world?\", \"language\": \"english\", \"category\": \"geography\"},\n",
    "    {\"input\": \"Who was the first person to walk on the moon?\", \"language\": \"english\", \"category\": \"history\"},\n",
    "    \n",
    "    # English Technical/AI (25 questions)\n",
    "    {\"input\": \"Explain machine learning algorithms\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is deep learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How do neural networks work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is natural language processing?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain computer vision technology\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is reinforcement learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How does speech recognition work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What are recommendation algorithms?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain cloud computing\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is blockchain technology?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"How do search engines work?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is cybersecurity?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Explain big data analytics\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is the Internet of Things?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"How does encryption work?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What is virtual reality?\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"Explain quantum computing\", \"language\": \"english\", \"category\": \"technology\"},\n",
    "    {\"input\": \"What are clustering algorithms?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How does supervised learning work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is unsupervised learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain decision trees in machine learning\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is feature engineering?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"How do convolutional neural networks work?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"What is transfer learning?\", \"language\": \"english\", \"category\": \"ai\"},\n",
    "    {\"input\": \"Explain generative adversarial networks\", \"language\": \"english\", \"category\": \"ai\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🛠️ Import Required Libraries and Setup Environment\n",
    "\n",
    "- `torch`: PyTorch library for tensor computations and model inference.  \n",
    "- `json`: For JSON serialization and deserialization.  \n",
    "- `time`: To measure time intervals.  \n",
    "- `psutil`: To monitor system resources like CPU and RAM usage.  \n",
    "- `GPUtil`: To track GPU usage and GPU memory consumption.  \n",
    "- `datetime`: For timestamping and time-related operations.  \n",
    "- `transformers`: Hugging Face Transformers for loading tokenizers and causal language models, and managing generation configurations.  \n",
    "- `warnings`: To suppress unwanted warning messages during execution.  \n",
    "- `huggingface_hub`: To authenticate and interact with Hugging Face Hub models.\n",
    "\n",
    "This setup is essential for running language model inference efficiently, while monitoring system performance and handling API authentication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:44.364190Z",
     "iopub.status.busy": "2025-07-31T09:29:44.363499Z",
     "iopub.status.idle": "2025-07-31T09:29:44.373960Z",
     "shell.execute_reply": "2025-07-31T09:29:44.373371Z",
     "shell.execute_reply.started": "2025-07-31T09:29:44.364158Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import GPUtil\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import warnings\n",
    "from huggingface_hub import login\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🖥️ System Monitoring Class\n",
    "\n",
    "This class tracks system resource usage during model inference or other processes:\n",
    "\n",
    "- **Attributes:**  \n",
    "  - `start_time`: Timestamp when monitoring begins.  \n",
    "  - `start_ram`: RAM usage in GB at start.  \n",
    "  - `start_cpu`: CPU usage % at start.  \n",
    "  - `start_gpu_memory`: GPU memory used at start (if GPU available).\n",
    "\n",
    "- **Methods:**  \n",
    "  - `start_monitoring()`: Records initial resource metrics (time, RAM, CPU, GPU).  \n",
    "  - `get_metrics()`: Calculates and returns resource usage since monitoring started, including:  \n",
    "    - Response time (seconds)  \n",
    "    - RAM usage (GB) delta  \n",
    "    - CPU usage (%)  \n",
    "    - GPU load (%) (if GPU detected)  \n",
    "    - GPU memory usage (MB) (if GPU detected)\n",
    "\n",
    "- **Usage:**  \n",
    "  Call `start_monitoring()` before starting the task, then `get_metrics()` after completion to gather resource stats.\n",
    "\n",
    "This helps track system performance and detect bottlenecks or resource constraints during model generation or heavy computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:29:48.480096Z",
     "iopub.status.busy": "2025-07-31T09:29:48.479456Z",
     "iopub.status.idle": "2025-07-31T09:29:48.487161Z",
     "shell.execute_reply": "2025-07-31T09:29:48.486422Z",
     "shell.execute_reply.started": "2025-07-31T09:29:48.480068Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# System monitoring class\n",
    "class SystemMonitor:\n",
    "    def __init__(self):\n",
    "        self.start_time = None\n",
    "        self.start_gpu_memory = None\n",
    "        self.start_ram = None\n",
    "        self.start_cpu = None\n",
    "        \n",
    "    def start_monitoring(self):\n",
    "        self.start_time = time.time()\n",
    "        self.start_ram = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "        self.start_cpu = psutil.cpu_percent()\n",
    "        \n",
    "        # GPU monitoring\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                self.start_gpu_memory = gpus[0].memoryUsed\n",
    "        except:\n",
    "            self.start_gpu_memory = None\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        end_time = time.time()\n",
    "        response_time = end_time - self.start_time\n",
    "        \n",
    "        current_ram = psutil.virtual_memory().used / (1024**3)  # GB\n",
    "        current_cpu = psutil.cpu_percent()\n",
    "        \n",
    "        ram_usage = current_ram - self.start_ram\n",
    "        cpu_usage = current_cpu\n",
    "        \n",
    "        gpu_usage = None\n",
    "        gpu_memory_used = None\n",
    "        \n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu = gpus[0]\n",
    "                gpu_usage = gpu.load * 100\n",
    "                gpu_memory_used = gpu.memoryUsed\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return {\n",
    "            \"response_time\": round(response_time, 3),\n",
    "            \"ram_usage_gb\": round(ram_usage, 3),\n",
    "            \"cpu_usage_percent\": round(cpu_usage, 2),\n",
    "            \"gpu_usage_percent\": round(gpu_usage, 2) if gpu_usage else None,\n",
    "            \"gpu_memory_mb\": gpu_memory_used if gpu_memory_used else None\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🔑 Authenticate Hugging Face Hub and Specify Model\n",
    "\n",
    "- Logs into Hugging Face Hub using your API token to access private or large models.  \n",
    "- Sets the variable `model_name` to specify the pretrained model to load (`inceptionai/jais-13b-chat`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:11:25.202578Z",
     "iopub.status.busy": "2025-07-31T09:11:25.201944Z",
     "iopub.status.idle": "2025-07-31T09:11:25.337763Z",
     "shell.execute_reply": "2025-07-31T09:11:25.337058Z",
     "shell.execute_reply.started": "2025-07-31T09:11:25.202553Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "login(\"hf___\")\n",
    "model_name = \"inceptionai/jais-13b-chat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📥 Load Tokenizer and Model from Hugging Face Hub\n",
    "\n",
    "- **Tokenizer Loading:**  \n",
    "  Uses `AutoTokenizer.from_pretrained()` to load the tokenizer for the specified `model_name`.  \n",
    "  - `trust_remote_code=True` allows execution of custom tokenizer code if provided by the model repository.  \n",
    "  - `padding_side=\"left\"` pads sequences on the left, useful for causal language models that predict tokens on the right.\n",
    "\n",
    "- **Model Loading:**  \n",
    "  Uses `AutoModelForCausalLM.from_pretrained()` to load the causal language model weights with these options:  \n",
    "  - `torch_dtype=torch.float16`: Loads model weights in half precision (float16) to reduce GPU memory usage and improve speed.  \n",
    "  - `device_map=\"auto\"`: Automatically places model layers on available GPU(s) and CPU to optimize memory and performance.  \n",
    "  - `low_cpu_mem_usage=True`: Reduces CPU RAM usage during model loading by using efficient strategies.\n",
    "\n",
    "**Print statements** indicate progress to help debug loading delays, especially for large models like `inceptionai/jais-13b-chat`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:12:46.872334Z",
     "iopub.status.busy": "2025-07-31T09:12:46.871566Z",
     "iopub.status.idle": "2025-07-31T09:21:41.009719Z",
     "shell.execute_reply": "2025-07-31T09:21:41.009081Z",
     "shell.execute_reply.started": "2025-07-31T09:12:46.872307Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d51024de72048dcb61c3b4e335a85be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/247 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022c3d74ef95476fa27804c4844fb82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/4.85M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86bc7016edf64246b66b5da9c9b48e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/131 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fdb683471b143b5853064f1f4821f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.26k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fcce18b8ab2449a9af16e4e2c8aaaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "configuration_jais.py:   0%|          | 0.00/6.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/inceptionai/jais-13b-chat:\n",
      "- configuration_jais.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a03447fc86694c279811442c25eedfd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modeling_jais.py:   0%|          | 0.00/68.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/inceptionai/jais-13b-chat:\n",
      "- modeling_jais.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "2025-07-31 09:12:56.622551: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753953176.872355      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753953176.960754      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1e27b5b4c441d1bcadda6b6e03aacc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/42.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15dd7cf0c748446cb7222aac5f59b636",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee856cdb495b42bdb75a7bd611f53e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00006.bin:   0%|          | 0.00/9.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6e4a6d356dc4075bbd2da41966f1bb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00006.bin:   0%|          | 0.00/9.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbba68675f1b4c8a805e6f8179c5ff45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00006.bin:   0%|          | 0.00/9.79G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da036363e0b04704ada08282787f5844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00006.bin:   0%|          | 0.00/9.75G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2819f5b4bcf545149790d2cccfd1a0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00006.bin:   0%|          | 0.00/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3ba646269d04f918e27b3a72f598f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00006.bin:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16eb5979ad3d40fab51cb9c25078d81b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/44.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18094345c84a40588d354a1d525b14d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d588c6f9b14547af9bfba7d4ec5a25d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"left\"\n",
    ")\n",
    "\n",
    "print(\"Loading model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,  # Use float16 for better memory efficiency\n",
    "    device_map=\"auto\",  # Automatically distribute across available GPUs\n",
    "    low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ⚙️ Ensure Tokenizer Has a Padding Token\n",
    "\n",
    "- Checks if the tokenizer lacks a `pad_token`.  \n",
    "- If missing, assigns the `eos_token` (end-of-sequence token) as the `pad_token`.  \n",
    "- This prevents errors during batch encoding and padding, especially for causal language models that may not have an explicit pad token by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:21:52.353722Z",
     "iopub.status.busy": "2025-07-31T09:21:52.352890Z",
     "iopub.status.idle": "2025-07-31T09:21:52.356943Z",
     "shell.execute_reply": "2025-07-31T09:21:52.356392Z",
     "shell.execute_reply.started": "2025-07-31T09:21:52.353694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Define Text Generation Configuration\n",
    "\n",
    "Sets up a `GenerationConfig` object to control how the model generates responses:\n",
    "\n",
    "- **Core Generation Parameters:**\n",
    "  - `max_new_tokens=512`: Limits generated output to 512 tokens.\n",
    "  - `min_length=10`: Ensures the output has at least 10 tokens.\n",
    "\n",
    "- **Sampling & Decoding Strategies:**\n",
    "  - `do_sample=True`: Enables sampling instead of deterministic decoding.\n",
    "  - `temperature=0.7`: Adds randomness to predictions (lower = more focused).\n",
    "  - `top_k=50`: Limits sampling to the top 50 most likely tokens.\n",
    "  - `top_p=0.9`: Nucleus sampling (includes tokens until 90% cumulative probability).\n",
    "  - `typical_p=0.95`: Typical decoding based on entropy of tokens.\n",
    "\n",
    "- **Repetition & Coherence Control:**\n",
    "  - `repetition_penalty=1.1`: Penalizes repeated tokens.\n",
    "  - `no_repeat_ngram_size=3`: Prevents repeating sequences of 3 or more tokens.\n",
    "\n",
    "- **Token IDs:**\n",
    "  - `pad_token_id`, `eos_token_id`, `bos_token_id`: Set based on tokenizer to handle special tokens properly.\n",
    "\n",
    "- **Beam Search (optional here):**\n",
    "  - `num_beams=1`: Disables beam search (acts as greedy decoding).\n",
    "  - `early_stopping=True`: Stops generation when an EOS token is reached.\n",
    "\n",
    "- **Advanced Controls (mostly unused or left as default):**\n",
    "  Includes penalties, constraints, suppression lists, and other fine-tuning options for advanced decoding control.\n",
    "\n",
    "> This configuration is passed to `model.generate()` to guide and fine-tune the response behavior of the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:21:55.948829Z",
     "iopub.status.busy": "2025-07-31T09:21:55.948559Z",
     "iopub.status.idle": "2025-07-31T09:21:55.955450Z",
     "shell.execute_reply": "2025-07-31T09:21:55.954718Z",
     "shell.execute_reply.started": "2025-07-31T09:21:55.948809Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "generation_config = GenerationConfig(\n",
    "    # Core parameters\n",
    "    max_new_tokens=512,           # Maximum number of tokens to generate\n",
    "    min_length=10,                # Minimum length of generated sequence\n",
    "    \n",
    "    # Sampling parameters\n",
    "    do_sample=True,               # Enable sampling\n",
    "    temperature=0.7,              # Controls randomness (0.1-2.0)\n",
    "    top_k=50,                     # Top-k sampling\n",
    "    top_p=0.9,                    # Nucleus sampling\n",
    "    typical_p=0.95,               # Typical sampling\n",
    "    \n",
    "    # Repetition control\n",
    "    repetition_penalty=1.1,       # Penalty for repetition\n",
    "    no_repeat_ngram_size=3,       # Prevent repeating n-grams\n",
    "    \n",
    "    # Special tokens\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    bos_token_id=tokenizer.bos_token_id,\n",
    "    \n",
    "    # Decoding strategy\n",
    "    num_beams=1,                  # Number of beams for beam search (1 = greedy)\n",
    "    early_stopping=True,          # Stop when EOS is generated\n",
    "    \n",
    "    # Additional parameters\n",
    "    length_penalty=1.0,           # Length penalty for beam search\n",
    "    diversity_penalty=0.0,        # Diversity penalty for diverse beam search\n",
    "    encoder_no_repeat_ngram_size=0,\n",
    "    bad_words_ids=None,\n",
    "    force_words_ids=None,\n",
    "    renormalize_logits=False,\n",
    "    constraints=None,\n",
    "    forced_bos_token_id=None,\n",
    "    forced_eos_token_id=None,\n",
    "    remove_invalid_values=False,\n",
    "    exponential_decay_length_penalty=None,\n",
    "    suppress_tokens=None,\n",
    "    begin_suppress_tokens=None,\n",
    "    forced_decoder_ids=None,\n",
    "    sequence_bias=None,\n",
    "    guidance_scale=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧪 Run Comprehensive Test Suite with System Monitoring\n",
    "\n",
    "This function runs a full set of LLM test cases with system performance tracking. It:\n",
    "\n",
    "- Iterates over a list of 100 predefined `test_cases`.\n",
    "- Logs input, category, and language for each question.\n",
    "- Monitors system resource usage (RAM, CPU, GPU, time) for each generation using `SystemMonitor`.\n",
    "- Calls a response generator (`generate_response_with_monitoring()`).\n",
    "- Captures and prints performance metrics for each question.\n",
    "- Appends results (response + metrics) to a list.\n",
    "- Catches and logs any exceptions per test case.\n",
    "- Saves final results to a `.json` file.\n",
    "- Generates a test summary JSON via `generate_test_summary()`.\n",
    "\n",
    "**Metrics Tracked Per Response:**\n",
    "- Response time in seconds\n",
    "- RAM usage in GB\n",
    "- CPU usage %\n",
    "- GPU usage % (if available)\n",
    "- GPU memory used in MB\n",
    "\n",
    "**Output:**\n",
    "- A detailed results file (`llm_test_results.json`)\n",
    "- A summary statistics file (`llm_test_results_summary.json`)\n",
    "\n",
    "This function helps benchmark LLM behavior across a variety of inputs while tracking compute efficiency and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:05.363322Z",
     "iopub.status.busy": "2025-07-31T09:32:05.362747Z",
     "iopub.status.idle": "2025-07-31T09:32:05.372148Z",
     "shell.execute_reply": "2025-07-31T09:32:05.371532Z",
     "shell.execute_reply.started": "2025-07-31T09:32:05.363300Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Enhanced test runner with system monitoring\n",
    "def run_comprehensive_test(model, tokenizer, generation_config, output_file=\"llm_test_results.json\"):\n",
    "    \"\"\"\n",
    "    Run comprehensive test suite with 100 questions and system monitoring\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    total_questions = len(test_cases)\n",
    "    \n",
    "    print(f\"Starting comprehensive test with {total_questions} questions...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\nQuestion {i}/{total_questions}\")\n",
    "        print(f\"Category: {test_case['category']}\")\n",
    "        print(f\"Language: {test_case['language']}\")\n",
    "        print(f\"Input: {test_case['input']}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Initialize system monitor\n",
    "        monitor = SystemMonitor()\n",
    "        monitor.start_monitoring()\n",
    "        \n",
    "        try:\n",
    "            # Generate response\n",
    "            response = generate_response_with_monitoring(\n",
    "                test_case['input'],\n",
    "                language=test_case['language'],\n",
    "                model=model,\n",
    "                tokenizer=tokenizer,\n",
    "                generation_config=generation_config\n",
    "            )\n",
    "            \n",
    "            # Get system metrics\n",
    "            metrics = monitor.get_metrics()\n",
    "            \n",
    "            # Store result\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"category\": test_case['category'],\n",
    "                \"language\": test_case['language'],\n",
    "                \"input\": test_case['input'],\n",
    "                \"response\": response,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": metrics,\n",
    "                \"status\": \"success\"\n",
    "            }\n",
    "            \n",
    "            print(f\"Response: {response[:200]}{'...' if len(response) > 200 else ''}\")\n",
    "            print(f\"Response Time: {metrics['response_time']}s\")\n",
    "            print(f\"RAM Usage: {metrics['ram_usage_gb']} GB\")\n",
    "            print(f\"CPU Usage: {metrics['cpu_usage_percent']}%\")\n",
    "            if metrics['gpu_usage_percent']:\n",
    "                print(f\"GPU Usage: {metrics['gpu_usage_percent']}%\")\n",
    "                print(f\"GPU Memory: {metrics['gpu_memory_mb']} MB\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            result = {\n",
    "                \"question_id\": i,\n",
    "                \"category\": test_case['category'],\n",
    "                \"language\": test_case['language'],\n",
    "                \"input\": test_case['input'],\n",
    "                \"response\": None,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"performance_metrics\": None,\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "            print(f\"Error: {str(e)}\")\n",
    "        \n",
    "        results.append(result)\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (i / total_questions) * 100\n",
    "        print(f\"Progress: {progress:.1f}%\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Save results to JSON file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    # Generate summary\n",
    "    generate_test_summary(results, output_file.replace('.json', '_summary.json'))\n",
    "    \n",
    "    print(f\"\\nTest completed! Results saved to {output_file}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 💬 Generate LLM Response with Bilingual Prompt and Performance Tracking\n",
    "\n",
    "This function generates a response using the JAIS model, configured with Hugging Face Transformers:\n",
    "\n",
    "- **Inputs:**\n",
    "  - `user_input`: The raw input question or text.\n",
    "  - `language`: The target response language (`en`, `ar`, etc.).\n",
    "  - `model`: A causal language model instance.\n",
    "  - `tokenizer`: Corresponding tokenizer.\n",
    "  - `generation_config`: Hugging Face `GenerationConfig` controlling decoding behavior.\n",
    "\n",
    "- **Steps:**\n",
    "  1. Creates a **bilingual prompt** using `create_bilingual_prompt()` to condition the model.\n",
    "  2. Tokenizes the prompt with truncation and padding (max 2048 tokens).\n",
    "  3. Moves inputs to the correct device (`model.device`, usually GPU).\n",
    "  4. Runs the model in **inference-only mode** (`torch.no_grad()` with `use_cache=True`) to speed up decoding.\n",
    "  5. Decodes only the **newly generated tokens**, excluding the input part.\n",
    "  6. Returns the cleaned, final model response as a string.\n",
    "\n",
    "> This function is called per test case and used in conjunction with `SystemMonitor` to track performance per generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:16.583909Z",
     "iopub.status.busy": "2025-07-31T09:32:16.583656Z",
     "iopub.status.idle": "2025-07-31T09:32:16.589352Z",
     "shell.execute_reply": "2025-07-31T09:32:16.588502Z",
     "shell.execute_reply.started": "2025-07-31T09:32:16.583892Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_response_with_monitoring(user_input, language, model, tokenizer, generation_config):\n",
    "    \"\"\"\n",
    "    Generate response with the JAIS model\n",
    "    \"\"\"\n",
    "    prompt = create_bilingual_prompt(user_input, language)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=2048,\n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            generation_config=generation_config,\n",
    "            use_cache=True\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(\n",
    "        outputs[0][inputs['input_ids'].shape[1]:],\n",
    "        skip_special_tokens=True,\n",
    "        clean_up_tokenization_spaces=True\n",
    "    )\n",
    "    \n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🌐 Create Bilingual Prompt for JAIS Model (English + Arabic)\n",
    "\n",
    "This function generates a language-aware prompt designed for the **JAIS multilingual LLM**, adapting to the user's input language:\n",
    "\n",
    "- **Inputs:**\n",
    "  - `user_message`: The user's raw question or message.\n",
    "  - `language`: Optional language override (`\"arabic\"` or `\"auto\"`). If `\"auto\"`, the function detects Arabic script based on Unicode ranges.\n",
    "\n",
    "- **Logic:**\n",
    "  - If the message is in Arabic (or explicitly specified), the prompt is constructed using Arabic system instructions.\n",
    "  - Otherwise, the prompt is constructed in English.\n",
    "  \n",
    "- **Prompt Format:**\n",
    "  - Starts with a role definition (`Instruction:` / `التعليمات:`).\n",
    "  - Followed by the user's question (`Question:` / `السؤال:`).\n",
    "  - Ends with an open-ended answer section (`Response:` / `الإجابة:`) for the model to complete.\n",
    "\n",
    "> This prompt design ensures consistent structure and bilingual adaptability, improving the performance of multilingual models like `jais-13b-chat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:32:41.028110Z",
     "iopub.status.busy": "2025-07-31T09:32:41.027493Z",
     "iopub.status.idle": "2025-07-31T09:32:41.032461Z",
     "shell.execute_reply": "2025-07-31T09:32:41.031773Z",
     "shell.execute_reply.started": "2025-07-31T09:32:41.028089Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_bilingual_prompt(user_message, language=\"auto\"):\n",
    "    \"\"\"\n",
    "    Create a bilingual prompt for JAIS model\n",
    "    \"\"\"\n",
    "    if language == \"arabic\" or (language == \"auto\" and any('\\u0600' <= char <= '\\u06FF' for char in user_message)):\n",
    "        system_prompt = \"\"\"أنت مساعد ذكي ومفيد يتحدث العربية والإنجليزية. أجب على الأسئلة بوضوح ودقة.\"\"\"\n",
    "        prompt = f\"\"\"### التعليمات:\n",
    "{system_prompt}\n",
    "\n",
    "### السؤال:\n",
    "{user_message}\n",
    "\n",
    "### الإجابة:\n",
    "\"\"\"\n",
    "    else:\n",
    "        system_prompt = \"\"\"You are a helpful and intelligent assistant that speaks both Arabic and English. Answer questions clearly and accurately.\"\"\"\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{system_prompt}\n",
    "\n",
    "### Question:\n",
    "{user_message}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 📊 Generate Test Summary and Performance Statistics\n",
    "\n",
    "This function analyzes the results from the comprehensive test run and produces a summary JSON report.\n",
    "\n",
    "---\n",
    "\n",
    "#### ✅ Key Outputs:\n",
    "- **Total questions tested**\n",
    "- **Number of successful and failed responses**\n",
    "- **Success rate (%)**\n",
    "- **Average system performance metrics:**\n",
    "  - Response time (s)\n",
    "  - RAM usage (GB)\n",
    "  - CPU usage (%)\n",
    "  - GPU usage (%) — if available\n",
    "  - GPU memory usage (MB) — if available\n",
    "- **Category-level success breakdown**\n",
    "- **Language-level success breakdown**\n",
    "- **Timestamp of test summary creation**\n",
    "\n",
    "---\n",
    "\n",
    "#### 🛠 How It Works:\n",
    "1. Filters for successful responses only.\n",
    "2. Calculates mean values for all available performance metrics.\n",
    "3. Groups results by category and language for detailed analysis.\n",
    "4. Outputs the summary to a JSON file (e.g., `llm_test_results_summary.json`).\n",
    "5. Displays key metrics to console.\n",
    "\n",
    "> This is the final reporting step in your testing pipeline. It allows performance comparison across different model versions, prompts, or environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-31T09:33:00.972081Z",
     "iopub.status.busy": "2025-07-31T09:33:00.971764Z",
     "iopub.status.idle": "2025-07-31T09:33:00.983027Z",
     "shell.execute_reply": "2025-07-31T09:33:00.982331Z",
     "shell.execute_reply.started": "2025-07-31T09:33:00.972032Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_test_summary(results, summary_file):\n",
    "    \"\"\"\n",
    "    Generate test summary statistics\n",
    "    \"\"\"\n",
    "    total_questions = len(results)\n",
    "    successful_responses = len([r for r in results if r['status'] == 'success'])\n",
    "    failed_responses = total_questions - successful_responses\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    successful_results = [r for r in results if r['status'] == 'success' and r['performance_metrics']]\n",
    "    \n",
    "    if successful_results:\n",
    "        avg_response_time = sum(r['performance_metrics']['response_time'] for r in successful_results) / len(successful_results)\n",
    "        avg_ram_usage = sum(r['performance_metrics']['ram_usage_gb'] for r in successful_results) / len(successful_results)\n",
    "        avg_cpu_usage = sum(r['performance_metrics']['cpu_usage_percent'] for r in successful_results) / len(successful_results)\n",
    "        \n",
    "        gpu_results = [r for r in successful_results if r['performance_metrics']['gpu_usage_percent']]\n",
    "        avg_gpu_usage = sum(r['performance_metrics']['gpu_usage_percent'] for r in gpu_results) / len(gpu_results) if gpu_results else None\n",
    "        avg_gpu_memory = sum(r['performance_metrics']['gpu_memory_mb'] for r in gpu_results) / len(gpu_results) if gpu_results else None\n",
    "    else:\n",
    "        avg_response_time = avg_ram_usage = avg_cpu_usage = avg_gpu_usage = avg_gpu_memory = None\n",
    "    \n",
    "    # Category breakdown\n",
    "    categories = {}\n",
    "    languages = {}\n",
    "    \n",
    "    for result in results:\n",
    "        cat = result['category']\n",
    "        lang = result['language']\n",
    "        \n",
    "        if cat not in categories:\n",
    "            categories[cat] = {'total': 0, 'success': 0}\n",
    "        if lang not in languages:\n",
    "            languages[lang] = {'total': 0, 'success': 0}\n",
    "        \n",
    "        categories[cat]['total'] += 1\n",
    "        languages[lang]['total'] += 1\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            categories[cat]['success'] += 1\n",
    "            languages[lang]['success'] += 1\n",
    "    \n",
    "    summary = {\n",
    "        \"test_overview\": {\n",
    "            \"total_questions\": total_questions,\n",
    "            \"successful_responses\": successful_responses,\n",
    "            \"failed_responses\": failed_responses,\n",
    "            \"success_rate\": round((successful_responses / total_questions) * 100, 2)\n",
    "        },\n",
    "        \"performance_metrics\": {\n",
    "            \"average_response_time\": round(avg_response_time, 3) if avg_response_time else None,\n",
    "            \"average_ram_usage_gb\": round(avg_ram_usage, 3) if avg_ram_usage else None,\n",
    "            \"average_cpu_usage_percent\": round(avg_cpu_usage, 2) if avg_cpu_usage else None,\n",
    "            \"average_gpu_usage_percent\": round(avg_gpu_usage, 2) if avg_gpu_usage else None,\n",
    "            \"average_gpu_memory_mb\": round(avg_gpu_memory, 2) if avg_gpu_memory else None\n",
    "        },\n",
    "        \"category_breakdown\": categories,\n",
    "        \"language_breakdown\": languages,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(summary_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"\\nTest Summary:\")\n",
    "    print(f\"Success Rate: {summary['test_overview']['success_rate']}%\")\n",
    "    print(f\"Average Response Time: {summary['performance_metrics']['average_response_time']}s\")\n",
    "    print(f\"Summary saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🚀 Main Execution: Run the Comprehensive Test Suite\n",
    "\n",
    "This is the entry point that triggers the full benchmarking process when the script is run directly.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔧 Steps Performed:\n",
    "\n",
    "1. **Define Generation Configuration:**  \n",
    "   Sets up generation parameters for the JAIS model:\n",
    "   - Sampling (`do_sample=True`) with temperature, top-k, and top-p.\n",
    "   - Repetition penalty and n-gram constraints to reduce redundancy.\n",
    "   - Special token IDs set from the tokenizer.\n",
    "   - `early_stopping=True` to stop on EOS token.\n",
    "\n",
    "2. **Run the Test Suite:**  \n",
    "   Calls `run_comprehensive_test()` with:\n",
    "   - The loaded model and tokenizer\n",
    "   - The configured generation settings\n",
    "   - An output file for saving detailed results with system metrics\n",
    "\n",
    "3. **Print Final Summary:**  \n",
    "   Displays the total number of processed questions and confirms that the results were saved successfully.\n",
    "\n",
    "---\n",
    "\n",
    "> This block should be placed at the end of the script to ensure proper standalone execution or when running inside a notebook with an `if __name__ == \"__main__\":` guard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-07-31T09:34:02.393948Z",
     "iopub.status.busy": "2025-07-31T09:34:02.392817Z",
     "iopub.status.idle": "2025-07-31T09:44:31.525160Z",
     "shell.execute_reply": "2025-07-31T09:44:31.524445Z",
     "shell.execute_reply.started": "2025-07-31T09:34:02.393913Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting comprehensive test suite...\n",
      "Starting comprehensive test with 100 questions...\n",
      "================================================================================\n",
      "\n",
      "Question 1/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هي عاصمة المملكة العربية السعودية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الرياض\n",
      "Response Time: 0.498s\n",
      "RAM Usage: 0.018 GB\n",
      "CPU Usage: 28.4%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 1.0%\n",
      "================================================================================\n",
      "\n",
      "Question 2/100\n",
      "Category: history\n",
      "Language: arabic\n",
      "Input: من هو أول خليفة في الإسلام؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الخليفة الأول كان أبو بكر الصديق (أبوحفص عبدالله بن عثمان التيمي القرشي).\n",
      "Response Time: 3.22s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 2.0%\n",
      "================================================================================\n",
      "\n",
      "Question 3/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هو أطول نهر في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: نهر النيل.\n",
      "Response Time: 0.884s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 3.0%\n",
      "================================================================================\n",
      "\n",
      "Question 4/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: اذكر خمسة أنواع من الفواكه\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1. التفاح \n",
      "2. الموز \n",
      "3. البرتقال \n",
      "4. الكمثرى \n",
      "5. الفراولة\n",
      "Response Time: 4.325s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 4.0%\n",
      "================================================================================\n",
      "\n",
      "Question 5/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هي عاصمة مصر؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: القاهرة\n",
      "Response Time: 0.385s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 24.8%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 5.0%\n",
      "================================================================================\n",
      "\n",
      "Question 6/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: كم عدد أيام السنة الميلادية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: هناك 366 يوم في السنة الميلادية إذا كانت سنة كبيسة، و365 يومًا إذا كانت عادية.\n",
      "Response Time: 3.699s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 6.0%\n",
      "================================================================================\n",
      "\n",
      "Question 7/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هو أكبر محيط في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: المحيط الهادئ هو أكبر المحيطات، إذ يمتد على مساحة 63.8 مليون ميل مربع (166.0 مليون كيلومتر مربع).\n",
      "Response Time: 4.378s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 7.0%\n",
      "================================================================================\n",
      "\n",
      "Question 8/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: من اخترع المصباح الكهربائي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: تم اختراع المصباح الكهربائي من قبل توماس إديسون في عام 1879.\n",
      "Response Time: 2.376s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.8%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 8.0%\n",
      "================================================================================\n",
      "\n",
      "Question 9/100\n",
      "Category: general\n",
      "Language: arabic\n",
      "Input: ما هي عملة دولة الإمارات العربية المتحدة؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: العملة الرسمية في الإمارات العربية هي الدرهم الإماراتي (AED).\n",
      "Response Time: 2.378s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 9.0%\n",
      "================================================================================\n",
      "\n",
      "Question 10/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: كم عدد قارات العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 7 قارات في العالم وهي آسيا وأفريقيا وأوروبا وأمريكا الشمالية وأمريكا الجنوبية وأستراليا والقارة القطبية الجنوبية.\n",
      "Response Time: 3.322s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 10.0%\n",
      "================================================================================\n",
      "\n",
      "Question 11/100\n",
      "Category: nature\n",
      "Language: arabic\n",
      "Input: ما هو الحيوان الأسرع في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الفهد أو الشيتا (بالانجليزية: Cheetah)، ويعد أسرع حيوان بري في العالم، حيث تبلغ سرعته 112 ميلاً في الساعة (180 كيلومترًا في الساعة).\n",
      "Response Time: 6.56s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 11.0%\n",
      "================================================================================\n",
      "\n",
      "Question 12/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: في أي سنة تم اختراع الإنترنت؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: تم اختراع الإنترنت عام 1969م.\n",
      "Response Time: 1.345s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 12.0%\n",
      "================================================================================\n",
      "\n",
      "Question 13/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هي أكبر دولة في العالم من حيث المساحة؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: روسيا\n",
      "Response Time: 0.381s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.2%\n",
      "GPU Usage: 48.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 13.0%\n",
      "================================================================================\n",
      "\n",
      "Question 14/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: كم عدد العظام في جسم الإنسان البالغ؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: البشر لديهم 206 عظام عند الولادة, ولكن هذا الرقم يتناقص إلى 206 عظام ناضجة في حياة البالغين بسبب الاندماج\n",
      "Response Time: 3.918s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 14.0%\n",
      "================================================================================\n",
      "\n",
      "Question 15/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هو أعمق خندق في المحيط؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: خندق ماريانا (بالإنجليزية: Mariana Trench) هو أعمق نقطة في المحيطات، وفي ذات الوقت أعمق نقطة معروفة في المجموعة الشمسية بأكملها; إذ يبلغ طوله حوالي 36,070.9 كيلومتر (22,238 ميل).\n",
      "Response Time: 7.483s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 41.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 15.0%\n",
      "================================================================================\n",
      "\n",
      "Question 16/100\n",
      "Category: literature\n",
      "Language: arabic\n",
      "Input: من كتب رواية مئة عام من العزلة؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: غابرييل غارثيا ماركيث\n",
      "Response Time: 1.508s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 16.0%\n",
      "================================================================================\n",
      "\n",
      "Question 17/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هي أصغر دولة في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: مونتسرات\n",
      "Response Time: 0.695s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 17.0%\n",
      "================================================================================\n",
      "\n",
      "Question 18/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: كم عدد أسنان الإنسان البالغ؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 32\n",
      "Response Time: 0.382s\n",
      "RAM Usage: -0.022 GB\n",
      "CPU Usage: 37.1%\n",
      "GPU Usage: 50.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 18.0%\n",
      "================================================================================\n",
      "\n",
      "Question 19/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: ما هو رمز عنصر الذهب في الجدول الدوري؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الرمز الكيميائي للذهب هو Au.\n",
      "Response Time: 1.34s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 19.0%\n",
      "================================================================================\n",
      "\n",
      "Question 20/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: في أي قارة تقع دولة البرازيل؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: أمريكا الجنوبية\n",
      "Response Time: 0.536s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 20.0%\n",
      "================================================================================\n",
      "\n",
      "Question 21/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: ما هي وحدة قياس الضغط الجوي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: يتم التعبير عن الضغط الجوي بوحدات رطل لكل بوصة مربعة أو كيلو باسكال.\n",
      "Response Time: 2.949s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 21.0%\n",
      "================================================================================\n",
      "\n",
      "Question 22/100\n",
      "Category: science\n",
      "Language: arabic\n",
      "Input: كم عدد ألوان قوس قزح؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: سبعة\n",
      "Response Time: 0.537s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.2%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 22.0%\n",
      "================================================================================\n",
      "\n",
      "Question 23/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هو أطول جبل في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: أطول جبل في  العالم هو جبل إيفرست (بالإنجليزية Mount Everest) وينطق إي-فيرست، وهو جبل يقع في سلسلة جبال الهملايا على حدود الصين (الإقليم الصيني: التبت) ونيبال (دوارا) ، ويبلغ ارتفاعه 8,848 متراً فوق س...\n",
      "Response Time: 10.392s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 44.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 23.0%\n",
      "================================================================================\n",
      "\n",
      "Question 24/100\n",
      "Category: history\n",
      "Language: arabic\n",
      "Input: في أي سنة انتهت الحرب العالمية الثانية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: انتهت الحرب العالمية في عام 1945.\n",
      "Response Time: 1.517s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 24.0%\n",
      "================================================================================\n",
      "\n",
      "Question 25/100\n",
      "Category: geography\n",
      "Language: arabic\n",
      "Input: ما هي أكبر صحراء في العالم؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الصحراء الكبرى هي أكبر الصحارى الحارة في العالم، حيث تحتل مساحة تقدر بنحو 3.6 مليون كيلومتر مربع في شمال أفريقيا.\n",
      "Response Time: 4.411s\n",
      "RAM Usage: 0.006 GB\n",
      "CPU Usage: 26.3%\n",
      "GPU Usage: 38.0%\n",
      "GPU Memory: 14713.0 MB\n",
      "Progress: 25.0%\n",
      "================================================================================\n",
      "\n",
      "Question 26/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: اشرح لي مفهوم الذكاء الاصطناعي\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الذكاء الاصطناعي هو فرع من علوم الحاسوب يشير إلى الأنظمة التي يمكنها تنفيذ المهام المرتبطة تقليديًا بالذكاء البشري ، مثل الإدراك البصري ، والتعرف على الكلام ، واتخاذ القرارات ، وترجمة اللغة ، والترجمة...\n",
      "Response Time: 15.032s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 26.0%\n",
      "================================================================================\n",
      "\n",
      "Question 27/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هو التعلم الآلي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: التعلم الآلي هو فرع من الذكاء الاصطناعي الذي يستخدم الخوارزميات لتحليل البيانات واستخلاص النماذج لتحديد أنماط في البيانات التي يمكن استخدامها لاتخاذ القرارات والتنبؤات.\n",
      "Response Time: 5.129s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.3%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 27.0%\n",
      "================================================================================\n",
      "\n",
      "Question 28/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما الفرق بين البرمجة والبرمجة بالذكاء الاصطناعي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: البرمجة هي عملية كتابة تعليمات وتوجيه أوامر لجهاز الحاسوب أو الروبوت من أجل إتمام مهمة معينة. بينما برمجة الذكاء الاصطناعي هي عملية إنشاء برامج وتطبيقات تعتمد على الذكاء الاصطناعي لإتمام مهام محددة، م...\n",
      "Response Time: 7.908s\n",
      "RAM Usage: -0.019 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 28.0%\n",
      "================================================================================\n",
      "\n",
      "Question 29/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم الشبكات العصبية\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الشبكة العصبية هي بنية تهدف إلى محاكاة وظائف الدماغ البشري. تتكون من مجموعة من العقد (العصبونات) والتي تتصل ببعضها عبر أوزان اتصال يمكن تعديلها. تستخدم الشبكات العصبية في الكثير من التطبيقات مثل التصن...\n",
      "Response Time: 8.786s\n",
      "RAM Usage: 0.009 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 41.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 29.0%\n",
      "================================================================================\n",
      "\n",
      "Question 30/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هي خوارزميات التعلم العميق؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: هي نماذج حاسوبية مستوحاة من الدماغ البشري ، تُستخدم لحل مشاكل الذكاء الاصطناعي وتعلم الآلة.\n",
      "Response Time: 3.287s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 27.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 30.0%\n",
      "================================================================================\n",
      "\n",
      "Question 31/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: كيف يعمل نظام التعرف على الكلام؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: يمكن لأنظمة التعرف على الصوت استخدام خوارزميات التعلم الآلي لتحليل أصوات الكلمات ، ثم توليد تمثيل رقمي لتلك الكلمات. تستخدم هذه التمثيلات الرقمية من قبل الحواسيب لتفسير كلمات اللغة الطبيعية بنفس الطري...\n",
      "Response Time: 6.678s\n",
      "RAM Usage: 0.012 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 31.0%\n",
      "================================================================================\n",
      "\n",
      "Question 32/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هو معالجة اللغة الطبيعية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: معالجة اللغة الطبيعية (بالإنجليزية: Natural Language Processing) وتختصر بNLP هي مجال يهتم بالهندسة البرمجية في كيفية تصميم برامجا لفهم, تحليل, تلخيص, وترشيح النصوص باللغة الطبيعية.\n",
      "Response Time: 6.521s\n",
      "RAM Usage: -0.018 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 32.0%\n",
      "================================================================================\n",
      "\n",
      "Question 33/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم البيانات الضخمة\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: البيانات الضخمة هي مجموعة من البيانات ذات الحجم الكبير جدًا والمعقدة لدرجة أنه يصبح من الصعب معالجتها باستخدام أداة واحدة فقط من أدوات إدارة قواعد البيانات أو باستخدام تطبيقات معالجة البيانات التقليدي...\n",
      "Response Time: 6.183s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 33.0%\n",
      "================================================================================\n",
      "\n",
      "Question 34/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ما هي الحوسبة السحابية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الحوسبة السحابية عبارة عن مفهوم عام يشير إلى العملية التي يتم من خلالها تقديم الموارد الحاسوبية (Software & Hardware) كخدمات عبر الإنترنت، بما في ذلك المساحة التخزينية ومعالجات المعلومات وخدمات الشبكا...\n",
      "Response Time: 7.491s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 34.0%\n",
      "================================================================================\n",
      "\n",
      "Question 35/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: كيف تعمل خوارزميات التوصية؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: تعمل خوارزميات التوصيه عن طريق تحليل تاريخ تصفح المستخدم للانترنت و استناداً الى هذا التاريخ يقترح المتصفح مواقع ذات صله بما اهتم به المستخدم سابقاً بالاضافه الى المواقع التي تهمل من قبل المتصفح و لا ...\n",
      "Response Time: 8.786s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 35.0%\n",
      "================================================================================\n",
      "\n",
      "Question 36/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هو الفرق بين الذكاء الاصطناعي والتعلم الآلي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الذكاء الاصطناعي (AI) هو نهج متعدد التخصصات لتصميم الأجهزة والبرمجيات التي يمكنها تنفيذ المهام التي تتطلب الإدراك والاستدلال والاتصال. بينما التعلم الآلي (ML) هو مجموعة من التقنيات الإحصائية التي تمكن...\n",
      "Response Time: 12.717s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 36.0%\n",
      "================================================================================\n",
      "\n",
      "Question 37/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم الرؤية الحاسوبية\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الرؤية الحاسوبية هي مجال من مجالات معالجة الصور يحاول تطوير طرق محوسبة تسمح لجهاز الكمبيوتر بالتعرف على الكائنات وفهم بيئتها 3D. \n",
      "\n",
      "تتضمن بعض التطبيقات الشائعة للرؤية الحاسبية الكشف عن الاصطدامات, والت...\n",
      "Response Time: 11.092s\n",
      "RAM Usage: 0.006 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 37.0%\n",
      "================================================================================\n",
      "\n",
      "Question 38/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ما هي تقنية البلوك تشين؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: البلوك تشين هو دفتر أستاذ رقمي غير قابل للتغيير للمعاملات.\n",
      "Response Time: 2.475s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 38.0%\n",
      "================================================================================\n",
      "\n",
      "Question 39/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: كيف يعمل الذكاء الاصطناعي في الطب؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: يعمل الذكاء الاصطناعي بشكل متزايد في مجال الطب ، بدءًا من تحليل الصور الطبية إلى اكتشاف الأدوية. يمكن للذكاء الاصطناعي أن يساعد الأطباء على تشخيص وعلاج المرضى بشكل أكثر دقة وكفاءة من ذي قبل.\n",
      "Response Time: 6.538s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 39.0%\n",
      "================================================================================\n",
      "\n",
      "Question 40/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هي خوارزميات التجميع في التعلم الآلي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: التجميع هو أحد أهم أساليب التعلم الآلي، حيث يتم تجميع الأشياء المتشابهة معًا وفصلها عن الأشياء المختلفة. ويعتمد ذلك على إيجاد التشابهات بين البيانات واستخلاصها، مما يساعد على فهم وتحليل البيانات بشكل ...\n",
      "Response Time: 7.505s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 40.0%\n",
      "================================================================================\n",
      "\n",
      "Question 41/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم التعلم المعزز\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: التعلم المعزز هو عملية تحسين السلوك أو الأداء من خلال تقديم التعزيز الإيجابي مثل المكافآت، بعد إتمام المهمة المطلوبة بنجاح. يتم استخدام هذه الطريقة في التدريب والتعليم والتحفيز بشكل شائع.\n",
      "Response Time: 6.032s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 41.0%\n",
      "================================================================================\n",
      "\n",
      "Question 42/100\n",
      "Category: programming\n",
      "Language: arabic\n",
      "Input: ما هو الفرق بين Python و Java؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Python هي لغة برمجة عالية المستوى تستخدم أسلوب البرمجة الكائنية، بينما Java هي لغة برمجية ذات مستوى عالي تستخدم أسلوب برمجة تجريدي.\n",
      "\n",
      "\n",
      "Python أسهل في التعلم والبرمجة من Java، ولكن Java أسرع وأكثر كفاءة...\n",
      "Response Time: 7.975s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 42.0%\n",
      "================================================================================\n",
      "\n",
      "Question 43/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: كيف يعمل نظام إدارة قواعد البيانات؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: نظام إدارة قواعد بيانات (DBMS)  هو عبارة عن مجموعة من البرمجيات التي تستخدم في قاعدة البيانات وتساعد في تنفيذ عمليات تحتاجها في التعامل مع البيانات، مثل الإنشاء والتحديث والحذف والبحث والتصفية والإحصا...\n",
      "Response Time: 8.299s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 43.0%\n",
      "================================================================================\n",
      "\n",
      "Question 44/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: ما هي تقنيات الأمن السيبراني؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: تقنيات الأمن السيبراني أو الأمن الإلكتروني  هي التقنيات والأدوات المستخدمة لحماية الشبكات والأجهزة الإلكترونية من الهجمات الإلكترونية والقرصنة.\n",
      "Response Time: 3.767s\n",
      "RAM Usage: -0.025 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 44.0%\n",
      "================================================================================\n",
      "\n",
      "Question 45/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم إنترنت الأشياء\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الـ\"أشياءِ الإلكتُرونية\" المُتصلة بالانترنت تُمكِّن الجمادات من التحكُلُّم بالأجهزة الأخرى وإنزال البيانات منها أو استقبال الأوامر منها عبر الشبكة، مثال ذلك: حساسات درجة الحرارة التي تُمكن المبرد الذك...\n",
      "Response Time: 16.886s\n",
      "RAM Usage: 0.019 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 45.0%\n",
      "================================================================================\n",
      "\n",
      "Question 46/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هي خوارزميات التصنيف؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: خوارزميات التصنيف هي أحد فروع التعلم الآلي التي تستخدم لتصنيف البيانات إلى فئات محددة مسبقًا، مثل تصنيف البريد الإلكتروني كـ(سبام) أو (ليس سبام).\n",
      "Response Time: 6.029s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14733.0 MB\n",
      "Progress: 46.0%\n",
      "================================================================================\n",
      "\n",
      "Question 47/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: كيف يعمل التشفير في الحاسوب؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: التشفير هو عملية تحويل البيانات من شكلها الطبيعي المفهوم لنا إلى شكل غير مفهوم بحيث يتعذر على من لا يملك مفتاح الشفرة فهمها. يستخدم التشفير لحماية المعلومات التي يتم إرسالها عبر الإنترنت والتي تكون حس...\n",
      "Response Time: 20.561s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 47.0%\n",
      "================================================================================\n",
      "\n",
      "Question 48/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هو التعلم الغير المراقب؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: التعلم غير الخاضع للإشراف هو نوع من خوارزميات تعلم الآلة التي تستخدم بيانات الإدخال في شكل مستقل عن تسميات البيانات أو الإشراف البشري المباشر، وتستخدم عادةً البيانات الموجودة مسبقًا لحل المشاكل الجديد...\n",
      "Response Time: 7.005s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 50.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 48.0%\n",
      "================================================================================\n",
      "\n",
      "Question 49/100\n",
      "Category: technology\n",
      "Language: arabic\n",
      "Input: اشرح مفهوم الواقع الافتراضي\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: الواقع الافتراضي هو محاكاة واقعية لبيئة أو وضع معين، يتم إنشاؤه بواسطة الكمبيوتر، ويتضمن عادةً المحاكاة الكاملة للبيئات الطبيعية والاصطناعية مع الأشكال والأصوات والرائحة. ويهدف الواقع الافتراضي إلى تو...\n",
      "Response Time: 12.033s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 49.0%\n",
      "================================================================================\n",
      "\n",
      "Question 50/100\n",
      "Category: ai\n",
      "Language: arabic\n",
      "Input: ما هي خوارزميات البحث في الذكاء الاصطناعي؟\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: خوارزميات البحث هي مجموعة من الخوارزميات التي يستخدمها الذكاء الاصطناعي للبحث عن حلول أو أنماط في البيانات الكبيرة. تشمل هذه الخوارزميات خوارزميات مثل البحث الثنائي والبحث بين العمق والأول (breadth-fi...\n",
      "Response Time: 8.319s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 50.0%\n",
      "================================================================================\n",
      "\n",
      "Question 51/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is artificial intelligence?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The field of computer science that gives computers the ability to perform tasks that require human intelligence, such as visual perception, speech recognition, decision-making, and language translatio...\n",
      "Response Time: 5.88s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.5%\n",
      "GPU Usage: 53.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 51.0%\n",
      "================================================================================\n",
      "\n",
      "Question 52/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the capital of France?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Paris\n",
      "Response Time: 0.548s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 52.0%\n",
      "================================================================================\n",
      "\n",
      "Question 53/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: Who invented the telephone?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Alexander Graham Bell\n",
      "Response Time: 0.869s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 53.0%\n",
      "================================================================================\n",
      "\n",
      "Question 54/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the largest planet in our solar system?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Answer: Jupiter is the biggest planet in the solar system. It's 1,320 kilometers wide.\n",
      "Response Time: 3.467s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 54.0%\n",
      "================================================================================\n",
      "\n",
      "Question 55/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: How many continents are there?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There are seven continents.\n",
      "Response Time: 1.2s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 55.0%\n",
      "================================================================================\n",
      "\n",
      "Question 56/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the speed of light?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The speed of the light in vacuum, approximately 299,792,458 metres per second (m/s), is a fundamental physical constant important in many areas of science.\n",
      "Response Time: 5.899s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 56.0%\n",
      "================================================================================\n",
      "\n",
      "Question 57/100\n",
      "Category: literature\n",
      "Language: english\n",
      "Input: Who wrote Romeo and Juliet?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: William Shakespeare\n",
      "Response Time: 0.712s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 57.0%\n",
      "================================================================================\n",
      "\n",
      "Question 58/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the chemical symbol for water?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: H2O\n",
      "Response Time: 0.872s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 45.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 58.0%\n",
      "================================================================================\n",
      "\n",
      "Question 59/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: In which year did World War II end?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1945\n",
      "Response Time: 0.553s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.3%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 59.0%\n",
      "================================================================================\n",
      "\n",
      "Question 60/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the smallest country in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Vatican City.\n",
      "Response Time: 0.868s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 60.0%\n",
      "================================================================================\n",
      "\n",
      "Question 61/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: How many bones are in the human body?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The adult human body contains 206 bones.\n",
      "Response Time: 1.687s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 61.0%\n",
      "================================================================================\n",
      "\n",
      "Question 62/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the longest river in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The Nile River, which runs through Egypt, Sudan, Uganda, Democratic Republic of the Congo, Ethiopia, Eritrea, and other countries, is the world's longest river, measuring about 6,853 kilometers (4,258...\n",
      "Response Time: 8.314s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 40.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 62.0%\n",
      "================================================================================\n",
      "\n",
      "Question 63/100\n",
      "Category: art\n",
      "Language: english\n",
      "Input: Who painted the Mona Lisa?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Leonardo da Vinci is believed to have painted the painting.\n",
      "Response Time: 2.651s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 63.0%\n",
      "================================================================================\n",
      "\n",
      "Question 64/100\n",
      "Category: general\n",
      "Language: english\n",
      "Input: What is the currency of Japan?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The currency of the Japan is Japanese yen (¥)\n",
      "Response Time: 2.337s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 27.1%\n",
      "GPU Usage: 42.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 64.0%\n",
      "================================================================================\n",
      "\n",
      "Question 65/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: How many chambers does a human heart have?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 4\n",
      "Response Time: 0.548s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 65.0%\n",
      "================================================================================\n",
      "\n",
      "Question 66/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the tallest mountain in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The tallest peak on Earth is Mount Everest, standing at 8,848 meters (29,029 feet) above sea level.\n",
      "Response Time: 4.933s\n",
      "RAM Usage: -0.024 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 66.0%\n",
      "================================================================================\n",
      "\n",
      "Question 67/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: Who discovered penicillin?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Alexander Fleming\n",
      "Response Time: 0.873s\n",
      "RAM Usage: 0.012 GB\n",
      "CPU Usage: 25.5%\n",
      "GPU Usage: 52.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 67.0%\n",
      "================================================================================\n",
      "\n",
      "Question 68/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the largest ocean on Earth?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The largest ocean in the world is the Pacific Ocean, covering approximately 46% of the Earth's water surface area.\n",
      "Response Time: 4.117s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 68.0%\n",
      "================================================================================\n",
      "\n",
      "Question 69/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: In what year was the internet invented?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 1991\n",
      "Response Time: 0.551s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 25.3%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 69.0%\n",
      "================================================================================\n",
      "\n",
      "Question 70/100\n",
      "Category: nature\n",
      "Language: english\n",
      "Input: What is the fastest animal on land?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The cheetah.\n",
      "Response Time: 1.04s\n",
      "RAM Usage: -0.026 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 47.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 70.0%\n",
      "================================================================================\n",
      "\n",
      "Question 71/100\n",
      "Category: general\n",
      "Language: english\n",
      "Input: How many days are in a leap year?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: There are 366 days in a regular year, but there are only 365 days in an ordinary leap year. This is because one day is subtracted from February every four years.\n",
      "Response Time: 5.893s\n",
      "RAM Usage: -0.017 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 71.0%\n",
      "================================================================================\n",
      "\n",
      "Question 72/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: What is the chemical symbol for gold?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Au\n",
      "Response Time: 0.549s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 72.0%\n",
      "================================================================================\n",
      "\n",
      "Question 73/100\n",
      "Category: science\n",
      "Language: english\n",
      "Input: Which planet is known as the Red Planet?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Mars is the red planet.\n",
      "Response Time: 1.523s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 73.0%\n",
      "================================================================================\n",
      "\n",
      "Question 74/100\n",
      "Category: geography\n",
      "Language: english\n",
      "Input: What is the largest desert in the world?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: The largest desert by size is the Antarctica, which covers 14 million square kilometers (5.4 million square miles). However, the Sahara Desert is the hottest desert on Earth with an average surface te...\n",
      "Response Time: 9.804s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 74.0%\n",
      "================================================================================\n",
      "\n",
      "Question 75/100\n",
      "Category: history\n",
      "Language: english\n",
      "Input: Who was the first person to walk on the moon?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Apollo 11 astronaut Neil Armstrong became the first human to step foot onto the lunar surface.\n",
      "Response Time: 3.311s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 75.0%\n",
      "================================================================================\n",
      "\n",
      "Question 76/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain machine learning algorithms\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Machine learning is a branch of artificial intelligence in which machines can learn without being explicitly programmed. Machine learning algorithms use data to train themselves, find patterns, and ma...\n",
      "Response Time: 10.256s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 76.0%\n",
      "================================================================================\n",
      "\n",
      "Question 77/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is deep learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Deep Learning is an application of artificial intelligence (AI) and machine learning which uses neural networks with multiple layers to model high level abstractions in data with the aim of making acc...\n",
      "Response Time: 6.864s\n",
      "RAM Usage: -0.018 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 77.0%\n",
      "================================================================================\n",
      "\n",
      "Question 78/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How do neural networks work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A neural network is a computer program based on the structure of the human brain, which has many interconnected areas called neurons.\n",
      "Response Time: 4.266s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 25.6%\n",
      "GPU Usage: 55.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 78.0%\n",
      "================================================================================\n",
      "\n",
      "Question 79/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is natural language processing?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Natural Language Processing (NLP) is the branch of Artificial Intelligence that gives computers ability to understand, interpret and generate human languages.\n",
      "Response Time: 4.751s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 79.0%\n",
      "================================================================================\n",
      "\n",
      "Question 80/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain computer vision technology\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Computer Vision is an interdisciplinary field concerned with the study of software and algorithms for acquiring, processing, analyzing and understanding visual information. The goal of this field is t...\n",
      "Response Time: 7.822s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 25.8%\n",
      "GPU Usage: 37.0%\n",
      "GPU Memory: 14753.0 MB\n",
      "Progress: 80.0%\n",
      "================================================================================\n",
      "\n",
      "Question 81/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is reinforcement learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Reinforcement Learning (RL) is an area of machine learning concerned with enabling agents to take actions based on rewards or punishments they receive over time. RL is the study of how to program an a...\n",
      "Response Time: 25.942s\n",
      "RAM Usage: -0.014 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 81.0%\n",
      "================================================================================\n",
      "\n",
      "Question 82/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How does speech recognition work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Speech recognition is the technology that enables devices to understand and process human speech into text. The process begins when sound waves from your voice reach the microphone, which converts tho...\n",
      "Response Time: 22.651s\n",
      "RAM Usage: -0.021 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 53.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 82.0%\n",
      "================================================================================\n",
      "\n",
      "Question 83/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What are recommendation algorithms?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Recommendation algorithms are used to find the best possible match for your search query.\n",
      "Response Time: 3.127s\n",
      "RAM Usage: 0.001 GB\n",
      "CPU Usage: 26.8%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 83.0%\n",
      "================================================================================\n",
      "\n",
      "Question 84/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain cloud computing\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Cloud computing is the on-demand availability of computer system resources without direct active management by the user.\n",
      "Response Time: 3.614s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 84.0%\n",
      "================================================================================\n",
      "\n",
      "Question 85/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is blockchain technology?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Blockchain is a type of distributed ledger technology (DLT) that was originally created to support the cryptocurrency bitcoin. It stores data in blocks, which are linked using cryptography. Once a blo...\n",
      "Response Time: 16.752s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 85.0%\n",
      "================================================================================\n",
      "\n",
      "Question 86/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: How do search engines work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Search engine algorithms use a variety of techniques to determine which websites are the best answer to a given query, and rank them accordingly. These include analyzing content, links, and other aspe...\n",
      "Response Time: 6.711s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 86.0%\n",
      "================================================================================\n",
      "\n",
      "Question 87/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is cybersecurity?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Cybersecurity is the protection of information technology systems, including hardware, software, and data from unauthorized access, use, disclosure, disruption, modification, or destruction while ensu...\n",
      "Response Time: 8.654s\n",
      "RAM Usage: -0.022 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 87.0%\n",
      "================================================================================\n",
      "\n",
      "Question 88/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain big data analytics\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Big data analytics is the process of collecting, organizing, analyzing, and interpreting large amounts of data to discover insights and patterns that can help businesses make informed decisions. This ...\n",
      "Response Time: 18.396s\n",
      "RAM Usage: 0.005 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 49.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 88.0%\n",
      "================================================================================\n",
      "\n",
      "Question 89/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is the Internet of Things?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: It's a network of physical objects, such as appliances and sensors, which can connect to the internet and exchange data.\n",
      "Response Time: 4.116s\n",
      "RAM Usage: 0.004 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 35.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 89.0%\n",
      "================================================================================\n",
      "\n",
      "Question 90/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: How does encryption work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Encryption is the process of converting plaintext into ciphertext, which is unreadable unless decrypted using an encryption key or passphrase. The encrypted data can then be transmitted across an inse...\n",
      "Response Time: 11.567s\n",
      "RAM Usage: 0.002 GB\n",
      "CPU Usage: 26.4%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 90.0%\n",
      "================================================================================\n",
      "\n",
      "Question 91/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: What is virtual reality?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Virtual Reality (VR) is an artificial, computer-generated simulation environment in which users can interact in a seemingly real or physical way. VR technology is used to create simulations of three d...\n",
      "Response Time: 13.515s\n",
      "RAM Usage: -0.017 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 91.0%\n",
      "================================================================================\n",
      "\n",
      "Question 92/100\n",
      "Category: technology\n",
      "Language: english\n",
      "Input: Explain quantum computing\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Quantum computing is a new field of computer science based on the idea that you can encode much more information into quantum mechanical systems such as atoms or photons, than classical physics allows...\n",
      "Response Time: 10.277s\n",
      "RAM Usage: 0.008 GB\n",
      "CPU Usage: 26.1%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 92.0%\n",
      "================================================================================\n",
      "\n",
      "Question 93/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What are clustering algorithms?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Clustering algorithms are unsupervised learning algorithms used for grouping data into different clusters based on their similarity to each other. The main goal of these algorithms is to find patterns...\n",
      "Response Time: 14.48s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 25.9%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 93.0%\n",
      "================================================================================\n",
      "\n",
      "Question 94/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How does supervised learning work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Answer:Supervised learning is a type of machine learning algorithm where the system is given a set of examples with known outputs (the supervision) and the algorithm learns to map inputs to their corr...\n",
      "Response Time: 6.872s\n",
      "RAM Usage: 0.011 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 94.0%\n",
      "================================================================================\n",
      "\n",
      "Question 95/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is unsupervised learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: unsupervised learning is a machine learning algorithm that finds patterns in data without being explicitly programmed to do so, using algorithms such as clustering.\n",
      "Response Time: 4.782s\n",
      "RAM Usage: -0.0 GB\n",
      "CPU Usage: 27.5%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 95.0%\n",
      "================================================================================\n",
      "\n",
      "Question 96/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain decision trees in machine learning\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A decision tree is a type of supervised learning algorithm used for classification or regression problems. It works by splitting the dataset into subsets using a series of conditions (or \"tests\") whic...\n",
      "Response Time: 22.149s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 39.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 96.0%\n",
      "================================================================================\n",
      "\n",
      "Question 97/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is feature engineering?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: It means to create new features from existing data to improve model performance.\n",
      "Response Time: 2.661s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 25.7%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 97.0%\n",
      "================================================================================\n",
      "\n",
      "Question 98/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: How do convolutional neural networks work?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: A convolutional layer applies a set of learnable filters to volumes of the input data, usually 3x3 or 5x5 in size, and produces a single value as output. These filter responses are then passed through...\n",
      "Response Time: 11.091s\n",
      "RAM Usage: 0.0 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 36.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 98.0%\n",
      "================================================================================\n",
      "\n",
      "Question 99/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: What is transfer learning?\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Transfer Learning is a very useful technique in which knowledge gained by training one model on an existing dataset can be used to train another model on a different but related task or dataset, signi...\n",
      "Response Time: 14.165s\n",
      "RAM Usage: -0.02 GB\n",
      "CPU Usage: 26.2%\n",
      "GPU Usage: 51.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 99.0%\n",
      "================================================================================\n",
      "\n",
      "Question 100/100\n",
      "Category: ai\n",
      "Language: english\n",
      "Input: Explain generative adversarial networks\n",
      "------------------------------------------------------------\n",
      "Response: generative adversarial network (GAN) is an artificial neural network used to generate new data, based on previously generated data. It consists of two competing neural networks called the generator an...\n",
      "Response Time: 19.192s\n",
      "RAM Usage: 0.003 GB\n",
      "CPU Usage: 26.0%\n",
      "GPU Usage: 54.0%\n",
      "GPU Memory: 14793.0 MB\n",
      "Progress: 100.0%\n",
      "================================================================================\n",
      "\n",
      "Test Summary:\n",
      "Success Rate: 100.0%\n",
      "Average Response Time: 6.261s\n",
      "Summary saved to: jais_13b_comprehensive_test_results_summary.json\n",
      "\n",
      "Test completed! Results saved to jais_13b_comprehensive_test_results.json\n",
      "\n",
      "Test completed successfully!\n",
      "Total questions processed: 100\n",
      "Results saved to JSON file with performance metrics.\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Generation configuration\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_k=50,\n",
    "        top_p=0.9,\n",
    "        repetition_penalty=1.1,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        early_stopping=True,\n",
    "        no_repeat_ngram_size=3\n",
    "    )\n",
    "    \n",
    "    print(\"Starting comprehensive test suite...\")\n",
    "    results = run_comprehensive_test(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        generation_config=generation_config,\n",
    "        output_file=\"jais_13b_comprehensive_test_results.json\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nTest completed successfully!\")\n",
    "    print(f\"Total questions processed: {len(results)}\")\n",
    "    print(\"Results saved to JSON file with performance metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
