[
  {
    "query": "What is supervised learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is supervised learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 7,
      "prompt": "QUERY: What is supervised learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.598] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.290] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 Let\u2019s look at each of these criteria a bit more closely. Supervised/Unsupervised Learning Machine Learning systems can be classified according to the amount and type of supervision they get during training. There are four major categories: supervised learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\u2010 ing. Supervised learning In supervised learning, the training data you feed to the algorithm includes the desired solutions, called labels (Figure 1-5). \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.240] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 (nonspam, also called \u201cham\u201d) emails. The examples that the system uses to learn are called the training set. Each training example is called a training instance (or sample). In this case, the task T is to flag spam for new emails, the experience E is the training data, and the performance measure P needs to be defined; for example, you can use the ratio of correctly classified emails. This particular performance measure is called accuracy and it is often used in classification tasks. \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.224] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.221] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups. 8. Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). 9. An online learning system can learn incrementally, as opposed to a batch learn\u2010 ing system. This makes it capable of adapting rapidly to both changing data and \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.220] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 learning) \u2022 Whether they work by simply comparing new data points to known data points, or instead detect patterns in the training data and build a predictive model, much like scientists do (instance-based versus model-based learning) These criteria are not exclusive; you can combine them in any way you like. For example, a state-of-the-art spam filter may learn on the fly using a deep neural net\u2010 Types of Machine Learning Systems  |  7 \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning.\n- [Score: 0.204] Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 4. What are the two most common supervised tasks? 5. Can you name four common unsupervised tasks? 6. What type of Machine Learning algorithm would you use to allow a robot to walk in various unknown terrains? 7. What type of algorithm would you use to segment your customers into multiple groups? 8. Would you frame the problem of spam detection as a supervised learning prob\u2010 lem or an unsupervised learning problem? 9. What is an online learning system? 10. What is out-of-core learning? \u2192 mic solution, to replace long lists of hand-tuned rules, to build systems that adapt to fluctuating environments, and finally to help humans learn (e.g., data mining). 3. A labeled training set is a training set that contains the desired solution (a.k.a. a label) for each instance. 4. The two most common supervised tasks are regression and classification. 5. Common unsupervised tasks include clustering, visualization, dimensionality reduction, and association rule learning."
    }
  },
  {
    "query": "Explain unsupervised learning.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain unsupervised learning.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 7,
      "prompt": "QUERY: Explain unsupervised learning.\nRELATED EVIDENCE PATHS:\n- [Score: 0.588] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.257] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 and it is able to name everyone in every photo, which is useful for searching photos. Figure 1-11. Semisupervised learning Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsu\u2010 pervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.237] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, Kernel \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.229] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11). \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.204] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed. Once again, think about the MNIST dataset: all handwritten digit images have some \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.204] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 or automatically removing outliers from a dataset before feeding it to another learn\u2010 ing algorithm. The system is trained with normal instances, and when it sees a new instance it can tell whether it looks like a normal one or whether it is likely an anom\u2010 aly (see Figure 1-10). Figure 1-10. Anomaly detection Finally, another common unsupervised task is association rule learning, in which the goal is to dig into large amounts of data and discover interesting relations between \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010\n- [Score: 0.204] Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010"
    }
  },
  {
    "query": "What is reinforcement learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is reinforcement learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 33,
      "prompt": "QUERY: What is reinforcement learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.637] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.397] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 including a discussion of Markov decision processes (MDP). We will use these techni\u2010 ques to train a model to balance a pole on a moving cart, and another to play Atari games. The same techniques can be used for a wide variety of tasks, from walking robots to self-driving cars. Learning to Optimize Rewards In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.373] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.365] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 in the data and use them to make predictions. In Reinforcement Learning, the goal is to find a good policy. \u2022 Unlike in supervised learning, the agent is not explicitly given the \u201cright\u201d answer. It must learn by trial and error. \u2022 Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.350] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.326] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 \u2022 A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don\u2019t need to worry about explora\u2010 tion; they just feed on the training data they are given. \u2022 In supervised and unsupervised learning, training instances are typically inde\u2010 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.320] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.317] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Go player, (d) thermostat, (e) automatic trader5 Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it better find the exit as quickly as possible! There are many other examples of tasks where Rein\u2010 forcement Learning is well suited, such as self-driving cars, placing ads on a web page, or controlling where an image classification system should focus its attention. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.310] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent\u2019s pieces. 6. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro\u2010 duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.294] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the \u201cgene pool.\u201d Policy Search The algorithm used by the software agent to determine its actions is called its policy.  For example, the policy could be a neural network taking observations as inputs and outputting the action to take (see Figure 16-2). Figure 16-2. Reinforcement Learning using a neural network policy \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.287] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 wildest dreams. In this chapter we will first explain what Reinforcement Learning is and what it is good at, and then we will present two of the most important techniques in deep Reinforcement Learning: policy gradients and deep Q-networks (DQN), 443 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.277] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 directly try to optimize the policy to increase rewards, the algorithms we will look at now are less direct: the agent learns to estimate the expected sum of discounted future rewards for each state, or the expected sum of discounted future rewards for each action in each state, then uses this knowledge to decide how to act. To understand these algorithms, we must first introduce Markov decision processes (MDP). Markov Decision Processes \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.276] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Introduction to OpenAI Gym One of the challenges of Reinforcement Learning is that in order to train an agent, you first need to have a working environment. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world and you can directly train your robot in that environment, but this has its limits: if the robot falls off a cliff, \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.261] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 1 For more details, be sure to check out Richard Sutton and Andrew Barto\u2019s book on RL, Reinforcement Learn\u2010 ing: An Introduction (MIT Press), or David Silver\u2019s free online RL course at University College London. 2 \u201cPlaying Atari with Deep Reinforcement Learning,\u201d V. Mnih et al. (2013). 3 \u201cHuman-level control through deep reinforcement learning,\u201d V. Mnih et al. (2015). 4 Check out the videos of DeepMind\u2019s system learning to play Space Invaders, Breakout, and more at https:// goo.gl/yTsH6X. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.257] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Evaluating Actions: The Credit Assignment Problem If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability and the tar\u2010 get probability. It would just be regular supervised learning. However, in Reinforce\u2010 ment Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.256] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). 7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally) \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.252] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Figure 16-3. Four points in policy space and the agent\u2019s corresponding behavior Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regards to the policy parameters, then tweaking these parameters by following the gradient toward higher rewards (gradient ascent). This approach is called policy gradients (PG), which we will discuss in more detail later in this chapter. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.250] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 any case, RL still requires quite a lot of patience and tweaking, but the end result is very exciting. Exercises 1. How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning? 2. Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent? What are possible actions? What are the rewards? \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.250] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 tive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observa\u2010 tions. 2. Here are a few possible applications of Reinforcement Learning, other than those mentioned in Chapter 16: Music personalization The environment is a user\u2019s personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.249] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 goo.gl/yTsH6X. CHAPTER 16 Reinforcement Learning Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years,1 in particular in games (e.g., TD-Gammon, a Backgammon playing program) and in machine control, but seldom making the headline news. But a revolution took place in 2013 when researchers from an English \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.230] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should inject as much prior knowledge as possible into the agent, as it will speed up training dramatically. For example, you could add negative rewards propor\u2010 tional to the distance from the center of the screen, and to the pole\u2019s angle. Also, if you already have a reasonably good policy (e.g., \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.229] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 books). Note that there are actually two ways the agent can lose the game: either the pole can tilt too much, or the cart can go completely off the screen. With 250 training iterations, the policy learns to balance the pole quite well, but it is not yet good enough at avoiding going off the screen. A few hundred more training iterations will fix that. 458  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.227] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 at https://github.com/ageron/handson-ml. Chapter 16: Reinforcement Learning 1. Reinforcement Learning is an area of Machine Learning aimed at creating agents capable of taking actions in an environment in a way that maximizes rewards over time. There are many differences between RL and regular supervised and unsupervised learning. Here are a few: \u2022 In supervised and unsupervised learning, the goal is generally to find patterns \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.222] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.222] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.220] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 exploration policy\u2014for example, a purely random policy\u2014to explore the MDP, and as it progresses the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 16-4). Equation 16-4. TD Learning algorithm Vk + 1 s 1 \u2212\u03b1 Vk s + \u03b1 r + \u03b3 . Vk s\u2032 \u2022 \u03b1 is the learning rate (e.g., 0.01). TD Learning has many similarities with Stochastic Gradient Descent, in particular the fact that it handles one sample at a time. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.219] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 which stabilizes the training process. In the rest of this chapter, we will use DeepMind\u2019s DQN algorithm to train an agent to play Ms. Pac-Man, much like DeepMind did in 2013. The code can easily be tweaked to learn to play the majority of Atari games quite well, provided you train it for long enough (it may take days or weeks, depending on your hardware). It can ach\u2010 ieve superhuman skill at most action games, but it is not so good at games with long- running storylines. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.215] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 environments, in reinforcement learning, 444-453, 465, 470 episodes (in RL), 450, 454-455, 457-458, 475 epochs, 120 \u03b5-insensitive, 157 equality contraints, 510 error analysis, 98-101 estimators, 62 Euclidian norm, 39 eval(), 242 evaluating models, 29-31 explained variance, 217 explained variance ratio, 216 exploding gradients, 278 (see also gradients, vanishing and explod\u2010 ing) exploration policies, 465 exponential decay, 287 exponential linear unit (ELU), 282-283 exponential scheduling, 306 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.209] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 The policy can be any algorithm you can think of, and it does not even have to be deterministic. For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability p every second, or randomly rotate left or right with probability 1 \u2013 p. The rotation angle would be a random angle between \u2013r and +r. Since this policy involves some randomness, it is called a stochastic policy. The robot will have an \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.208] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 4. To measure the performance of a Reinforcement Learning agent, you can simply sum up the rewards it gets. In a simulated environment, you can run many epi\u2010 sodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on). 5. The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.208] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 each transition at least once to know the rewards, and it must experience them multi\u2010 ple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.205] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 3. What is the discount rate? Can the optimal policy change if you modify the dis\u2010 count rate? 4. How do you measure the performance of a Reinforcement Learning agent? 5. What is the credit assignment problem? When does it occur? How can you allevi\u2010 ate it? Exercises  |  475 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.203] Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1): \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most"
    }
  },
  {
    "query": "How does a decision tree work?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "How does a decision tree work?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 13,
      "prompt": "QUERY: How does a decision tree work?\nRELATED EVIDENCE PATHS:\n- [Score: 0.511] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.296] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 an Iris-Virginica (depth 2, right). It\u2019s really that simple. One of the many qualities of Decision Trees is that they require very little data preparation. In particular, they don\u2019t require feature scaling or centering at all. A node\u2019s samples attribute counts how many training instances it applies to. For example, 100 training instances have a petal length greater than 2.45 cm (depth 1, right), among which 54 have a petal width smaller than 1.75 cm (depth 2, left). A \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.296] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 training data. Figure 6-7. Sensitivity to training set rotation More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data. For example, if you just remove the widest Iris- Versicolor from the iris training set (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model represented in Figure 6-8. As you can see, it looks very different from the previous Decision Tree (Figure 6-2). \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.224] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara\u2010 meters of a BaggingClassifier to control the ensemble itself.11 The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.214] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 ter 7), which are among the most powerful Machine Learning algorithms available today. In this chapter we will start by discussing how to train, visualize, and make predic\u2010 tions with Decision Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss how to regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of Decision Trees. Training and Visualizing a Decision Tree \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.213] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 why the predictions were made. For example, if a neural network says that a particu\u2010 lar person appears on a picture, it is hard to know what actually contributed to this prediction: did the model recognize that person\u2019s eyes? Her mouth? Her nose? Her shoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide nice and simple classification rules that can even be applied manually if need be (e.g., for flower classification). 172  |  Chapter 6: Decision Trees \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.208] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 Figure 6-8. Sensitivity to training set details Random Forests can limit this instability by averaging predictions over many trees, as we will see in the next chapter. Exercises 1. What is the approximate depth of a Decision Tree trained (without restrictions) on a training set with 1 million instances? 2. Is a node\u2019s Gini impurity generally lower or greater than its parent\u2019s? Is it gener\u2010 ally lower/greater, or always lower/greater? \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.208] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 CHAPTER 6 Decision Trees Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per\u2010 form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets. For example, in Chap\u2010 ter 2 you trained a DecisionTreeRegressor model on the California housing dataset, fitting it perfectly (actually overfitting it). Decision Trees are also the fundamental components of Random Forests (see Chap\u2010 \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.207] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 6. Decision Trees. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  169 Training and Visualizing a Decision Tree                                                                169 Making Predictions                                                                                                     171 Estimating Class Probabilities                                                                                   173 \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.204] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 3 log2 is the binary log, log2(m) = log(m) / log(2). Chapter 6: Decision Trees 1. The depth of a well-balanced binary tree containing m leaves is equal to log2(m)3, rounded up. A binary Decision Tree (one that makes only binary decisions, as is the case of all trees in Scikit-Learn) will end up more or less well balanced at the end of training, with one leaf per training instance if it is trained without restric\u2010 tions. Thus, if the training set contains one million instances, the Decision Tree \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.203] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 can simply look at the predicted class for that node and the Decision Tree predicts that your flower is an Iris-Setosa (class=setosa). Now suppose you find another flower, but this time the petal length is greater than 2.45 cm. You must move down to the root\u2019s right child node (depth 1, right), which is not a leaf node, so it asks another question: is the petal width smaller than 1.75 cm? If it is, then your flower is most likely an Iris-Versicolor (depth 2, left). If not, it is likely \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.202] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 Scikit-Learn uses the CART algorithm, which produces only binary trees: nonleaf nodes always have two children (i.e., questions only have yes/no answers). However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two chil\u2010 dren. Figure 6-2 shows this Decision Tree\u2019s decision boundaries. The thick vertical line rep\u2010 resents the decision boundary of the root node (depth 0): petal length = 2.45 cm. \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a\n- [Score: 0.201] To understand Decision Trees, let\u2019s just build one and take a look at how it makes pre\u2010 dictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4): from sklearn.datasets import load_iris from sklearn.tree import DecisionTreeClassifier iris = load_iris() X = iris.data[:, 2:] # petal length and width y = iris.target tree_clf = DecisionTreeClassifier(max_depth=2) tree_clf.fit(X, y) 169 \u2192 1 Graphviz is an open source graph visualization software package, available at http://www.graphviz.org/. You can visualize the trained Decision Tree by first using the export_graphviz()  method to output a graph definition file called iris_tree.dot: from sklearn.tree import export_graphviz export_graphviz(         tree_clf,         out_file=image_path(\"iris_tree.dot\"),         feature_names=iris.feature_names[2:],         class_names=iris.target_names,         rounded=True,         filled=True \u2192 6 It randomly selects the set of features to evaluate at each node. Instability Hopefully by now you are convinced that Decision Trees have a lot going for them: they are simple to understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, Figure 6-7 shows a"
    }
  },
  {
    "query": "What is overfitting in machine learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is overfitting in machine learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is overfitting in machine learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.472] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 Overfitting the Training Data Say you are visiting a foreign country and the taxi driver rips you off. You might be tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overfitting: it means that the model performs well on the training data, but it does not generalize well.\n- [Score: 0.200] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 ues found by the learning algorithm. 14. Some of the main challenges in Machine Learning are the lack of data, poor data quality, nonrepresentative data, uninformative features, excessively simple mod\u2010 els that underfit the training data, and excessively complex models that overfit the data. 15. If a model performs great on the training data but generalizes poorly to new instances, the model is likely overfitting the training data (or we got extremely \u2192 Overfitting the Training Data Say you are visiting a foreign country and the taxi driver rips you off. You might be tempted to say that all taxi drivers in that country are thieves. Overgeneralizing is something that we humans do all too often, and unfortunately machines can fall into the same trap if we are not careful. In Machine Learning this is called overfitting: it means that the model performs well on the training data, but it does not generalize well."
    }
  },
  {
    "query": "How can overfitting be prevented?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "How can overfitting be prevented?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: How can overfitting be prevented?\nRELATED EVIDENCE PATHS:\n- [Score: 0.402] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 Avoiding Overfitting Through Regularization  |  309\n- [Score: 0.270] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 Regularized Linear Models As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining the weights of \u2192 Avoiding Overfitting Through Regularization  |  309\n- [Score: 0.233] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 Figure 11-10. Generating new training instances from existing ones Avoiding Overfitting Through Regularization  |  313 \u2192 Avoiding Overfitting Through Regularization  |  309\n- [Score: 0.225] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 regularization to avoid overfitting. The question is: how do you choose the value of the regularization hyperparameter? One option is to train 100 different models using 100 different values for this hyperparameter. Suppose you find the best hyperparame\u2010 ter value that produces a model with the lowest generalization error, say just 5% error. So you launch this model into production, but unfortunately it does not perform as well as expected and produces 15% errors. What just happened? \u2192 Avoiding Overfitting Through Regularization  |  309\n- [Score: 0.220] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 To reduce overfitting, the authors used two regularization techniques we discussed in previous chapters: first they applied dropout (with a 50% dropout rate) during train\u2010 ing to the outputs of layers F8 and F9. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions. AlexNet also uses a competitive normalization step immediately after the ReLU step \u2192 Avoiding Overfitting Through Regularization  |  309\n- [Score: 0.213] lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 Avoiding Overfitting Through Regularization With four parameters I can fit an elephant and with five I can make him wiggle his trunk. \u2014John von Neumann, cited by Enrico Fermi in Nature 427 Deep neural networks typically have tens of thousands of parameters, sometimes even millions. With so many parameters, the network has an incredible amount of freedom and can fit a huge variety of complex datasets. But this great flexibility also means that it is prone to overfitting the training set. \u2192 Avoiding Overfitting Through Regularization  |  309"
    }
  },
  {
    "query": "What is underfitting?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is underfitting?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is underfitting?\nRELATED EVIDENCE PATHS:\n- [Score: 0.412] enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, so this rules out the last option. You could try to add more features (e.g., the log of the popula\u2010 tion), but first let\u2019s try a more complex model to see how it does. Let\u2019s train a DecisionTreeRegressor. This is a powerful model, capable of finding \u2192 In Chapter 2 you used cross-validation to get an estimate of a model\u2019s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it per\u2010 forms poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex. Another way is to look at the learning curves: these are plots of the model\u2019s perfor\u2010\n- [Score: 0.205] enough. As we saw in the previous chapter, the main ways to fix underfitting are to select a more powerful model, to feed the training algorithm with better features, or to reduce the constraints on the model. This model is not regularized, so this rules out the last option. You could try to add more features (e.g., the log of the popula\u2010 tion), but first let\u2019s try a more complex model to see how it does. Let\u2019s train a DecisionTreeRegressor. This is a powerful model, capable of finding \u2192 out). Lastly, your model needs to be neither too simple (in which case it will underfit) nor too complex (in which case it will overfit). There\u2019s just one last important topic to cover: once you have trained a model, you don\u2019t want to just \u201chope\u201d it generalizes to new cases. You want to evaluate it, and fine- tune it if necessary. Let\u2019s see how. Testing and Validating The only way to know how well a model will generalize to new cases is to actually try \u2192 In Chapter 2 you used cross-validation to get an estimate of a model\u2019s generalization performance. If a model performs well on the training data but generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it per\u2010 forms poorly on both, then it is underfitting. This is one way to tell when a model is too simple or too complex. Another way is to look at the learning curves: these are plots of the model\u2019s perfor\u2010"
    }
  },
  {
    "query": "Explain the bias-variance tradeoff.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain the bias-variance tradeoff.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: Explain the bias-variance tradeoff.\nRELATED EVIDENCE PATHS:\n- [Score: 0.611] errors: Bias This part of the generalization error is due to wrong assumptions, such as assum\u2010 ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10 Variance This part is due to the model\u2019s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol\u2010 ynomial model) is likely to have high variance, and thus to overfit the training data. 128  | \u2192 10 This notion of bias is not to be confused with the bias term of linear models. Figure 4-16. Learning curves for the polynomial model One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error. The Bias/Variance Tradeoff An important theoretical result of statistics and Machine Learning is the fact that a model\u2019s generalization error can be expressed as the sum of three very different errors: Bias\n- [Score: 0.246] errors: Bias This part of the generalization error is due to wrong assumptions, such as assum\u2010 ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10 Variance This part is due to the model\u2019s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol\u2010 ynomial model) is likely to have high variance, and thus to overfit the training data. 128  | \u2192 data much better than with plain Linear Regression. For example, Figure 4-14 applies a 300-degree polynomial model to the preceding training data, and compares the result with a pure linear model and a quadratic model (2nd-degree polynomial). Notice how the 300-degree polynomial model wiggles around to get as close as possi\u2010 ble to the training instances. Figure 4-14. High-degree Polynomial Regression Of course, this high-degree Polynomial Regression model is severely overfitting the \u2192 10 This notion of bias is not to be confused with the bias term of linear models. Figure 4-16. Learning curves for the polynomial model One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error. The Bias/Variance Tradeoff An important theoretical result of statistics and Machine Learning is the fact that a model\u2019s generalization error can be expressed as the sum of three very different errors: Bias\n- [Score: 0.224] errors: Bias This part of the generalization error is due to wrong assumptions, such as assum\u2010 ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10 Variance This part is due to the model\u2019s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol\u2010 ynomial model) is likely to have high variance, and thus to overfit the training data. 128  | \u2192 well. Figure 1-22 shows an example of a high-degree polynomial life satisfaction model that strongly overfits the training data. Even though it performs much better on the training data than the simple linear model, would you really trust its predictions? Figure 1-22. Overfitting the training data Complex models such as deep neural networks can detect subtle patterns in the data, but if the training set is noisy, or if it is too small (which introduces sampling noise), \u2192 10 This notion of bias is not to be confused with the bias term of linear models. Figure 4-16. Learning curves for the polynomial model One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error. The Bias/Variance Tradeoff An important theoretical result of statistics and Machine Learning is the fact that a model\u2019s generalization error can be expressed as the sum of three very different errors: Bias\n- [Score: 0.206] errors: Bias This part of the generalization error is due to wrong assumptions, such as assum\u2010 ing that the data is linear when it is actually quadratic. A high-bias model is most likely to underfit the training data.10 Variance This part is due to the model\u2019s excessive sensitivity to small variations in the training data. A model with many degrees of freedom (such as a high-degree pol\u2010 ynomial model) is likely to have high variance, and thus to overfit the training data. 128  | \u2192 9. If both the training error and the validation error are almost equal and fairly high, the model is likely underfitting the training set, which means it has a high bias. You should try reducing the regularization hyperparameter \u03b1. 10. Let\u2019s see: 480  |  Appendix A: Exercise Solutions \u2192 10 This notion of bias is not to be confused with the bias term of linear models. Figure 4-16. Learning curves for the polynomial model One way to improve an overfitting model is to feed it more training data until the validation error reaches the training error. The Bias/Variance Tradeoff An important theoretical result of statistics and Machine Learning is the fact that a model\u2019s generalization error can be expressed as the sum of three very different errors: Bias"
    }
  },
  {
    "query": "What are activation functions?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What are activation functions?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 7,
      "prompt": "QUERY: What are activation functions?\nRELATED EVIDENCE PATHS:\n- [Score: 0.503] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.273] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 3. The logistic activation function was a key ingredient in training the first MLPs because its derivative is always nonzero, so Gradient Descent can always roll down the slope. When the activation function is a step function, Gradient Descent cannot move, as there is no slope at all. 4. The step function, the logistic function, the hyperbolic tangent, the rectified lin\u2010 ear unit (see Figure 10-8). See Chapter 11 for other examples, such as ELU and variants of the ReLU. \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.231] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 Notice that once again we used a name scope for clarity. Also note that logits is the output of the neural network before going through the softmax activation function: for optimization reasons, we will handle the softmax computation later. As you might expect, TensorFlow comes with many handy functions to create standard neural network layers, so there\u2019s often no need to define your own neuron_layer() function like we just did. For example, TensorFlow\u2019s tf.lay \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.226] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 tion function used in each hidden layer and in the output layer.5 In general, the ReLU activation function (or one of its variants; see Chapter 11) is a good default for the hidden layers. For the output layer, in general you will want the logistic activation function for binary classification, the softmax activation function for multiclass classification, or no activation function for regression. If the MLP overfits the training data, you can try reducing the number of hidden \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.217] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 logistic function or the hyperbolic tangent function, which saturate at 1). For the output layer, the softmax activation function is generally a good choice for classification tasks when the classes are mutually exclusive. When they are not mutu\u2010 ally exclusive (or when there are just two classes), you generally want to use the logis\u2010 tic function. For regression tasks, you can simply use no activation function at all for the output layer. 274  | \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.212] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 variants in their experiments: training time was reduced and the neural network per\u2010 formed better on the test set. It is represented in Figure 11-3, and Equation 11-2 shows its definition. Equation 11-2. ELU activation function ELU\u03b1 z = \u03b1 exp z \u22121 if z < 0 z if z \u22650 Figure 11-3. ELU activation function 282  |  Chapter 11: Training Deep Neural Nets \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?\n- [Score: 0.205] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 variants of the ReLU. 5. Considering the MLP described in the question: suppose you have an MLP com\u2010 posed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neu\u2010 rons. All artificial neurons use the ReLU activation function. \u2022 The shape of the input matrix X is m \u00d7 10, where m represents the training batch size. \u2022 The shape of the hidden layer\u2019s weight vector Wh is 10 \u00d7 50 and the length of \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X?"
    }
  },
  {
    "query": "Describe the sigmoid activation function.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe the sigmoid activation function.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: Describe the sigmoid activation function.\nRELATED EVIDENCE PATHS:\n- [Score: 0.383] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 have provided similar strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the ReLU activation function (and its var\u2010 iants, including the ELU activation described shortly) is sometimes called He initiali\u2010 zation (after the last name of its author). This is the strategy we used in Chapter 10. Table 11-1. Initialization parameters for each type of activation function Activation function Uniform distribution [\u2013r, r] Normal distribution Logistic\n- [Score: 0.238] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 3. Why was the logistic activation function a key ingredient in training the first MLPs? 4. Name three popular activation functions. Can you draw them? 5. Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neurons. All artificial neurons use the ReLU activa\u2010 tion function. \u2022 What is the shape of the input matrix X? \u2192 have provided similar strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the ReLU activation function (and its var\u2010 iants, including the ELU activation described shortly) is sometimes called He initiali\u2010 zation (after the last name of its author). This is the strategy we used in Chapter 10. Table 11-1. Initialization parameters for each type of activation function Activation function Uniform distribution [\u2013r, r] Normal distribution Logistic\n- [Score: 0.215] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 have provided similar strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the ReLU activation function (and its var\u2010 iants, including the ELU activation described shortly) is sometimes called He initiali\u2010 zation (after the last name of its author). This is the strategy we used in Chapter 10. Table 11-1. Initialization parameters for each type of activation function Activation function Uniform distribution [\u2013r, r] Normal distribution Logistic\n- [Score: 0.206] Figure 10-8. Activation functions and their derivatives An MLP is often used for classification, with each output corresponding to a different binary class (e.g., spam/ham, urgent/not-urgent, and so on). When the classes are exclusive (e.g., classes 0 through 9 for digit image classification), the output layer is typically modified by replacing the individual activation functions by a shared soft\u2010 max function (see Figure 10-9). The softmax function was introduced in Chapter 4. \u2192 logistic function or the hyperbolic tangent function, which saturate at 1). For the output layer, the softmax activation function is generally a good choice for classification tasks when the classes are mutually exclusive. When they are not mutu\u2010 ally exclusive (or when there are just two classes), you generally want to use the logis\u2010 tic function. For regression tasks, you can simply use no activation function at all for the output layer. 274  | \u2192 have provided similar strategies for different activation functions, as shown in Table 11-1. The initialization strategy for the ReLU activation function (and its var\u2010 iants, including the ELU activation described shortly) is sometimes called He initiali\u2010 zation (after the last name of its author). This is the strategy we used in Chapter 10. Table 11-1. Initialization parameters for each type of activation function Activation function Uniform distribution [\u2013r, r] Normal distribution Logistic"
    }
  },
  {
    "query": "What is the ReLU activation function?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the ReLU activation function?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 12,
      "prompt": "QUERY: What is the ReLU activation function?\nRELATED EVIDENCE PATHS:\n- [Score: 0.534] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.286] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 variants of the ReLU. 5. Considering the MLP described in the question: suppose you have an MLP com\u2010 posed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3 artificial neu\u2010 rons. All artificial neurons use the ReLU activation function. \u2022 The shape of the input matrix X is m \u00d7 10, where m represents the training batch size. \u2022 The shape of the hidden layer\u2019s weight vector Wh is 10 \u00d7 50 and the length of \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.251] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 ing training this is compensated by the faster convergence rate. However, at test time an ELU network will be slower than a ReLU network. So which activation function should you use for the hidden layers of your deep neural networks? Although your mileage will vary, in general ELU > leaky ReLU (and its variants) > ReLU > tanh > logis\u2010 tic. If you care a lot about runtime performance, then you may pre\u2010 fer leaky ReLUs over ELUs. If you don\u2019t want to tweak yet another \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.248] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 exploding gradients problems were in part due to a poor choice of activation func\u2010 tion. Until then most people had assumed that if Mother Nature had chosen to use roughly sigmoid activation functions in biological neurons, they must be an excellent choice. But it turns out that other activation functions behave much better in deep neural networks, in particular the ReLU activation function, mostly because it does not saturate for positive values (and also because it is quite fast to compute). \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.236] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 all the neurons in any given layer will always have the same weights. It\u2019s like hav\u2010 ing just one neuron per layer, and much slower. It is virtually impossible for such a configuration to converge to a good solution. 2. It is perfectly fine to initialize the bias terms to zero. Some people like to initialize them just like weights, and that\u2019s okay too; it does not make much difference. 3. A few advantages of the ELU function over the ReLU function are: \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.234] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 6 \u201cFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs),\u201d D. Clevert, T. Unterthiner, S. Hochreiter (2015). was reported to strongly outperform ReLU on large image datasets, but on smaller datasets it runs the risk of overfitting the training set. Figure 11-2. Leaky ReLU Last but not least, a 2015 paper by Djork-Arn\u00e9 Clevert et al.6 proposed a new activa\u2010 tion function called the exponential linear unit (ELU) that outperformed all the ReLU \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.232] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 \u2022 It can take on negative values, so the average output of the neurons in any given layer is typically closer to 0 than when using the ReLU activation func\u2010 tion (which never outputs negative values). This helps alleviate the vanishing gradients problem. \u2022 It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units. \u2022 It is smooth everywhere, whereas the ReLU\u2019s slope abruptly jumps from 0 to 1 \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.228] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 1 Note that many researchers prefer to use the hyperbolic tangent (tanh) activation function in RNNs rather than the ReLU activation function. For example, take a look at by Vu Pham et al.\u2019s paper \u201cDropout Improves Recurrent Neural Networks for Handwriting Recognition\u201d. However, ReLU-based RNNs are also possible, as shown in Quoc V. Le et al.\u2019s paper \u201cA Simple Way to Initialize Recurrent Networks of Rectified Linear Units\u201d. \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.227] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 very similar, but it uses a softmax activation function instead of a ReLU activation function. So let\u2019s create a neuron_layer() function that we will use to create one layer at a time. It will need parameters to specify the inputs, the number of neurons, the activation function, and the name of the layer: Training a DNN Using Plain TensorFlow  |  267 \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.224] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 inputs is negative, it will start outputting 0. When this happens, the neuron is unlikely to come back to life since the gradient of the ReLU function is 0 when its input is negative. To solve this problem, you may want to use a variant of the ReLU function, such as the leaky ReLU. This function is defined as LeakyReLU\u03b1(z) = max(\u03b1z, z) (see Figure 11-2). The hyperparameter \u03b1 defines how much the function \u201cleaks\u201d: it is the \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.222] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 Biological neurons seem to implement a roughly sigmoid (S- shaped) activation function, so researchers stuck to sigmoid func\u2010 tions for a very long time. But it turns out that the ReLU activation function generally works better in ANNs. This is one of the cases where the biological analogy was misleading. Training an MLP with TensorFlow\u2019s High-Level API The simplest way to train an MLP with TensorFlow is to use the high-level API \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s\n- [Score: 0.208] ReLU activation function (we can change this by setting the activation_fn hyper\u2010 266  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 hyperparameter, you may just use the default \u03b1 values suggested earlier (0.01 for the leaky ReLU, and 1 for ELU). If you have spare time and computing power, you can use cross-validation to evalu\u2010 ate other activation functions, in particular RReLU if your network is overfitting, or PReLU if you have a huge training set. TensorFlow offers an elu() function that you can use to build your neural network. Simply set the activation argument when calling the dense() function, like this: \u2192 Unfortunately, the ReLU activation function is not perfect. It suffers from a problem known as the dying ReLUs: during training, some neurons effectively die, meaning they stop outputting anything other than 0. In some cases, you may find that half of your network\u2019s neurons are dead, especially if you used a large learning rate. During training, if a neuron\u2019s weights get updated such that the weighted sum of the neuron\u2019s"
    }
  },
  {
    "query": "What is a neural network?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a neural network?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 11,
      "prompt": "QUERY: What is a neural network?\nRELATED EVIDENCE PATHS:\n- [Score: 0.457] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.224] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 very similar, but it uses a softmax activation function instead of a ReLU activation function. So let\u2019s create a neuron_layer() function that we will use to create one layer at a time. It will need parameters to specify the inputs, the number of neurons, the activation function, and the name of the layer: Training a DNN Using Plain TensorFlow  |  267 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.223] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 8 \u201cLearning Internal Representations by Error Propagation,\u201d D. Rumelhart, G. Hinton, R. Williams (1986). 9 This algorithm was actually invented several times by various researchers in different fields, starting with P. Werbos in 1974. Multi-Layer Perceptron and Backpropagation An MLP is composed of one (passthrough) input layer, one or more layers of LTUs, called hidden layers, and one final layer of LTUs called the output layer (see \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.222] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 12 By Vincent Vanhoucke in his Deep Learning class on Udacity.com. reuse parts of a pretrained state-of-the-art network that performs a similar task. Training will be a lot faster and require much less data (we will discuss this in Chap\u2010 ter 11). Number of Neurons per Hidden Layer Obviously the number of neurons in the input and output layers is determined by the type of input and output your task requires. For example, the MNIST task requires 28 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.217] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 artificial neural networks (ANNs). However, although planes were inspired by birds, they don\u2019t have to flap their wings. Similarly, ANNs have gradually become quite dif\u2010 ferent from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying \u201cunits\u201d rather than \u201cneurons\u201d), lest we restrict our creativity to biologically plausible systems.1 ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\u2010 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.216] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 APPENDIX E Other Popular ANN Architectures In this appendix we will give a quick overview of a few historically important neural network architectures that are much less used today than deep Multi-Layer Percep\u2010 trons (Chapter 10), convolutional neural networks (Chapter 13), recurrent neural networks (Chapter 14), or autoencoders (Chapter 15). They are often mentioned in the literature, and some are still used in many applications, so it is worth knowing \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.216] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 central hidden layer (the coding layer). To put it simply, it looks like a sandwich. For example, an autoencoder for MNIST (introduced in Chapter 3) may have 784 inputs, followed by a hidden layer with 300 neurons, then a central hidden layer of 150 neu\u2010 rons, then another hidden layer with 300 neurons, and an output layer with 784 neu\u2010 rons. This stacked autoencoder is represented in Figure 15-3. Figure 15-3. Stacked autoencoder Stacked Autoencoders  |  421 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.215] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 The output of each neuron corresponds to the estimated probability of the corre\u2010 sponding class. Note that the signal flows only in one direction (from the inputs to the outputs), so this architecture is an example of a feedforward neural network (FNN). Figure 10-9. A modern MLP (including ReLU and softmax) for classification From Biological to Artificial Neurons  |  265 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.215] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 9 \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d A. Krizhevsky et al. (2012). \u2022 The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coeffi\u2010 cient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function. \u2022 Most neurons in C3 maps are connected to neurons in only three or four S2 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.214] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 study of neural networks) in favor of higher-level problems such as logic, problem solving, and search. However, it turns out that some of the limitations of Perceptrons can be eliminated by stacking multiple Perceptrons. The resulting ANN is called a Multi-Layer Perceptron (MLP). In particular, an MLP can solve the XOR problem, as you can verify by com\u2010 puting the output of the MLP represented on the right of Figure 10-6, for each com\u2010 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient\n- [Score: 0.202] 4 In the context of Machine Learning, the phrase \u201cneural networks\u201d generally refers to ANNs, not BNNs. 5 Drawing of a cortical lamination by S. Ramon y Cajal (public domain). Reproduced from https://en.wikipe dia.org/wiki/Cerebral_cortex. works (BNN)4 is still the subject of active research, but some parts of the brain have been mapped, and it seems that neurons are often organized in consecutive layers, as  shown in Figure 10-2. \u2192 (DeepMind\u2019s AlphaGo). In this chapter, we will introduce artificial neural networks, starting with a quick tour of the very first ANN architectures. Then we will present Multi-Layer Perceptrons (MLPs) and implement one using TensorFlow to tackle the MNIST digit classification problem (introduced in Chapter 3). 255 \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient"
    }
  },
  {
    "query": "How does backpropagation work?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "How does backpropagation work?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 8,
      "prompt": "QUERY: How does backpropagation work?\nRELATED EVIDENCE PATHS:\n- [Score: 0.510] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.327] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 the gradients, backpropagation uses reverse-mode autodiff (although it wasn\u2019t called that when backpropagation was invented, and it has been reinvented sev\u2010 eral times). Reverse-mode autodiff performs a forward pass through a computa\u2010 tion graph, computing every node\u2019s value for the current training batch, and then it performs a reverse pass, computing all the gradients at once (see Appendix D for more details). So what\u2019s the difference? Well, backpropagation refers to the \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.309] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 computes how much each neuron in the last hidden layer contributed to each output neuron\u2019s error. It then proceeds to measure how much of these error contributions came from each neuron in the previous hidden layer\u2014and so on until the algorithm reaches the input layer. This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (hence the name of the algorithm). If you check out the \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.304] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 Descent using reverse-mode autodiff (Gradient Descent was introduced in Chapter 4, and autodiff was discussed in Chapter 9). For each training instance, the algorithm feeds it to the network and computes the output of every neuron in each consecutive layer (this is the forward pass, just like when making predictions). Then it measures the network\u2019s output error (i.e., the dif\u2010 ference between the desired output and the actual output of the network), and it \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.279] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 whole process of training an artificial neural network using multiple backpropa\u2010 gation steps, each of which computes gradients and uses them to perform a Gra\u2010 dient Descent step. In contrast, reverse-mode autodiff is a simply a technique to compute gradients efficiently, and it happens to be used by backpropagation. 8. Here is a list of all the hyperparameters you can tweak in a basic MLP: the num\u2010 ber of hidden layers, the number of neurons in each hidden layer, and the activa\u2010 \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.240] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 of RAM, especially during training, because the reverse pass of backpropagation requires all the intermediate values computed during the forward pass. 366  |  Chapter 13: Convolutional Neural Networks \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.216] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 Figure 10-7). Every layer except the output layer includes a bias neuron and is fully connected to the next layer. When an ANN has two or more hidden layers, it is called  a deep neural network (DNN). Figure 10-7. Multi-Layer Perceptron For many years researchers struggled to find a way to train MLPs, without success. But in 1986, D. E. Rumelhart et al. published a groundbreaking article8 introducing the backpropagation training algorithm.9 Today we would describe it as Gradient \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through\n- [Score: 0.204] 7. Backpropagation is a technique used to train artificial neural networks. It first computes the gradients of the cost function with regards to every model parame\u2010 ter (all the weights and biases), and then it performs a Gradient Descent step using these gradients. This backpropagation step is typically performed thou\u2010 sands or millions of times, using many training batches, until the model parame\u2010 ters converge to values that (hopefully) minimize the cost function. To compute \u2192 7. What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff? 8. Can you list all the hyperparameters you can tweak in an MLP? If the MLP over\u2010 fits the training data, how could you tweak these hyperparameters to try to solve the problem? 9. Train a deep MLP on the MNIST dataset and see if you can get over 98% preci\u2010 sion. Just like in the last exercise of Chapter 9, try adding all the bells and whistles \u2192 reverse-mode autodiff algorithm in Appendix D, you will find that the forward and reverse passes of backpropagation simply perform reverse-mode autodiff. The last step of the backpropagation algorithm is a Gradient Descent step on all the connec\u2010 tion weights in the network, using the error gradients measured earlier. Let\u2019s make this even shorter: for each training instance the backpropagation algo\u2010 rithm first makes a prediction (forward pass), measures the error, then goes through"
    }
  },
  {
    "query": "What is gradient descent?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is gradient descent?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 17,
      "prompt": "QUERY: What is gradient descent?\nRELATED EVIDENCE PATHS:\n- [Score: 0.591] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.305] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 As you can see, on the left the Gradient Descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time. When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn\u2019s StandardScaler \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.283] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum (see Figure 4-3). Figure 4-3. Gradient Descent An important parameter in Gradient Descent is the size of the steps, determined by  the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time (see Figure 4-4). Gradient Descent  |  113 \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.258] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 down the steep slope quite fast, but then it takes a very long time to go down the val\u2010 ley. In contrast, Momentum optimization will roll down the bottom of the valley faster and faster until it reaches the bottom (the optimum). In deep neural networks that don\u2019t use Batch Normalization, the upper layers will often end up having inputs with very different scales, so using Momentum optimization helps a lot. It can also help roll past local optima. \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.258] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly.4 These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high). In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have very different scales. Figure 4-7 shows Gradient Descent on a train\u2010 \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.246] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 minimize the cost function over the training set). \u2022 Using an iterative optimization approach, called Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of Gradient Descent that we will use again and again when we study neural networks in Part II: Batch GD, Mini-batch GD, and Stochastic GD. 107 \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.241] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 plateaus, and all sorts of irregular terrains, making convergence to the minimum very difficult. Figure 4-6 shows the two main challenges with Gradient Descent: if the ran\u2010 dom initialization starts the algorithm on the left, then it will converge to a local mini\u2010 mum, which is not as good as the global minimum. If it starts on the right, then it will take a very long time to cross the plateau, and if you stop too early you will never reach the global minimum. 114  |  Chapter 4: Training Models \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.239] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 0 (the slope changes abruptly, which can make Gradient Descent bounce around). However, in practice it works very well and has the advantage of being fast to compute. Most importantly, the fact that it does not have a maximum out\u2010 put value also helps reduce some issues during Gradient Descent (we will come back to this in Chapter 11). These popular activation functions and their derivatives are represented in Figure 10-8. 264  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.237] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 computing si \u2190 si + (\u2202 J(\u03b8) / \u2202 \u03b8i)2 for each element si of the vector s; in other words, each si accumulates the squares of the partial derivative of the cost function with regards to parameter \u03b8i. If the cost function is steep along the ith dimension, then si will get larger and larger at each iteration. The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector is scaled down by a factor of \ufffd+ \ufffd (the \u2298 symbol represents the \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.218] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 1 \u201cUnderstanding the Difficulty of Training Deep Feedforward Neural Networks,\u201d X. Glorot, Y Bengio (2010). parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step. Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.215] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 learning rate \u03b7 comes into play:6 multiply the gradient vector by \u03b7 to determine the size of the downhill step (Equation 4-7). Equation 4-7. Gradient Descent step \u03b8 next step = \u03b8 \u2212\u03b7\u2207\u03b8 MSE \u03b8 Let\u2019s look at a quick implementation of this algorithm: eta = 0.1  # learning rate n_iterations = 1000 m = 100 theta = np.random.randn(2,1)  # random initialization for iteration in range(n_iterations):     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)     theta = theta - eta * gradients Gradient Descent \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.207] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 is to go downhill in the direction of the steepest slope. This is exactly what Gradient Descent does: it measures the local gradient of the error function with regards to the  parameter vector \u03b8, and it goes in the direction of descending gradient. Once the gra\u2010 dient is zero, you have reached a minimum! Concretely, you start by filling \u03b8 with random values (this is called random initializa\u2010 tion), and then you improve it gradually, taking one baby step at a time, each step \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.205] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 the learning rate \u03b7. The equation is: \u03b8 \u2190 \u03b8 \u2013 \u03b7\u2207\u03b8J(\u03b8). It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly. Momentum optimization cares a great deal about what previous gradients were: at each iteration, it subtracts the local gradient from the momentum vector m (multi\u2010 plied by the learning rate \u03b7), and it updates the weights by simply adding this momentum vector (see Equation 11-4). In other words, the gradient is used as an \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.205] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 ing sets (but we will see much faster Gradient Descent algorithms shortly). However, Gradient Descent scales well with the number of features; training a Linear Regression model when there are hun\u2010 dreds of thousands of features is much faster using Gradient Descent than using the Normal Equation. Once you have the gradient vector, which points uphill, just go in the opposite direc\u2010 tion to go downhill. This means subtracting \u2207\u03b8MSE(\u03b8) from \u03b8. This is where the \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.204] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 minimize() method. This is because we want to tweak the gradients before we apply them.10 The compute_gradients() method returns a list of gradient vector/variable pairs (one pair per trainable variable). Let\u2019s put all the gradients in a list, to make it more convenient to obtain their values: gradients = [grad for grad, variable in grads_and_vars] Okay, now comes the tricky part. During the execution phase, the algorithm will run \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.202] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 12 \u201cAdaptive Subgradient Methods for Online Learning and Stochastic Optimization,\u201d J. Duchi et al. (2011). Figure 11-6. Regular versus Nesterov Momentum optimization AdaGrad Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum. \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by\n- [Score: 0.201] Gradient Descent Gradient Descent is a very generic optimization algorithm capable of finding optimal solutions to a wide range of problems. The general idea of Gradient Descent is to tweak parameters iteratively in order to minimize a cost function. Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly \u2192 terminal velocity is equal to 10 times the gradient times the learning rate, so Momen\u2010 tum optimization ends up going 10 times faster than Gradient Descent! This allows Momentum optimization to escape from plateaus much faster than Gradient Descent. In particular, we saw in Chapter 4 that when the inputs have very different scales the  cost function will look like an elongated bowl (see Figure 4-7). Gradient Descent goes \u2192 velocity (if there is some friction or air resistance). This is the very simple idea behind Momentum optimization, proposed by Boris Polyak in 1964.10 In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom. Recall that Gradient Descent simply updates the weights \u03b8 by directly subtracting the gradient of the cost function J(\u03b8) with regards to the weights (\u2207\u03b8J(\u03b8)) multiplied by"
    }
  },
  {
    "query": "Explain stochastic gradient descent.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain stochastic gradient descent.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 13,
      "prompt": "QUERY: Explain stochastic gradient descent.\nRELATED EVIDENCE PATHS:\n- [Score: 0.479] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.257] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 Gradient Descent  |  117 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.253] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 minimize the cost function over the training set). \u2022 Using an iterative optimization approach, called Gradient Descent (GD), that gradually tweaks the model parameters to minimize the cost function over the training set, eventually converging to the same set of parameters as the first method. We will look at a few variants of Gradient Descent that we will use again and again when we study neural networks in Part II: Batch GD, Mini-batch GD, and Stochastic GD. 107 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.252] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 solution, then the algorithm will have to run about 10 times longer. Stochastic Gradient Descent The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.249] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 ent Descent: at each step, instead of computing the gradients based on the full train\u2010 ing set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini- Gradient Descent  |  121 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.235] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 specific standard deviation helps the algorithm converge much faster (we will discuss this further in Chapter 11; it is one of those small tweaks to neural net\u2010 works that have had a tremendous impact on their efficiency). It is important to initialize connection weights randomly for all hidden layers to avoid any symme\u2010 tries that the Gradient Descent algorithm would be unable to break.11 4. The next line creates a b variable for biases, initialized to 0 (no symmetry issue in \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.232] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.230] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early. This code implements Stochastic Gradient Descent using a simple learning schedule: n_epochs = 50 t0, t1 = 5, 50  # learning schedule hyperparameters def learning_schedule(t):     return t0 / (t + t1) theta = np.random.randn(2,1)  # random initialization \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.218] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 The Normal Equation                                                                                              110 Computational Complexity                                                                                    112 Gradient Descent                                                                                                         113 Batch Gradient Descent                                                                                           116 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.213] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 minimum, while both Stochastic GD and Mini-batch GD continue to walk around. However, don\u2019t forget that Batch GD takes a lot of time to take each step, and Stochas\u2010 tic GD and Mini-batch GD would also reach the minimum if you used a good learn\u2010 ing schedule. Figure 4-11. Gradient Descent paths in parameter space Let\u2019s compare the algorithms we\u2019ve discussed so far for Linear Regression8 (recall that m is the number of training instances and n is the number of features); see Table 4-1. \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.209] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 To implement Mini-batch Gradient Descent, we only need to tweak the existing code slightly. First change the definition of X and y in the construction phase to make them  placeholder nodes: X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\") y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") Then define the batch size and compute the total number of batches: batch_size = 100 n_batches = int(np.ceil(m / batch_size)) \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.206] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 NumPy\u2019s rand() function. \u2022 The assign() function creates a node that will assign a new value to a variable. In this case, it implements the Batch Gradient Descent step \u03b8(next step) = \u03b8 \u2013 \u03b7\u2207\u03b8MSE(\u03b8). \u2022 The main loop executes the training step over and over again (n_epochs times), and every 100 iterations it prints out the current Mean Squared Error (mse). You should see the MSE go down at every iteration. n_epochs = 1000 learning_rate = 0.01 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125\n- [Score: 0.202] Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 is that the learning rate is too high and the algorithm is diverging. If the training error also goes up, then this is clearly the problem and you should reduce the learning rate. However, if the training error is not going up, then your model is overfitting the training set and you should stop training. 6. Due to their random nature, neither Stochastic Gradient Descent nor Mini-batch Gradient Descent is guaranteed to make progress at every single training itera\u2010 \u2192 Stochastic Gradient Descent                                                                                   119 Mini-batch Gradient Descent                                                                                 121 Polynomial Regression                                                                                                123 Learning Curves                                                                                                           125"
    }
  },
  {
    "query": "What are epochs in training?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What are epochs in training?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 18,
      "prompt": "QUERY: What are epochs in training?\nRELATED EVIDENCE PATHS:\n- [Score: 0.545] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.359] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 training set (55,000 instances), a validation set (5,000 instances), and a test set (10,000 instances). So let\u2019s use this helper: from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\") Now we define the number of epochs that we want to run, as well as the size of the mini-batches: n_epochs = 40 batch_size = 50 And now we can train the model: with tf.Session() as sess:     init.run()     for epoch in range(n_epochs): \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.291] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 init = tf.global_variables_initializer() with tf.Session() as sess:     sess.run(init)     for epoch in range(n_epochs): Implementing Gradient Descent  |  239 \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.280] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 9 \u201cOn the difficulty of training recurrent neural networks,\u201d R. Pascanu et al. (2013). we need to do is get the list of operations in that collection and run them at each training iteration: extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size):             X_batch, y_batch = mnist.train.next_batch(batch_size) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.270] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size):             X_batch, y_batch = mnist.train.next_batch(batch_size)             X_batch = X_batch.reshape((-1, n_steps, n_inputs))             sess.run(training_op, feed_dict={X: X_batch, y: y_batch})         acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})         acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test}) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.258] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 To implement Mini-batch Gradient Descent, we only need to tweak the existing code slightly. First change the definition of X and y in the construction phase to make them  placeholder nodes: X = tf.placeholder(tf.float32, shape=(None, n + 1), name=\"X\") y = tf.placeholder(tf.float32, shape=(None, 1), name=\"y\") Then define the batch size and compute the total number of batches: batch_size = 100 n_batches = int(np.ceil(m / batch_size)) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.251] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 data in a moment. from tensorflow.examples.tutorials.mnist import input_data mnist = input_data.read_data_sets(\"/tmp/data/\") X_test = mnist.test.images.reshape((-1, n_steps, n_inputs)) y_test = mnist.test.labels Now we are ready to train the RNN. The execution phase is exactly the same as for the MNIST classifier in Chapter 10, except that we reshape each training batch before feeding it to the network. n_epochs = 100 batch_size = 150 with tf.Session() as sess:     init.run() \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.241] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 for iteration in range(mnist.train.num_examples // batch_size):             X_batch, y_batch = mnist.train.next_batch(batch_size)             sess.run(training_op, feed_dict={X: X_batch, y: y_batch})         acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})         acc_val = accuracy.eval(feed_dict={X: mnist.validation.images,                                            y: mnist.validation.labels})         print(epoch, \"Train accuracy:\", acc_train, \"Val accuracy:\", acc_val) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.232] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 save_path = saver.save(sess, \"./my_model_final.ckpt\") This code opens a TensorFlow session, and it runs the init node that initializes all the variables. Then it runs the main training loop: at each epoch, the code iterates through a number of mini-batches that corresponds to the training set size. Each mini-batch is fetched via the next_batch() method, and then the code simply runs the training operation, feeding it the current mini-batch input data and targets. Next, \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.220] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 you could first run the whole training set through the lower layers (assuming you have enough RAM), then during training, instead of building batches of training instances, you would build batches of outputs from hidden layer 2 and feed them to the training operation: import numpy as np n_batches = mnist.train.num_examples // batch_size with tf.Session() as sess:     init.run()     restore_saver.restore(sess, \"./my_model_final.ckpt\") \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.217] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 Figure 4-10. Stochastic Gradient Descent first 10 steps Note that since instances are picked randomly, some instances may be picked several times per epoch while others may not be picked at all. If you want to be sure that the algorithm goes through every instance at each epoch, another approach is to shuffle the training set, then go through it instance by instance, then shuffle it again, and so on. However, this generally converges more slowly. \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.214] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 n_iterations = 1500 batch_size = 50 with tf.Session() as sess:     init.run()     for iteration in range(n_iterations):         X_batch, y_batch = [...]  # fetch the next training batch         sess.run(training_op, feed_dict={X: X_batch, y: y_batch})         if iteration % 100 == 0:             mse = loss.eval(feed_dict={X: X_batch, y: y_batch})             print(iteration, \"\\tMSE:\", mse) The program\u2019s output should look like this: 0       MSE: 13.6543 100     MSE: 0.538476 \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.211] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 solution, then the algorithm will have to run about 10 times longer. Stochastic Gradient Descent The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow when the training set is large. At the opposite extreme, Stochastic Gradient Descent just picks a random instance in the training set at every step and computes the gradients \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.209] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.207] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 init = tf.global_variables_initializer() You can then train the model normally. Note that the digit labels (y_batch) are unused: n_epochs = 5 batch_size = 150 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs): 422  |  Chapter 15: Autoencoders \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.205] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 outputs = tf.transpose(tf.stack(output_seqs), perm=[1, 0, 2]) Now we can run the network by feeding it a single tensor that contains all the mini- batch sequences: X_batch = np.array([          # t = 0     t = 1         [[0, 1, 2], [9, 8, 7]], # instance 0         [[3, 4, 5], [0, 0, 0]], # instance 1         [[6, 7, 8], [6, 5, 4]], # instance 2         [[9, 0, 1], [3, 2, 1]], # instance 3     ]) with tf.Session() as sess:     init.run()     outputs_val = outputs.eval(feed_dict={X: X_batch}) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.201] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 try:         for step in range(max_steps):             sess.run(training_op)     except tf.errors.OutOfRangeError as ex:         pass # no more training instances In this example, the first mini-batch will contain the first two instances of the CSV file, and the second mini-batch will contain the last instance. TensorFlow queues don\u2019t handle sparse tensors well, so if your training instances are sparse you should parse the records after the instance queue. \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets\n- [Score: 0.201] for epoch in range(n_epochs):     for i in range(m):         random_index = np.random.randint(m)         xi = X_b[random_index:random_index+1]         yi = y[random_index:random_index+1]         gradients = 2 * xi.T.dot(xi.dot(theta) - yi)         eta = learning_schedule(epoch * m + i)         theta = theta - eta * gradients By convention we iterate by rounds of m iterations; each round is called an epoch.  While the Batch Gradient Descent code iterated 1,000 times through the whole train\u2010 \u2192 h2_cache = sess.run(hidden2, feed_dict={X: mnist.train.images})     for epoch in range(n_epochs):         shuffled_idx = np.random.permutation(mnist.train.num_examples)         hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)         y_batches = np.array_split(mnist.train.labels[shuffled_idx], n_batches)         for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):             sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch}) \u2192 with tf.Session() as sess:     init.run()     for epoch in range(n_epochs):         for iteration in range(mnist.train.num_examples // batch_size): 312  |  Chapter 11: Training Deep Neural Nets"
    }
  },
  {
    "query": "What is a loss function?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a loss function?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is a loss function?\nRELATED EVIDENCE PATHS:\n- [Score: 0.210] small and as few as possible Hinge Loss The function max(0, 1 \u2013 t) is called the hinge loss function (represented below). It is equal to 0 when t \u2265 1. Its derivative (slope) is equal to \u20131 if t < 1 and 0 if t > 1. It is not differentiable at t = 1, but just like for Lasso Regression (see \u201cLasso Regression\u201d on page 132) you can still use Gradient Descent using any subderivative at t = 1 (i.e., any value between \u20131 and 0). 166  |  Chapter 5: Support Vector Machines \u2192 computes how much each neuron in the last hidden layer contributed to each output neuron\u2019s error. It then proceeds to measure how much of these error contributions came from each neuron in the previous hidden layer\u2014and so on until the algorithm reaches the input layer. This reverse pass efficiently measures the error gradient across all the connection weights in the network by propagating the error gradient backward in the network (hence the name of the algorithm). If you check out the"
    }
  },
  {
    "query": "Define cross-entropy loss.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Define cross-entropy loss.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: Define cross-entropy loss.\nRELATED EVIDENCE PATHS:\n- [Score: 0.506] a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters). Equation 4-22. Cross entropy cost function J \u0398 = \u22121 m \u2211 i = 1 m \u2211 k = 1 K yk i log pk i \u2022 yk i  is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to 0. Notice that when there are just two classes (K = 2), this cost function is equivalent to \u2192 the Logistic Regression\u2019s cost function (log loss; see Equation 4-17). Cross Entropy Cross entropy originated from information theory. Suppose you want to efficiently transmit information about the weather every day. If there are eight options (sunny, rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you think it will be sunny almost every day, it would be much more efficient to code\n- [Score: 0.278] a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters). Equation 4-22. Cross entropy cost function J \u0398 = \u22121 m \u2211 i = 1 m \u2211 k = 1 K yk i log pk i \u2022 yk i  is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to 0. Notice that when there are just two classes (K = 2), this cost function is equivalent to \u2192 \u201csunny\u201d on just one bit (0) and the other seven options on 4 bits (starting with a 1). Cross entropy measures the average number of bits you actually send per option. If your assumption about the weather is perfect, cross entropy will just be equal to the entropy of the weather itself (i.e., its intrinsic unpredictability). But if your assump\u2010 tions are wrong (e.g., if it rains often), cross entropy will be greater by an amount  called the Kullback\u2013Leibler divergence. \u2192 the Logistic Regression\u2019s cost function (log loss; see Equation 4-17). Cross Entropy Cross entropy originated from information theory. Suppose you want to efficiently transmit information about the weather every day. If there are eight options (sunny, rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you think it will be sunny almost every day, it would be much more efficient to code\n- [Score: 0.232] a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters). Equation 4-22. Cross entropy cost function J \u0398 = \u22121 m \u2211 i = 1 m \u2211 k = 1 K yk i log pk i \u2022 yk i  is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to 0. Notice that when there are just two classes (K = 2), this cost function is equivalent to \u2192 The cross entropy between two probability distributions p and q is defined as H p, q = \u2212\u2211x p x log q x  (at least when the distributions are discrete). The gradient vector of this cost function with regards to \u03b8(k) is given by Equation 4-23: Equation 4-23. Cross entropy gradient vector for class k \u2207 \u03b8 k J \u0398 = 1 m \u2211 i = 1 m pk i \u2212yk i x i Now you can compute the gradient vector for every class, then use Gradient Descent \u2192 the Logistic Regression\u2019s cost function (log loss; see Equation 4-17). Cross Entropy Cross entropy originated from information theory. Suppose you want to efficiently transmit information about the weather every day. If there are eight options (sunny, rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you think it will be sunny almost every day, it would be much more efficient to code\n- [Score: 0.214] a low probability for a target class. Cross entropy is frequently used to measure how well a set of estimated class probabilities match the target classes (we will use it again several times in the following chapters). Equation 4-22. Cross entropy cost function J \u0398 = \u22121 m \u2211 i = 1 m \u2211 k = 1 K yk i log pk i \u2022 yk i  is equal to 1 if the target class for the ith instance is k; otherwise, it is equal to 0. Notice that when there are just two classes (K = 2), this cost function is equivalent to \u2192 max_cross_entropy_with_logits(): it computes the cross entropy based on the Training a DNN Using Plain TensorFlow  |  269 \u2192 the Logistic Regression\u2019s cost function (log loss; see Equation 4-17). Cross Entropy Cross entropy originated from information theory. Suppose you want to efficiently transmit information about the weather every day. If there are eight options (sunny, rainy, etc.), you could encode each option using 3 bits since 23 = 8. However, if you think it will be sunny almost every day, it would be much more efficient to code"
    }
  },
  {
    "query": "What is mean squared error?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is mean squared error?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is mean squared error?\nRELATED EVIDENCE PATHS:\n- [Score: 0.478] Absolute Error (also called the Average Absolute Deviation; see Equation 2-2): Equation 2-2. Mean Absolute Error MAE X, h = 1 m \u2211 i = 1 m h x i \u2212y i Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible: \u2022 Computing the root of a sum of squares (RMSE) corresponds to the Euclidian norm: it is the notion of distance you are familiar with. It is also called the \u21132 \u2192 Equation 2-1. Root Mean Square Error (RMSE) RMSE X, h = 1 m \u2211 i = 1 m h x i \u2212y i 2 Look at the Big Picture  |  37\n- [Score: 0.217] Absolute Error (also called the Average Absolute Deviation; see Equation 2-2): Equation 2-2. Mean Absolute Error MAE X, h = 1 m \u2211 i = 1 m h x i \u2212y i Both the RMSE and the MAE are ways to measure the distance between two vectors: the vector of predictions and the vector of target values. Various distance measures, or norms, are possible: \u2022 Computing the root of a sum of squares (RMSE) corresponds to the Euclidian norm: it is the notion of distance you are familiar with. It is also called the \u21132 \u2192 \u2022 h is your system\u2019s prediction function, also called a hypothesis. When your system is given an instance\u2019s feature vector x(i), it outputs a predicted value \u0177(i) = h(x(i)) for that instance (\u0177 is pronounced \u201cy-hat\u201d). \u2014 For example, if your system predicts that the median housing price in the first district is $158,400, then \u0177(1) = h(x(1)) = 158,400. The prediction error for this district is \u0177(1) \u2013 y(1) = 2,000. \u2022 RMSE(X,h) is the cost function measured on the set of examples using your \u2192 Equation 2-1. Root Mean Square Error (RMSE) RMSE X, h = 1 m \u2211 i = 1 m h x i \u2212y i 2 Look at the Big Picture  |  37"
    }
  },
  {
    "query": "What is a support vector machine (SVM)?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a support vector machine (SVM)?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 21,
      "prompt": "QUERY: What is a support vector machine (SVM)?\nRELATED EVIDENCE PATHS:\n- [Score: 0.641] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.338] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 SVM classification classes. Table 5-1. Comparison of Scikit-Learn classes for SVM classification Class Time complexity Out-of-core support Scaling required Kernel trick LinearSVC O(m \u00d7 n) No Yes No SGDClassifier O(m \u00d7 n) Yes Yes No SVC O(m\u00b2 \u00d7 n) to O(m\u00b3 \u00d7 n) No Yes Yes SVM Regression As we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup\u2010 port linear and nonlinear classification, but it also supports linear and nonlinear \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.330] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.320] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 150  |  Chapter 5: Support Vector Machines \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.295] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 \u2022 Logistic Regression \u2022 Support Vector Machines (SVMs) \u2022 Decision Trees and Random Forests \u2022 Neural networks2 Types of Machine Learning Systems  |  9 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.284] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 Equation 5-2. Equation 5-2. Linear SVM classifier prediction y = 0 if wT \u00b7 x + b < 0, 1 if wT \u00b7 x + b \u22650 158  |  Chapter 5: Support Vector Machines \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.272] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 linear data, one with a large margin (\u03f5 = 1.5) and the other with a small margin (\u03f5 = 0.5). 156  |  Chapter 5: Support Vector Machines \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.265] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 7 \u201cIncremental and Decremental Support Vector Machine Learning,\u201d G. Cauwenberghs, T. Poggio (2001). 8 \u201cFast Kernel Classifiers with Online and Active Learning,\u201c A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005). It is also possible to implement online kernelized SVMs\u2014for example, using \u201cIncre\u2010 mental and Decremental SVM Learning\u201d7 or \u201cFast Kernel Classifiers with Online and Active Learning.\u201d8 However, these are implemented in Matlab and C++. For large- \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.264] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 trains a linear SVM model (using the LinearSVC class with C = 1 and the hinge loss Linear SVM Classification  |  149 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.255] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 from sklearn.svm import SVR svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1) svm_poly_reg.fit(X, y) SVMs can also be used for outlier detection; see Scikit-Learn\u2019s doc\u2010 umentation for more details. Under the Hood This section explains how SVMs make predictions and how their training algorithms work, starting with linear SVM classifiers. You can safely skip it and go straight to the exercises at the end of this chapter if you are just getting started with Machine Learn\u2010 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.251] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 SVM Regression                                                                                                           156 Under the Hood                                                                                                           158 Decision Function and Predictions                                                                       158 Training Objective                                                                                                   159 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.240] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 scale nonlinear problems, you may want to consider using neural networks instead  (see Part II). Exercises 1. What is the fundamental idea behind Support Vector Machines? 2. What is a support vector? 3. Why is it important to scale the inputs when using SVMs? 4. Can an SVM classifier output a confidence score when it classifies an instance? What about a probability? 5. Should you use the primal or the dual form of the SVM problem to train a model \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.234] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 try the Gaussian RBF kernel as well; it works well in most cases. Then if you have spare time and computing power, you can also experiment with a few other kernels using cross-validation and grid search, especially if there are kernels specialized for your training set\u2019s data structure. Nonlinear SVM Classification  |  155 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.227] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 12. See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 5: Support Vector Machines 1. The fundamental idea behind Support Vector Machines is to fit the widest possi\u2010 ble \u201cstreet\u201d between the classes. In other words, the goal is to have the largest pos\u2010 sible margin between the decision boundary that separates the two classes and the training instances. When performing soft margin classification, the SVM \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.215] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 Types of Machine Learning Systems  |  17 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.214] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 Nonlinear SVM Classification Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features (as you did in Chapter 4); in some cases this can result in a linearly separable dataset. Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.213] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 regression (see Linear Regression) Ridge Regression, 129-131, 134 SVM, 147-150 Linear Regression, 20, 69, 107-123, 134 computational complexity, 112 Gradient Descent in, 113-123 learning curves in, 125-129 Normal Equation, 110-112 regularizing models (see regularization) using Stochastic Gradient Descent (SGD), 121 with TensorFlow, 237-238 linear SVM classification, 147-150 linear threshold units (LTUs), 259 Lipschitz continuous, 115 LLE (Locally Linear Embedding), 223-225 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.213] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 84, 131 stochastic neurons, 522 stochastic policy, 446 stratified sampling, 51-53, 85 stride, 361 string kernels, 155 string_input_producer(), 345 strong learners, 184 subderivatives, 166 subgradient vector, 133 subsample, 201, 367 supervised learning, 8-9 Support Vector Machines (SVMs), 96, 147-168 decision function and predictions, 158-159 dual problem, 509-511 kernelized SVM, 163-166 linear classification, 147-150 mechanics of, 158-167 nonlinear classification, 151-156 online SVMs, 166-167 \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.210] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 port Vector Machines with different kernels, possibly a neural network, etc.), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models. 72  |  Chapter 2: End-to-End Machine Learning Project \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.202] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 StandardScaler, 116, 239, 266 SVM classification classes, 156 TF.Learn, 233 user guide, xvi score(), 62 search space, 75, 272 second-order partial derivatives (Hessians), 304 542  |  Index \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151\n- [Score: 0.200] CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 5. Support Vector Machines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  147 Linear SVM Classification                                                                                          147 Soft Margin Classification                                                                                       148 Nonlinear SVM Classification                                                                                   151"
    }
  },
  {
    "query": "How do kernels work in SVM?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "How do kernels work in SVM?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 10,
      "prompt": "QUERY: How do kernels work in SVM?\nRELATED EVIDENCE PATHS:\n- [Score: 0.432] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.254] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 Figure 5-6. Linear SVM classifier using polynomial features Polynomial Kernel Adding polynomial features is simple to implement and can work great with all sorts of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it cannot deal with very complex datasets, and with a high polynomial degree it creates a huge number of features, making the model too slow. Fortunately, when using SVMs you can apply an almost miraculous mathematical \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.246] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 CHAPTER 5 Support Vector Machines A Support Vector Machine (SVM) is a very powerful and versatile Machine Learning model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is one of the most popular models in Machine Learning, and any\u2010 one interested in Machine Learning should have it in their toolbox. SVMs are partic\u2010 ularly well suited for classification of complex but small- or medium-sized datasets. \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.245] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 SVM classification classes. Table 5-1. Comparison of Scikit-Learn classes for SVM classification Class Time complexity Out-of-core support Scaling required Kernel trick LinearSVC O(m \u00d7 n) No Yes No SGDClassifier O(m \u00d7 n) Yes Yes No SVC O(m\u00b2 \u00d7 n) to O(m\u00b3 \u00d7 n) No Yes Yes SVM Regression As we mentioned earlier, the SVM algorithm is quite versatile: not only does it sup\u2010 port linear and nonlinear classification, but it also supports linear and nonlinear \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.223] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 from sklearn.svm import SVR svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1) svm_poly_reg.fit(X, y) SVMs can also be used for outlier detection; see Scikit-Learn\u2019s doc\u2010 umentation for more details. Under the Hood This section explains how SVMs make predictions and how their training algorithms work, starting with linear SVM classifiers. You can safely skip it and go straight to the exercises at the end of this chapter if you are just getting started with Machine Learn\u2010 \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.222] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 1 \u201cA Dual Coordinate Descent Method for Large-scale Linear SVM,\u201d Lin et al. (2008). 2 \u201cSequential Minimal Optimization (SMO),\u201d J. Platt (1998). Computational Complexity The LinearSVC class is based on the liblinear library, which implements an optimized algorithm for linear SVMs.1 It does not support the kernel trick, but it scales almost linearly with the number of training instances and the number of features: its training time complexity is roughly O(m \u00d7 n). \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.217] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 7 \u201cIncremental and Decremental Support Vector Machine Learning,\u201d G. Cauwenberghs, T. Poggio (2001). 8 \u201cFast Kernel Classifiers with Online and Active Learning,\u201c A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005). It is also possible to implement online kernelized SVMs\u2014for example, using \u201cIncre\u2010 mental and Decremental SVM Learning\u201d7 or \u201cFast Kernel Classifiers with Online and Active Learning.\u201d8 However, these are implemented in Matlab and C++. For large- \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.215] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 Equation 5-2. Equation 5-2. Linear SVM classifier prediction y = 0 if wT \u00b7 x + b < 0, 1 if wT \u00b7 x + b \u22650 158  |  Chapter 5: Support Vector Machines \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.210] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 function is equal to 0: it is the intersection of two planes, which is a straight line (rep\u2010 resented by the thick solid line).3 Figure 5-12. Decision function for the iris dataset The dashed lines represent the points where the decision function is equal to 1 or \u20131: they are parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear SVM classifier means finding the value of w and b that \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to\n- [Score: 0.202] This chapter will explain the core concepts of SVMs, how to use them, and how they work. Linear SVM Classification The fundamental idea behind SVMs is best explained with some pictures. Figure 5-1 shows part of the iris dataset that was introduced at the end of Chapter 4. The two classes can clearly be separated easily with a straight line (they are linearly separable). The left plot shows the decision boundaries of three possible linear classifiers. The \u2192 Nonlinear SVM Classification Although linear SVM classifiers are efficient and work surprisingly well in many cases, many datasets are not even close to being linearly separable. One approach to handling nonlinear datasets is to add more features, such as polynomial features (as you did in Chapter 4); in some cases this can result in a linearly separable dataset. Consider the left plot in Figure 5-5: it represents a simple dataset with just one feature \u2192 went through the trouble of actually transforming the training set then fitting a linear SVM algorithm, but this trick makes the whole process much more computationally efficient. This is the essence of the kernel trick. The function K(a, b) = (aT \u00b7 b)2 is called a 2nd-degree polynomial kernel. In Machine Learning, a kernel is a function capable of computing the dot product \u03d5(a)T \u00b7 \u03d5(b) based only on the original vectors a and b, without having to compute (or even to"
    }
  },
  {
    "query": "What is clustering?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is clustering?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: What is clustering?\nRELATED EVIDENCE PATHS:\n- [Score: 0.385] 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11). \u2192 resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11\n- [Score: 0.230] 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11). \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010 \u2192 resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11\n- [Score: 0.203] 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11). \u2192 Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11"
    }
  },
  {
    "query": "Explain k-means clustering.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain k-means clustering.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Explain k-means clustering.\nRELATED EVIDENCE PATHS:\n- [Score: 0.396] resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction\n- [Score: 0.302] resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010 \u2192 Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction\n- [Score: 0.228] resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, Kernel \u2192 Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction"
    }
  },
  {
    "query": "What is hierarchical clustering?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is hierarchical clustering?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is hierarchical clustering?\nRELATED EVIDENCE PATHS:\n- [Score: 0.446] resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 sentation of a high-dimensional dataset, generally for visualization, clustering, or classification. The neurons are spread across a map (typically 2D for visualization, but it can be any number of dimensions you want), as shown in Figure E-5, and each neuron has a weighted connection to every input (note that the diagram shows just two inputs, but there are typically a very large number, since the whole point of SOMs is to reduce dimensionality). Figure E-5. Self-organizing maps\n- [Score: 0.208] resentation of your data that can easily be plotted (Figure 1-9). These algorithms try to preserve as much structure as they can (e.g., trying to keep separate clusters in the input space from overlapping in the visualization), so you can understand how the data is organized and perhaps identify unsuspected patterns. Figure 1-9. Example of a t-SNE visualization highlighting semantic clusters3 Types of Machine Learning Systems  |  11 \u2192 3 Notice how animals are rather well separated from vehicles, how horses are close to deer but far from birds, and so on. Figure reproduced with permission from Socher, Ganjoo, Manning, and Ng (2013), \u201cT-SNE visual\u2010 ization of the semantic word space.\u201d Figure 1-8. Clustering Visualization algorithms are also good examples of unsupervised learning algorithms: you feed them a lot of complex and unlabeled data, and they output a 2D or 3D rep\u2010 \u2192 sentation of a high-dimensional dataset, generally for visualization, clustering, or classification. The neurons are spread across a map (typically 2D for visualization, but it can be any number of dimensions you want), as shown in Figure E-5, and each neuron has a weighted connection to every input (note that the diagram shows just two inputs, but there are typically a very large number, since the whole point of SOMs is to reduce dimensionality). Figure E-5. Self-organizing maps"
    }
  },
  {
    "query": "What is dimensionality reduction?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is dimensionality reduction?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 30,
      "prompt": "QUERY: What is dimensionality reduction?\nRELATED EVIDENCE PATHS:\n- [Score: 0.793] Main Approaches for Dimensionality Reduction  |  209 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.500] Main Approaches for Dimensionality Reduction  |  209 \u2192 208  |  Chapter 8: Dimensionality Reduction \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.421] Main Approaches for Dimensionality Reduction  |  209 \u2192 The Curse of Dimensionality                                                                                     208 Main Approaches for Dimensionality Reduction                                                   209 Projection                                                                                                                  209 Manifold Learning                                                                                                   212 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.385] Main Approaches for Dimensionality Reduction  |  209 \u2192 able to assume that it probably carries little information. 216  |  Chapter 8: Dimensionality Reduction \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.356] Main Approaches for Dimensionality Reduction  |  209 \u2192 (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, Kernel \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.336] Main Approaches for Dimensionality Reduction  |  209 \u2192 simpler solution; it all depends on the dataset. Hopefully you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms. 212  |  Chapter 8: Dimensionality Reduction \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.328] Main Approaches for Dimensionality Reduction  |  209 \u2192 weights, and O(dm2) for constructing the low-dimensional representations. Unfortu\u2010 nately, the m2 in the last term makes this algorithm scale poorly to very large datasets. Other Dimensionality Reduction Techniques There are many other dimensionality reduction techniques, several of which are available in Scikit-Learn. Here are some of the most popular: \u2022 Multidimensional Scaling (MDS) reduces dimensionality while trying to preserve the distances between the instances (see Figure 8-13). \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.325] Main Approaches for Dimensionality Reduction  |  209 \u2192 Kernel PCA                                                                                                                   220 Selecting a Kernel and Tuning Hyperparameters                                                221 LLE                                                                                                                                 223 Other Dimensionality Reduction Techniques                                                         225 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.322] Main Approaches for Dimensionality Reduction  |  209 \u2192 Stacking                                                                                                                         202 Exercises                                                                                                                        204 8. Dimensionality Reduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  207 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.315] Main Approaches for Dimensionality Reduction  |  209 \u2192 Figure 8-5. Squashing by projecting onto a plane (left) versus unrolling the Swiss roll (right) Main Approaches for Dimensionality Reduction  |  211 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.299] Main Approaches for Dimensionality Reduction  |  209 \u2192 CHAPTER 8 Dimensionality Reduction Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often referred to as the curse of dimensionality. Fortunately, in real-world problems, it is often possible to reduce the number of fea\u2010 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.289] Main Approaches for Dimensionality Reduction  |  209 \u2192 of that second algorithm; if dimensionality reduction did not lose too much information, then the algorithm should perform just as well as when using the original dataset. 8. It can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large num\u2010 ber of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE. This two-step approach will likely yield the \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.288] Main Approaches for Dimensionality Reduction  |  209 \u2192 dequeuing data, 335 describe(), 46 device blocks, 331 device(), 323 dimensionality reduction, 12, 207-227, 417 approaches to Manifold Learning, 212 projection, 209-211 choosing the right number of dimensions, 217 curse of dimensionality, 207-209 and data visualization, 207 Isomap, 226 LLE (Locally Linear Embedding), 223-225 Multidimensional Scaling, 225-226 PCA (Principal Component Analysis), 213-220 t-Distributed Stochastic Neighbor Embed\u2010 ding (t-SNE), 226 discount rate, 453 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.282] Main Approaches for Dimensionality Reduction  |  209 \u2192 good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier. Figure 8-13. Reducing the Swiss roll to 2D using various techniques Exercises 1. What are the main motivations for reducing a dataset\u2019s dimensionality? What are the main drawbacks? 2. What is the curse of dimensionality? 3. Once a dataset\u2019s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why? \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.271] Main Approaches for Dimensionality Reduction  |  209 \u2192 6. In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA? 7. How can you evaluate the performance of a dimensionality reduction algorithm on your dataset? 8. Does it make any sense to chain two different dimensionality reduction algo\u2010 rithms? 9. Load the MNIST dataset (introduced in Chapter 3) and split it into a training set and a test set (take the first 60,000 instances for training, and the remaining \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.258] Main Approaches for Dimensionality Reduction  |  209 \u2192 4. Can PCA be used to reduce the dimensionality of a highly nonlinear dataset? 5. Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%. How many dimensions will the resulting dataset have? 226  |  Chapter 8: Dimensionality Reduction \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.257] Main Approaches for Dimensionality Reduction  |  209 \u2192 Many dimensionality reduction algorithms work by modeling the manifold on which the training instances lie; this is called Manifold Learning. It relies on the manifold assumption, also called the manifold hypothesis, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption is very often empirically observed. Once again, think about the MNIST dataset: all handwritten digit images have some \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.249] Main Approaches for Dimensionality Reduction  |  209 \u2192 in the MNIST problem), you would need more training instances than atoms in the observable universe in order for training instances to be within 0.1 of each other on average, assuming they were spread out uniformly across all dimensions. Main Approaches for Dimensionality Reduction Before we dive into specific dimensionality reduction algorithms, let\u2019s take a look at the two main approaches to reducing dimensionality: projection and Manifold Learning. Projection \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.240] Main Approaches for Dimensionality Reduction  |  209 \u2192 with an RBF kernel (see Chapter 5 for more details about the RBF kernel and the other kernels): from sklearn.decomposition import KernelPCA rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.04) X_reduced = rbf_pca.fit_transform(X) Figure 8-10 shows the Swiss roll, reduced to two dimensions using a linear kernel (equivalent to simply using the PCA class), an RBF kernel, and a sigmoid kernel (Logistic). 220  |  Chapter 8: Dimensionality Reduction \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.238] Main Approaches for Dimensionality Reduction  |  209 \u2192 complex nonlinear decision boundary in the original space. It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear projections for dimensionality reduction. This is called Kernel PCA (kPCA).6 It is often good at preserving clusters of instances after projection, or sometimes even unrolling datasets that lie close to a twisted manifold. For example, the following code uses Scikit-Learn\u2019s KernelPCA class to perform kPCA \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.234] Main Approaches for Dimensionality Reduction  |  209 \u2192 decreasing the learning rate. You could also use early stopping to find the right number of predictors (you probably have too many). For the solutions to exercises 8 and 9, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 8: Dimensionality Reduction 1. Motivations and drawbacks: \u2022 The main motivations for dimensionality reduction are: \u2014 To speed up a subsequent training algorithm (in some cases it may even \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.233] Main Approaches for Dimensionality Reduction  |  209 \u2192 reduction (NLDR) technique. It is a Manifold Learning technique that does not rely on projections like the previous algorithms. In a nutshell, LLE works by first measur\u2010 ing how each training instance linearly relates to its closest neighbors (c.n.), and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This makes it particularly \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.225] Main Approaches for Dimensionality Reduction  |  209 \u2192 However, projection is not always the best approach to dimensionality reduction. In many cases the subspace may twist and turn, such as in the famous Swiss roll toy data\u2010 set represented in Figure 8-4. Figure 8-4. Swiss roll dataset Simply projecting onto a plane (e.g., by dropping x3) would squash different layers of the Swiss roll together, as shown on the left of Figure 8-5. However, what you really want is to unroll the Swiss roll to obtain the 2D dataset on the right of Figure 8-5. \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.223] Main Approaches for Dimensionality Reduction  |  209 \u2192 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.220] Main Approaches for Dimensionality Reduction  |  209 \u2192 Choosing the Right Number of Dimensions Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen\u2010 sionality for data visualization\u2014in that case you will generally want to reduce the dimensionality down to 2 or 3. The following code computes PCA without reducing dimensionality, then computes \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.216] Main Approaches for Dimensionality Reduction  |  209 \u2192 smaller than n. rnd_pca = PCA(n_components=154, svd_solver=\"randomized\") X_reduced = rnd_pca.fit_transform(X_train) Kernel PCA In Chapter 5 we discussed the kernel trick, a mathematical technique that implicitly maps instances into a very high-dimensional space (called the feature space), enabling nonlinear classification and regression with Support Vector Machines. Recall that a linear decision boundary in the high-dimensional feature space corresponds to a \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.214] Main Approaches for Dimensionality Reduction  |  209 \u2192 Polynomial Kernel                                                                                                   152 Adding Similarity Features                                                                                     153 Gaussian RBF Kernel                                                                                               154 Computational Complexity                                                                                    156 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.204] Main Approaches for Dimensionality Reduction  |  209 \u2192 Figure 8-10. Swiss roll reduced to 2D using kPCA with various kernels Selecting a Kernel and Tuning Hyperparameters As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you select the best kernel and hyperparameter values. However, dimensionality reduction is often a preparation step for a supervised learning task (e.g., classification), so you can simply use grid search to select the kernel and hyper\u2010 \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.202] Main Approaches for Dimensionality Reduction  |  209 \u2192 PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets. 7. Intuitively, a dimensionality reduction algorithm performs well if it eliminates a \u2192 Other Dimensionality Reduction Techniques  |  225\n- [Score: 0.200] Main Approaches for Dimensionality Reduction  |  209 \u2192 ing the first d principal components (i.e., the matrix composed of the first d columns of V), as shown in Equation 8-2. Equation 8-2. Projecting the training set down to d dimensions Xd\u2010proj = X \u00b7 Wd The following Python code projects the training set onto the plane defined by the first two principal components: W2 = Vt.T[:, :2] X2D = X_centered.dot(W2) There you have it! You now know how to reduce the dimensionality of any dataset \u2192 Other Dimensionality Reduction Techniques  |  225"
    }
  },
  {
    "query": "Describe principal component analysis (PCA).",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe principal component analysis (PCA).",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: Describe principal component analysis (PCA).\nRELATED EVIDENCE PATHS:\n- [Score: 0.386] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n.\n- [Score: 0.246] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 down to any number of dimensions, while preserving as much variance as possible. Using Scikit-Learn Scikit-Learn\u2019s PCA class implements PCA using SVD decomposition just like we did before. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data): from sklearn.decomposition import PCA pca = PCA(n_components = 2) X2D = pca.fit_transform(X) \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n.\n- [Score: 0.231] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 Figure 8-6. The decision boundary may not always be simpler with lower dimensions PCA Principal Component Analysis (PCA) is by far the most popular dimensionality reduc\u2010 tion algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it. Preserving the Variance Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is repre\u2010 \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n.\n- [Score: 0.222] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 Choosing the Right Number of Dimensions Instead of arbitrarily choosing the number of dimensions to reduce down to, it is generally preferable to choose the number of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of course, you are reducing dimen\u2010 sionality for data visualization\u2014in that case you will generally want to reduce the dimensionality down to 2 or 3. The following code computes PCA without reducing dimensionality, then computes \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n.\n- [Score: 0.213] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 The direction of the principal components is not stable: if you per\u2010 turb the training set slightly and run PCA again, some of the new PCs may point in the opposite direction of the original PCs. How\u2010 ever, they will generally still lie on the same axes. In some cases, a pair of PCs may even rotate or swap, but the plane they define will generally remain the same. So how can you find the principal components of a training set? Luckily, there is a \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n.\n- [Score: 0.211] simple idea behind PCA.4 Principal Components PCA identifies the axis that accounts for the largest amount of variance in the train\u2010 ing set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional data\u2010 set, PCA would also find a third axis, orthogonal to both previous axes, and a fourth, \u2192 PCA. Incremental PCA is also useful for online tasks, when you need to apply PCA on the fly, every time a new instance arrives. Randomized PCA is useful when you want to considerably reduce dimensionality and the dataset fits in memory; in this case, it is much faster than regular PCA. Finally, Kernel PCA is useful for nonlinear datasets. 7. Intuitively, a dimensionality reduction algorithm performs well if it eliminates a \u2192 6 \u201cKernel Principal Component Analysis,\u201d B. Sch\u00f6lkopf, A. Smola, K. M\u00fcller (1999). Randomized PCA Scikit-Learn offers yet another option to perform PCA, called Randomized PCA. This is a stochastic algorithm that quickly finds an approximation of the first d principal components. Its computational complexity is O(m \u00d7 d2) + O(d3), instead of O(m \u00d7 n2) + O(n3), so it is dramatically faster than the previous algorithms when d is much  smaller than n."
    }
  },
  {
    "query": "What is t-SNE?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is t-SNE?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is t-SNE?\nRELATED EVIDENCE PATHS:\n- [Score: 0.201] 9 The geodesic distance between two nodes in a graph is the number of nodes on the shortest path between these nodes. \u2022 Isomap creates a graph by connecting each instance to its nearest neighbors, then reduces dimensionality while trying to preserve the geodesic distances9 between the instances. \u2022 t-Distributed Stochastic Neighbor Embedding (t-SNE) reduces dimensionality while trying to keep similar instances close and dissimilar instances apart. It is \u2192 c(t\u20131) traverses the network from left to right, you can see that it first goes through a forget gate, dropping some memories, and then it adds some new memories via the addition operation (which adds the memories that were selected by an input gate). The result c(t) is sent straight out, without any further transformation. So, at each time step, some memories are dropped and some memories are added. Moreover, after the"
    }
  },
  {
    "query": "What is feature scaling?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is feature scaling?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is feature scaling?\nRELATED EVIDENCE PATHS:\n- [Score: 0.318] Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\u2010 ing the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason. Standardization is quite different: first it subtracts the mean value (so standardized \u2192 \u2022 Decompose features (e.g., categorical, date/time, etc.). \u2022 Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). \u2022 Aggregate features into promising new features. 4. Feature scaling: standardize or normalize features. Short-List Promising Models Notes: \u2022 If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests)."
    }
  },
  {
    "query": "Explain normalization vs standardization.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain normalization vs standardization.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Explain normalization vs standardization.\nRELATED EVIDENCE PATHS:\n- [Score: 0.588] values always have a zero mean), and then it divides by the variance so that the result\u2010 ing distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standard\u2010 ization is much less affected by outliers. For example, suppose a district had a median \u2192 Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\u2010 ing the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason. Standardization is quite different: first it subtracts the mean value (so standardized\n- [Score: 0.279] values always have a zero mean), and then it divides by the variance so that the result\u2010 ing distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standard\u2010 ization is much less affected by outliers. For example, suppose a district had a median \u2192 income equal to 100 (by mistake). Min-max scaling would then crush all the other values from 0\u201315 down to 0\u20130.15, whereas standardization would not be much affec\u2010 ted. Scikit-Learn provides a transformer called StandardScaler for standardization. As with all the transformations, it is important to fit the scalers to the training data only, not to the full dataset (including the test set). Only then can you use them to transform the training set and the test set (and new data). \u2192 Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\u2010 ing the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason. Standardization is quite different: first it subtracts the mean value (so standardized\n- [Score: 0.216] values always have a zero mean), and then it divides by the variance so that the result\u2010 ing distribution has unit variance. Unlike min-max scaling, standardization does not bound values to a specific range, which may be a problem for some algorithms (e.g., neural networks often expect an input value ranging from 0 to 1). However, standard\u2010 ization is much less affected by outliers. For example, suppose a district had a median \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010 \u2192 Min-max scaling (many people call this normalization) is quite simple: values are shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\u2010 ing the min value and dividing by the max minus the min. Scikit-Learn provides a transformer called MinMaxScaler for this. It has a feature_range hyperparameter that lets you change the range if you don\u2019t want 0\u20131 for some reason. Standardization is quite different: first it subtracts the mean value (so standardized"
    }
  },
  {
    "query": "What is regularization?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is regularization?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 8,
      "prompt": "QUERY: What is regularization?\nRELATED EVIDENCE PATHS:\n- [Score: 0.539] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.327] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 Regularized Linear Models As we saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model is to reduce the number of polynomial degrees. For a linear model, regularization is typically achieved by constraining the weights of \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.248] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 regularization to avoid overfitting. The question is: how do you choose the value of the regularization hyperparameter? One option is to train 100 different models using 100 different values for this hyperparameter. Suppose you find the best hyperparame\u2010 ter value that produces a model with the lowest generalization error, say just 5% error. So you launch this model into production, but unfortunately it does not perform as well as expected and produces 15% errors. What just happened? \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.244] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 1 It is often the case that a learning algorithm will try to optimize a different function than the performance measure used to evaluate the final model. This is generally because that function is easier to compute, because it has useful differentiation properties that the performance measure lacks, or because we want to constrain the model during training, as we will see when we discuss regularization. This can be written much more concisely using a vectorized form, as shown in Equa\u2010 tion 4-2. \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.235] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 Irreducible error This part is due to the noisiness of the data itself. The only way to reduce this part of the error is to clean up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers). Increasing a model\u2019s complexity will typically increase its variance and reduce its bias. Conversely, reducing a model\u2019s complexity increases its bias and reduces its variance.  This is why it is called a tradeoff. Regularized Linear Models \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.229] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights. Ridge Regression Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin\u2010 ear Regression: a regularization term equal to \u03b1\u2211i = 1 n \u03b8i 2 is added to the cost function.  This forces the learning algorithm to not only fit the data but also keep the model \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.211] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to\n- [Score: 0.209] (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data or by constraining the model \u2022 To gather more training data \u2022 To reduce the noise in the training data (e.g., fix data errors and remove outliers) Constraining a model to make it simpler and reduce the risk of overfitting is called regularization. For example, the linear model we defined earlier has two parameters, \u2192 max-norm regularization: for each neuron, it constrains the weights w of the incom\u2010 ing connections such that \u2225 w \u22252 \u2264 r, where r is the max-norm hyperparameter and \u2225 \u00b7 \u22252 is the \u21132 norm. We typically implement this constraint by computing \u2225w\u22252 after each training step and clipping w if needed (w w r \u2225w \u22252). Reducing r increases the amount of regularization and helps reduce overfitting. Max- norm regularization can also help alleviate the vanishing/exploding gradients prob\u2010 \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to"
    }
  },
  {
    "query": "Describe L1 regularization (Lasso).",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe L1 regularization (Lasso).",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 9,
      "prompt": "QUERY: Describe L1 regularization (Lasso).\nRELATED EVIDENCE PATHS:\n- [Score: 0.449] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.306] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 Elastic Net Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12). Equation 4-12. Elastic Net cost function J \u03b8 = MSE \u03b8 + r\u03b1 \u2211 i = 1 n \u03b8i + 1 \u2212r 2 \u03b1 \u2211 i = 1 n \u03b8i 2 \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.262] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6). \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.255] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights. Ridge Regression Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin\u2010 ear Regression: a regularization term equal to \u03b1\u2211i = 1 n \u03b8i 2 is added to the cost function.  This forces the learning algorithm to not only fit the data but also keep the model \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.242] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.214] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 regularization, 27-28, 30, 129-136 data augmentation, 313-314 Decision Trees, 175-176 dropout, 309-311 early stopping, 135-136, 307 Elastic Net, 134 Lasso Regression, 132-134 max-norm, 311-313 Ridge Regression, 129-131 shrinkage, 199 \u2113 1 and \u2113 2 regularization, 307-308 REINFORCE algorithms, 454 Reinforcement Learning (RL), 13-14, 443-476 actions, 453-454 credit assignment problem, 453-454 discount rate, 453 examples of, 444 Markov decision processes, 459-463 neural network policies, 451-453 \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.212] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 thing but uses an \u21132 penalty instead. The regularized minimum is closer to \u03b8 = 0 than the unregularized minimum, but the weights do not get fully eliminated. Figure 4-19. Lasso versus Ridge regularization On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is because the slope changes abruptly at \u03b82 = 0. You need to gradually reduce the learning rate in order to actually converge to the global minimum. \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.210] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 optimizer to zero out as many weights as it can (as discussed in Chapter 4 about Lasso Regression). However, in some cases these techniques may remain insufficient. One last option is to apply Dual Averaging, often called Follow The Regularized Leader (FTRL), a techni\u2010 que proposed by Yurii Nesterov.17 When used with \u21131 regularization, this technique often leads to very sparse models. TensorFlow implements a variant of FTRL called FTRL-Proximal18 in the FTRLOptimizer class. 304  | \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134\n- [Score: 0.206] Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 regression (see Linear Regression) Ridge Regression, 129-131, 134 SVM, 147-150 Linear Regression, 20, 69, 107-123, 134 computational complexity, 112 Gradient Descent in, 113-123 learning curves in, 125-129 Normal Equation, 110-112 regularizing models (see regularization) using Stochastic Gradient Descent (SGD), 121 with TensorFlow, 237-238 linear SVM classification, 147-150 linear threshold units (LTUs), 259 Lipschitz continuous, 115 LLE (Locally Linear Embedding), 223-225 \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134"
    }
  },
  {
    "query": "Describe L2 regularization (Ridge).",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe L2 regularization (Ridge).",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 7,
      "prompt": "QUERY: Describe L2 regularization (Ridge).\nRELATED EVIDENCE PATHS:\n- [Score: 0.511] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.262] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 Lasso Regression Least Absolute Shrinkage and Selection Operator Regression (simply called Lasso Regression) is another regularized version of Linear Regression: just like Ridge Regression, it adds a regularization term to the cost function, but it uses the \u21131 norm of the weight vector instead of half the square of the \u21132 norm (see Equation 4-10). Equation 4-10. Lasso Regression cost function J \u03b8 = MSE \u03b8 + \u03b1 \u2211 i = 1 n \u03b8i \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.251] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 thing but uses an \u21132 penalty instead. The regularized minimum is closer to \u03b8 = 0 than the unregularized minimum, but the weights do not get fully eliminated. Figure 4-19. Lasso versus Ridge regularization On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is because the slope changes abruptly at \u03b82 = 0. You need to gradually reduce the learning rate in order to actually converge to the global minimum. \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.241] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 Regularized Linear Models                                                                                         129 Ridge Regression                                                                                                      129 Lasso Regression                                                                                                      132 Elastic Net                                                                                                                 134 \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.232] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 Elastic Net Elastic Net is a middle ground between Ridge Regression and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso\u2019s regularization terms, and you can control the mix ratio r. When r = 0, Elastic Net is equivalent to Ridge Regression, and when r = 1, it is equivalent to Lasso Regression (see Equation 4-12). Equation 4-12. Elastic Net cost function J \u03b8 = MSE \u03b8 + r\u03b1 \u2211 i = 1 n \u03b8i + 1 \u2212r 2 \u03b1 \u2211 i = 1 n \u03b8i 2 \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.229] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 tion (a variant of Equation 4-9 using a matrix factorization technique by Andr\u00e9-Louis Cholesky): >>> from sklearn.linear_model import Ridge >>> ridge_reg = Ridge(alpha=1, solver=\"cholesky\") >>> ridge_reg.fit(X, y) >>> ridge_reg.predict([[1.5]]) array([[ 1.55071465]]) And using Stochastic Gradient Descent:14 >>> sgd_reg = SGDRegressor(penalty=\"l2\") >>> sgd_reg.fit(X, y.ravel()) >>> sgd_reg.predict([[1.5]]) array([ 1.13500145]) \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6).\n- [Score: 0.210] array([ 1.13500145]) The penalty hyperparameter sets the type of regularization term to use. Specifying \"l2\" indicates that you want SGD to add a regularization term to the cost function  equal to half the square of the \u21132 norm of the weight vector: this is simply Ridge Regression. Regularized Linear Models  |  131 \u2192 the model. We will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways to constrain the weights. Ridge Regression Ridge Regression (also called Tikhonov regularization) is a regularized version of Lin\u2010 ear Regression: a regularization term equal to \u03b1\u2211i = 1 n \u03b8i 2 is added to the cost function.  This forces the learning algorithm to not only fit the data but also keep the model \u2192 tion 4-8 presents the Ridge Regression cost function.11 Equation 4-8. Ridge Regression cost function J \u03b8 = MSE \u03b8 + \u03b11 2 \u2211 i = 1 n \u03b8i 2 Note that the bias term \u03b80 is not regularized (the sum starts at i = 1, not 0). If we define w as the vector of feature weights (\u03b81 to \u03b8n), then the regularization term is simply equal to \u00bd(\u2225 w \u22252)2, where \u2225 \u00b7 \u22252 represents the \u21132 norm of the weight vector.12 For Gradient Descent, just add \u03b1w to the MSE gradient vector (Equation 4-6)."
    }
  },
  {
    "query": "What is dropout in neural networks?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is dropout in neural networks?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: What is dropout in neural networks?\nRELATED EVIDENCE PATHS:\n- [Score: 0.573] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called\n- [Score: 0.335] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 20 \u201cImproving neural networks by preventing co-adaptation of feature detectors,\u201d G. Hinton et al. (2012). 21 \u201cDropout: A Simple Way to Prevent Neural Networks from Overfitting,\u201d N. Srivastava et al. (2014). Dropout The most popular regularization technique for deep neural networks is arguably dropout. It was proposed20 by G. E. Hinton in 2012 and further detailed in a paper21 by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of- \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called\n- [Score: 0.312] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 get a total input signal roughly twice as large as what the network was trained on, and it is unlikely to perform well. More generally, we need to multiply each input connec\u2010 tion weight by the keep probability (1 \u2013 p) after training. Alternatively, we can divide each neuron\u2019s output by the keep probability during training (these alternatives are not perfectly equivalent, but they work equally well). To implement dropout using TensorFlow, you can simply apply the tf.layers.drop \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called\n- [Score: 0.230] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 porarily \u201cdropped out,\u201d meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-9). The hyperparameter p is called the dropout rate, and it is typically set to 50%. After training, neurons don\u2019t get dropped anymore. And that\u2019s all (except for a technical detail we will discuss momen\u2010 tarily). Figure 11-9. Dropout regularization It is quite surprising at first that this rather brutal technique works at all. Would a \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called\n- [Score: 0.207] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 out() function to the input layer and/or to the output of any hidden layer you want. During training, this function randomly drops some items (setting them to 0) and divides the remaining items by the keep probability. After training, this function does nothing at all. The following code applies dropout regularization to our three-layer  neural network: [...] training = tf.placeholder_with_default(False, shape=(), name='training') dropout_rate = 0.5  # == 1 - keep_prob \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called\n- [Score: 0.200] pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. In the end you get a more robust network that generalizes bet\u2010 ter. Another way to understand the power of dropout is to realize that a unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of 2N possible networks (where N is the total number of drop\u2010 \u2192 X_drop = tf.layers.dropout(X, dropout_rate, training=training) with tf.name_scope(\"dnn\"):     hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,                               name=\"hidden1\")     hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training) 310  |  Chapter 11: Training Deep Neural Nets \u2192 small ones. Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort. Dropconnect is a variant of dropout where individual connections are dropped randomly rather than whole neurons. In general drop\u2010 out performs better. Max-Norm Regularization Another regularization technique that is quite popular for neural networks is called"
    }
  },
  {
    "query": "What is batch normalization?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is batch normalization?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 14,
      "prompt": "QUERY: What is batch normalization?\nRELATED EVIDENCE PATHS:\n- [Score: 0.675] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.362] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 8 Many researchers argue that it is just as good, or even better, to place the batch normalization layers after (rather than before) the activations. Implementing Batch Normalization with TensorFlow TensorFlow provides a tf.nn.batch_normalization() function that simply centers and normalizes the inputs, but you must compute the mean and standard deviation yourself (based on the mini-batch data during training or on the full dataset during \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.349] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 7 \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,\u201d S. Ioffe and C. Szegedy (2015). Batch Normalization Although using He initialization along with ELU (or any variant of ReLU) can signifi\u2010 cantly reduce the vanishing/exploding gradients problems at the beginning of train\u2010 ing, it doesn\u2019t guarantee that they won\u2019t come back during training. In a 2015 paper,7 Sergey Ioffe and Christian Szegedy proposed a technique called \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.320] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.303] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 function whether it should use the current mini-batch\u2019s mean and standard deviation (during training) or the whole training set\u2019s mean and standard deviation (during testing). Then, we alternate fully connected layers and batch normalization layers: the fully connected layers are created using the tf.layers.dense() function, just like we did in Chapter 10. Note that we don\u2019t specify any activation function for the fully connec\u2010 \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.279] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 ted layers because we want to apply the activation function after each batch normal\u2010 ization  layer.8  We  create  the  batch  normalization  layers  using  the 286  |  Chapter 11: Training Deep Neural Nets \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.273] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 dropout, described later in the chapter). Batch Normalization does, however, add some complexity to the model (although it removes the need for normalizing the input data since the first hidden layer will take care of that, provided it is batch-normalized). Moreover, there is a runtime penalty: the neural network makes slower predictions due to the extra computations required at each layer. So if you need predictions to be lightning-fast, you may want to check \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.263] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 deviation, so instead you simply use the whole training set\u2019s mean and standard devi\u2010 ation. These are typically efficiently computed during training using a moving aver\u2010 age. So, in total, four parameters are learned for each batch-normalized layer: \u03b3 (scale), \u03b2 (offset), \u03bc (mean), and \u03c3 (standard deviation). The authors demonstrated that this technique considerably improved all the deep neural networks they experimented with. The vanishing gradients problem was \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.247] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 operations, create a variable initializer, create a Saver, and so on. The execution phase is also pretty much the same, with two exceptions. First, during training, whenever you run an operation that depends on the batch_normaliza tion() layer, you need to set the training placeholder to True. Second, the batch_normalization() function creates a few operations that must be evaluated at each step during training in order to update the moving averages (recall that these \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.241] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 tf.layers.batch_normalization() function, setting its training and momentum parameters. The BN algorithm uses exponential decay to compute the running aver\u2010 ages, which is why it requires the momentum parameter: given a new value v, the run\u2010 ning average v is updated through the equation: v v \u00d7 momentum + v \u00d7 1 \u2212momentum A good momentum value is typically close to 1\u2014for example, 0.9, 0.99, or 0.999 (you want more 9s for larger datasets and smaller mini-batches). \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.240] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 Nonsaturating Activation Functions                                                                     281 Batch Normalization                                                                                                284 Gradient Clipping                                                                                                    288 Reusing Pretrained Layers                                                                                          289 \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.237] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 bn2_act = tf.nn.elu(bn2) logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\") logits = tf.layers.batch_normalization(logits_before_bn, training=training,                                        momentum=0.9) Let\u2019s walk through this code. The first lines are fairly self-explanatory, until we define the training placeholder: we will set it to True during training, but otherwise it will default to False. This will be used to tell the tf.layers.batch_normalization() \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.214] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 n_outputs = 10 X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\") training = tf.placeholder_with_default(False, shape=(), name='training') hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\") bn1 = tf.layers.batch_normalization(hidden1, training=training, momentum=0.9) bn1_act = tf.nn.elu(bn1) hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\") bn2 = tf.layers.batch_normalization(hidden2, training=training, momentum=0.9) bn2_act = tf.nn.elu(bn2) \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010\n- [Score: 0.202] Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 sess.run([training_op, extra_update_ops],                      feed_dict={training: True, X: X_batch, y: y_batch})         accuracy_val = accuracy.eval(feed_dict={X: mnist.test.images,                                                 y: mnist.test.labels})         print(epoch, \"Test accuracy:\", accuracy_val)     save_path = saver.save(sess, \"./my_model_final.ckpt\") That\u2019s all! In this tiny example with just two layers, it\u2019s unlikely that Batch Normaliza\u2010 \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010"
    }
  },
  {
    "query": "Explain convolutional neural networks (CNNs).",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain convolutional neural networks (CNNs).",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 37,
      "prompt": "QUERY: Explain convolutional neural networks (CNNs).\nRELATED EVIDENCE PATHS:\n- [Score: 0.551] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.299] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.282] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 assemble them into higher-level features in the next hidden layer, and so on. This hierarchical structure is common in real-world images, which is one of the reasons why CNNs work so well for image recognition. Convolutional Layer  |  359 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.277] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 while the rest gets blurred. Similarly, the upper-right image is what you get if all neu\u2010 rons use the horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter gives  Convolutional Layer  |  361 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.266] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 while higher layers combine the lower-level features into larger features. This works well with most natural images, giving CNNs a decisive head start com\u2010 pared to DNNs. 2. Let\u2019s compute how many parameters the CNN has. Since its first convolutional layer has 3 \u00d7 3 kernels, and the input has three channels (red, green, and blue), then each feature map has 3 \u00d7 3 \u00d7 3 weights, plus a bias term. That\u2019s 28 parame\u2010 ters per feature map. Since this first convolutional layer has 100 feature maps, it \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.264] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 larger images because of the huge number of parameters it requires. For example, a 100 \u00d7 100 image has 10,000 pixels, and if the first layer has just 1,000 neurons (which already severely restricts the amount of information transmitted to the next layer), this means a total of 10 million connections. And that\u2019s just the first layer. CNNs solve this problem using partially connected layers. Convolutional Layer The most important building block of a CNN is the convolutional layer:6 neurons in \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.264] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 Now let\u2019s look at ResNet\u2019s architecture (see Figure 13-14). It is actually surprisingly simple. It starts and ends exactly like GoogLeNet (except without a dropout layer), and in between is just a very deep stack of simple residual units. Each residual unit is composed of two convolutional layers, with Batch Normalization (BN) and ReLU activation, using 3 \u00d7 3 kernels and preserving spatial dimensions (stride 1, SAME padding). Figure 13-14. ResNet architecture \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.261] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 steroids, able to output feature maps that capture complex patterns at various scales. The number of convolutional kernels for each convolutional layer is a hyperparameter. Unfortunately, this means that you have six more hyperparameters to tweak for every inception layer you add. Now let\u2019s look at the architecture of the GoogLeNet CNN (see Figure 13-11). It is so deep that we had to represent it in three columns, but GoogLeNet is actually one tall \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.259] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 putationally expensive layers. \u2022 Second, each pair of convolutional layers ([1 \u00d7 1, 3 \u00d7 3] and [1 \u00d7 1, 5 \u00d7 5]) acts like a single, powerful convolutional layer, capable of capturing more complex patterns. Indeed, instead of sweeping a simple linear classifier across the image (as a single convolutional layer does), this pair of convolutional layers sweeps a two-layer neural network across the image. In short, you can think of the whole inception module as a convolutional layer on \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.254] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 \u2022 Because consecutive layers are only partially connected and because it heavily reuses its weights, a CNN has many fewer parameters than a fully connected DNN, which makes it much faster to train, reduces the risk of overfitting, and requires much less training data. \u2022 When a CNN has learned a kernel that can detect a particular feature, it can detect that feature anywhere on the image. In contrast, when a DNN learns a \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.247] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 was much deeper than previous CNNs (see Figure 13-11). This was made possible by sub-networks called inception modules,11 which allow GoogLeNet to use parameters much more efficiently than previous architectures: GoogLeNet actually has 10 times fewer parameters than AlexNet (roughly 6 million instead of 60 million). Figure 13-10 shows the architecture of an inception module. The notation \u201c3 \u00d7 3 + 2(S)\u201d means that the layer uses a 3 \u00d7 3 kernel, stride 2, and SAME padding. The input \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.246] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 the inputs are passed through a 1 \u00d7 1 convolutional layer with stride 2 and the right number of output feature maps (see Figure 13-15). Figure 13-15. Skip connection when changing feature map size and depth 378  |  Chapter 13: Convolutional Neural Networks \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.246] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 instead of stacking a pooling layer on top of each convolutional layer. The main innovation in GoogLeNet is the introduction of inception modules, which make it possible to have a much deeper net than previous CNN architectures, with fewer parameters. Finally, ResNet\u2019s main innovation is the introduction of skip connec\u2010 tions, which make it possible to go well beyond 100 layers. Arguably, its simplic\u2010 ity and consistency are also rather innovative. \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.239] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 horizontal white line in the middle. Once again, neurons using these weights will ignore everything in their receptive field except for the central horizontal line. Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in Figure 13-5 (bottom image), the layer will output the top-left image. Notice that the vertical white lines get enhanced \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.236] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities). Figure 13-9. Typical CNN architecture A common mistake is to use convolution kernels that are too large. You can often get the same effect as one convolutional layer with a 9 \u00d7 9 kernel by stacking two layers with 3 \u00d7 3 kernels, for a lot less compute and parameters. Over the years, variants of this fundamental architecture have been developed, lead\u2010 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.236] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 feature in one location, it can detect it only in that particular location. Since images typically have very repetitive features, CNNs are able to generalize much better than DNNs for image processing tasks such as classification, using fewer training examples. \u2022 Finally, a DNN has no prior knowledge of how pixels are organized; it does not know that nearby pixels are close. A CNN\u2019s architecture embeds this prior knowledge. Lower layers typically identify features in small areas of the images, \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.230] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 but it extends across all the previous layers\u2019 feature maps. In short, a convolutional layer simultaneously applies multiple filters to its inputs, making it capable of detect\u2010 ing multiple features anywhere in its inputs. The fact that all neurons in a feature map share the same parame\u2010 ters dramatically reduces the number of parameters in the model, but most importantly it means that once the CNN has learned to recognize a pattern in one location, it can recognize it in any other \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.225] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 lowed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps) thanks to the convolutional layers (see Figure 13-9). At the top of the stack, a regular feedforward neural network is added, composed of a few \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.224] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 vertical white line in the middle, and the other with a horizontal white line in the middle), and applies them to both images using a convolutional layer built using Ten\u2010 sorFlow\u2019s tf.nn.conv2d() function (with zero padding and a stride of 2). Finally, it plots one of the resulting feature maps (similar to the top-right image in Figure 13-5). 364  |  Chapter 13: Convolutional Neural Networks \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.222] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 Up to now, for simplicity, we have represented each convolutional layer as a thin 2D layer, but in reality it is composed of several feature maps of equal sizes, so it is more accurately represented in 3D (see Figure 13-6). Within one feature map, all neurons share the same parameters (weights and bias term), but different feature maps may have different parameters. A neuron\u2019s receptive field is the same as described earlier, \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.222] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 9 \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d A. Krizhevsky et al. (2012). \u2022 The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coeffi\u2010 cient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function. \u2022 Most neurons in C3 maps are connected to neurons in only three or four S2 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.220] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 signal is first copied and fed to four different layers. All convolutional layers use the ReLU activation function. Note that the second set of convolutional layers uses differ\u2010 ent kernel sizes (1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5), allowing them to capture patterns at different scales. Also note that every single layer uses a stride of 1 and SAME padding (even the max pooling layer), so their outputs all have the same height and width as their \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.218] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 \u2022 tf.layers.separable_conv2d() creates a separable convolutional layer that first acts like a depthwise convolutional layer, then applies a 1 \u00d7 1 convolutional layer to the resulting feature maps. This makes it possible to apply filters to arbitrary sets of inputs channels. Exercises 1. What are the advantages of a CNN over a fully connected DNN for image classi\u2010 fication? 380  |  Chapter 13: Convolutional Neural Networks \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.217] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 first to stack convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. Table 13-2 presents this architecture. Table 13-2. AlexNet architecture Layer Type Maps Size Kernel size Stride Padding Activation Out Fully Connected \u2013 1,000 \u2013 \u2013 \u2013 Softmax F9 Fully Connected \u2013 4,096 \u2013 \u2013 \u2013 ReLU F8 Fully Connected \u2013 4,096 \u2013 \u2013 \u2013 ReLU C7 Convolution 256 13 \u00d7 13 3 \u00d7 3 1 SAME ReLU C6 Convolution 384 13 \u00d7 13 3 \u00d7 3 1 SAME ReLU C5 Convolution 384 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.213] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 ception is not trivial at all, and to understand it we must look at how the sensory modules work. Convolutional neural networks (CNNs) emerged from the study of the brain\u2019s visual cortex, and they have been used in image recognition since the 1980s. In the last few years, thanks to the increase in computational power, the amount of available training data, and the tricks presented in Chapter 11 for training deep nets, CNNs have man\u2010 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.213] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 13. Convolutional Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357 The Architecture of the Visual Cortex                                                                     358 Convolutional Layer                                                                                                    359 Filters                                                                                                                         361 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.212] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 Figure 13-2. CNN layers with rectangular local receptive fields Until now, all multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. Now each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs. A neuron located in row i, column j of a given layer is connected to the outputs of the \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.212] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 Moreover, input images are also composed of multiple sublayers: one per color chan\u2010 nel. There are typically three: red, green, and blue (RGB). Grayscale images have just one channel, but some images may have much more\u2014for example, satellite images that capture extra light frequencies (such as infrared). Figure 13-6. Convolution layers with multiple feature maps, and images with three channels Specifically, a neuron located in row i, column j of the feature map k in a given convo\u2010 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.211] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 CNN Architectures                                                                                                      369 LeNet-5                                                                                                                      370 AlexNet                                                                                                                      371 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.210] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 any padding, which is why the size keeps shrinking as the image progresses through the network. 370  |  Chapter 13: Convolutional Neural Networks \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.209] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 12 \u201cDeep Residual Learning for Image Recognition,\u201d K. He (2015). \u2022 Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality and speed up the net. \u2022 Next, the average pooling layer uses a kernel the size of the feature maps with VALID padding, outputting 1 \u00d7 1 feature maps: this surprising strategy is called global average pooling. It effectively forces the previous layers to produce feature \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.209] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 You may wonder why inception modules have convolutional layers with 1 \u00d7 1 ker\u2010 nels. Surely these layers cannot capture any features since they look at only one pixel at a time? In fact, these layers serve two purposes: \u2022 First, they are configured to output many fewer feature maps than their inputs, so they serve as bottleneck layers, meaning they reduce dimensionality. This is par\u2010 ticularly useful before the 3 \u00d7 3 and 5 \u00d7 5 convolutions, since these are very com\u2010 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.206] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 10 \u201cGoing Deeper with Convolutions,\u201d C. Szegedy et al. (2015). 11 In the 2010 movie Inception, the characters keep going deeper and deeper into multiple layers of dreams, hence the name of these modules. GoogLeNet The GoogLeNet architecture was developed by Christian Szegedy et al. from Google Research,10 and it won the ILSVRC 2014 challenge by pushing the top-5 error rate below 7%. This great performance came in large part from the fact that the network \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.205] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 ResNets deeper than that, such as ResNet-152, use slightly different residual units. Instead of two 3 \u00d7 3 convolutional layers with (say) 256 feature maps, they use three convolutional layers: first a 1 \u00d7 1 convolutional layer with just 64 feature maps (4 times less), which acts a a bottleneck layer (as discussed already), then a 3 \u00d7 3 layer with 64 feature maps, and finally another 1 \u00d7 1 convolutional layer with 256 feature \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.205] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 13 \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition,\u201d K. Simonyan and A. Zisserman (2015). 14 \u201cInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning,\u201d C. Szegedy et al. (2016). ResNet-34 is the ResNet with 34 layers (only counting the convolutional layers and the fully connected layer) containing three residual units that output 64 feature maps, 4 RUs with 128 maps, 6 RUs with 256 maps, and 3 RUs with 512 maps. \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.203] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 has a total of 2,800 parameters. The second convolutional layer has 3 \u00d7 3 kernels, and its input is the set of 100 feature maps of the previous layer, so each feature map has 3 \u00d7 3 \u00d7 100 = 900 weights, plus a bias term. Since it has 200 feature maps, this layer has 901 \u00d7 200 = 180,200 parameters. Finally, the third and last convolutional layer also has 3 \u00d7 3 kernels, and its input is the set of 200 feature maps of the previous layers, so each feature map has 3 \u00d7 3 \u00d7 200 = 1,800 weights, \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then\n- [Score: 0.200] Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 sented by the dashed arrows). This is likely to completely cancel out the benefit of the parallel computation, since cross-device communication is slow (especially if it is across separate machines). Figure 12-14. Splitting a fully connected neural network However, as we will see in Chapter 13, some neural network architectures, such as convolutional neural networks, contain layers that are only partially connected to the \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then"
    }
  },
  {
    "query": "What is pooling in CNNs?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is pooling in CNNs?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 25,
      "prompt": "QUERY: What is pooling in CNNs?\nRELATED EVIDENCE PATHS:\n- [Score: 0.459] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.292] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 \u2022 Reduce the mini-batch size. \u2022 Reduce dimensionality using a larger stride in one or more layers. \u2022 Remove one or more layers. \u2022 Use 16-bit floats instead of 32-bit floats. \u2022 Distribute the CNN across multiple devices. 4. A max pooling layer has no parameters at all, whereas a convolutional layer has quite a few (see the previous questions). 5. A local response normalization layer makes the neurons that most strongly acti\u2010 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.285] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 to 1. Moreover, it does not support pooling over both the spatial dimensions (height 368  |  Chapter 13: Convolutional Neural Networks \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.270] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 and width) and the depth dimension, so either ksize[1] and ksize[2] must both be equal to 1, or ksize[3] must be equal to 1. To create an average pooling layer, just use the avg_pool() function instead of max_pool(). Now you know all the building blocks to create a convolutional neural network. Let\u2019s see how to assemble them. CNN Architectures Typical CNN architectures stack a few convolutional layers (each one generally fol\u2010 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.269] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 12 \u201cDeep Residual Learning for Image Recognition,\u201d K. He (2015). \u2022 Then comes the tall stack of nine inception modules, interleaved with a couple max pooling layers to reduce dimensionality and speed up the net. \u2022 Next, the average pooling layer uses a kernel the size of the feature maps with VALID padding, outputting 1 \u00d7 1 feature maps: this surprising strategy is called global average pooling. It effectively forces the previous layers to produce feature \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.257] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 stride of 2, the output will be two times smaller in both directions (so its area will be four times smaller), simply dropping 75% of the input values. A pooling layer typically works on every input channel independently, so the output depth is the same as the input depth. You may alternatively pool over the depth dimension, as we will see next, in which case the image\u2019s spatial dimensions (height and width) remain unchanged, but the number of channels is reduced. \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.255] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 9 \u201cImageNet Classification with Deep Convolutional Neural Networks,\u201d A. Krizhevsky et al. (2012). \u2022 The average pooling layers are slightly more complex than usual: each neuron computes the mean of its inputs, then multiplies the result by a learnable coeffi\u2010 cient (one per map) and adds a learnable bias term (again, one per map), then finally applies the activation function. \u2022 Most neurons in C3 maps are connected to neurons in only three or four S2 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.254] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 first to stack convolutional layers directly on top of each other, instead of stacking a pooling layer on top of each convolutional layer. Table 13-2 presents this architecture. Table 13-2. AlexNet architecture Layer Type Maps Size Kernel size Stride Padding Activation Out Fully Connected \u2013 1,000 \u2013 \u2013 \u2013 Softmax F9 Fully Connected \u2013 4,096 \u2013 \u2013 \u2013 ReLU F8 Fully Connected \u2013 4,096 \u2013 \u2013 \u2013 ReLU C7 Convolution 256 13 \u00d7 13 3 \u00d7 3 1 SAME ReLU C6 Convolution 384 13 \u00d7 13 3 \u00d7 3 1 SAME ReLU C5 Convolution 384 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.243] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 lowed by a ReLU layer), then a pooling layer, then another few convolutional layers (+ReLU), then another pooling layer, and so on. The image gets smaller and smaller as it progresses through the network, but it also typically gets deeper and deeper (i.e., with more feature maps) thanks to the convolutional layers (see Figure 13-9). At the top of the stack, a regular feedforward neural network is added, composed of a few \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.234] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 inputs using an aggregation function such as the max or mean. Figure 13-8 shows a max pooling layer, which is the most common type of pooling layer. In this example, we use a 2 \u00d7 2 pooling kernel, a stride of 2, and no padding. Note that only the max input value in each kernel makes it to the next layer. The other inputs are dropped. Figure 13-8. Max pooling layer (2 \u00d7 2 pooling kernel, stride 2, no padding) This is obviously a very destructive kind of layer: even with a tiny 2 \u00d7 2 kernel and a \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.233] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 signal is first copied and fed to four different layers. All convolutional layers use the ReLU activation function. Note that the second set of convolutional layers uses differ\u2010 ent kernel sizes (1 \u00d7 1, 3 \u00d7 3, and 5 \u00d7 5), allowing them to capture patterns at different scales. Also note that every single layer uses a stride of 1 and SAME padding (even the max pooling layer), so their outputs all have the same height and width as their \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.231] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 neuron located in row i, column j in the upper layer is connected to the outputs of the neurons in the previous layer located in rows i \u00d7 sh to i \u00d7 sh + fh \u2013 1, columns j \u00d7 sw to j \u00d7 sw + fw \u2013 1, where sh and sw are the vertical and horizontal strides. Figure 13-4. Reducing dimensionality using a stride of 2 Filters A neuron\u2019s weights can be represented as a small image the size of the receptive field. For example, Figure 13-5 shows two possible sets of weights, called filters (or convolu\u2010 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.225] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 the first convolutional layer are not connected to every single pixel in the input image (like they were in previous chapters), but only to pixels in their receptive fields (see Figure 13-2). In turn, each neuron in the second convolutional layer is connected only to neurons located within a small rectangle in the first layer. This architecture allows the network to concentrate on low-level features in the first hidden layer, then \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.222] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 Figure 13-2. CNN layers with rectangular local receptive fields Until now, all multilayer neural networks we looked at had layers composed of a long line of neurons, and we had to flatten input images to 1D before feeding them to the neural network. Now each layer is represented in 2D, which makes it easier to match neurons with their corresponding inputs. A neuron located in row i, column j of a given layer is connected to the outputs of the \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.219] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 try using 16-bit floats instead of 32-bit floats. Or you could distrib\u2010 ute the CNN across multiple devices. Now let\u2019s look at the second common building block of CNNs: the pooling layer. Pooling Layer Once you understand how convolutional layers work, the pooling layers are quite easy to grasp. Their goal is to subsample (i.e., shrink) the input image in order to reduce the computational load, the memory usage, and the number of parameters (thereby limiting the risk of overfitting). \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.219] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 while the rest gets blurred. Similarly, the upper-right image is what you get if all neu\u2010 rons use the horizontal line filter; notice that the horizontal white lines get enhanced while the rest is blurred out. Thus, a layer full of neurons using the same filter gives  Convolutional Layer  |  361 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.218] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 mini-batch of 50 images? 3. If your GPU runs out of memory while training a CNN, what are five things you could try to solve the problem? 4. Why would you want to add a max pooling layer rather than a convolutional layer with the same stride? 5. When would you want to add a local response normalization layer? 6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main innovations in GoogLeNet and ResNet? \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.218] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 SAME ReLU C5 Convolution 384 13 \u00d7 13 3 \u00d7 3 1 SAME ReLU S4 Max Pooling 256 13 \u00d7 13 3 \u00d7 3 2 VALID \u2013 C3 Convolution 256 27 \u00d7 27 5 \u00d7 5 1 SAME ReLU S2 Max Pooling 96 27 \u00d7 27 3 \u00d7 3 2 VALID \u2013 C1 Convolution 96 55 \u00d7 55 11 \u00d7 11 4 SAME ReLU In Input 3 (RGB) 224 \u00d7 224 \u2013 \u2013 \u2013 \u2013 CNN Architectures  |  371 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.216] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 neurons in the previous layer located in rows i to i + fh \u2013 1, columns j to j + fw \u2013 1, where fh and fw are the height and width of the receptive field (see Figure 13-3). In order for a layer to have the same height and width as the previous layer, it is com\u2010 mon to add zeros around the inputs, as shown in the diagram. This is called zero pad\u2010 ding. Figure 13-3. Connections between layers and zero padding 360  |  Chapter 13: Convolutional Neural Networks \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.213] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 fully connected layers (+ReLUs), and the final layer outputs the prediction (e.g., a softmax layer that outputs estimated class probabilities). Figure 13-9. Typical CNN architecture A common mistake is to use convolution kernels that are too large. You can often get the same effect as one convolutional layer with a 9 \u00d7 9 kernel by stacking two layers with 3 \u00d7 3 kernels, for a lot less compute and parameters. Over the years, variants of this fundamental architecture have been developed, lead\u2010 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.212] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 [mini-batch size, height, width, channels]. The weights of a convolutional layer are represented as a 4D tensor of shape [fh, fw, fn\u2032, fn]. The bias terms of a convo\u2010 lutional layer are simply represented as a 1D tensor of shape [fn]. Let\u2019s look at a simple example. The following code loads two sample images, using Scikit-Learn\u2019s load_sample_images() (which loads two color images, one of a Chi\u2010 nese temple, and the other of a flower). Then it creates two 7 \u00d7 7 filters (one with a \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.209] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 horizontal white line in the middle. Once again, neurons using these weights will ignore everything in their receptive field except for the central horizontal line. Now if all neurons in a layer use the same vertical line filter (and the same bias term), and you feed the network the input image shown in Figure 13-5 (bottom image), the layer will output the top-left image. Notice that the vertical white lines get enhanced \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.208] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 max pooling layer in, 368 max-norm regularization with, 311 model zoo, 294 modularity, 248-250 Momentum optimization in, 298 name scopes, 247 neural network policies, 452 NLP tutorials, 410, 413 node value lifecycle, 237 operations (ops), 237 optimizer, 241 overview, 231-233 parallel distributed computing (see parallel distributed computing with TensorFlow) Python API construction, 267-271 execution, 271 using the neural network, 272 queues (see queues) reusing pretrained layers, 289-291 \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.205] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 Figure 13-11. GoogLeNet architecture Let\u2019s go through this network: \u2022 The first two layers divide the image\u2019s height and width by 4 (so its area is divided by 16), to reduce the computational load. \u2022 Then the local response normalization layer ensures that the previous layers learn a wide variety of features (as discussed earlier). \u2022 Two convolutional layers follow, where the first acts like a bottleneck layer. As explained earlier, you can think of this pair as a single smarter convolutional \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375\n- [Score: 0.204] Just like in convolutional layers, each neuron in a pooling layer is connected to the outputs of a limited number of neurons in the previous layer, located within a small rectangular receptive field. You must define its size, the stride, and the padding type, just like before. However, a pooling neuron has no weights; all it does is aggregate the Pooling Layer  |  367 \u2192 stack, including nine inception modules (the boxes with the spinning tops) that actually contain three layers each. The number of feature maps output by each convo\u2010 lutional layer and each pooling layer is shown before the kernel size. The six numbers in the inception modules represent the number of feature maps output by each con\u2010 volutional layer in the module (in the same order as in Figure 13-10). Note that all the convolutional layers use the ReLU activation function. 374  | \u2192 layer. \u2022 Again, a local response normalization layer ensures that the previous layers cap\u2010 ture a wide variety of patterns. \u2022 Next a max pooling layer reduces the image height and width by 2, again to speed up computations. CNN Architectures  |  375"
    }
  },
  {
    "query": "What is a recurrent neural network (RNN)?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a recurrent neural network (RNN)?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 17,
      "prompt": "QUERY: What is a recurrent neural network (RNN)?\nRELATED EVIDENCE PATHS:\n- [Score: 0.532] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.282] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 You can easily create a layer of recurrent neurons. At each time step t, every neuron receives both the input vector x(t) and the output vector from the previous time step y(t\u20131), as shown in Figure 14-2. Note that both the inputs and outputs are vectors now (when there was just a single neuron, the output was a scalar). Figure 14-2. A layer of recurrent neurons (left), unrolled through time (right) Each recurrent neuron has two sets of weights: one for the inputs x(t) and the other for \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.272] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 selected sequence of 20 consecutive values from the time series, and the target sequence is the same as the input sequence, except it is shifted by one time step into the future (see Figure 14-7). Figure 14-7. Time series (left), and a training instance from that series (right) First, let\u2019s create the RNN. It will contain 100 recurrent neurons and we will unroll it over 20 time steps since each training instance will be 20 inputs long. Each input will \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.263] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 Input and Output Sequences An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs (see Figure 14-4, top-left network). For example, this type of network is use\u2010 ful for predicting time series such as stock prices: you feed it the prices over the last N days, and it must output the prices shifted by one day into the future (i.e., from N \u2013 1 days ago to tomorrow). Alternatively, you could feed the network a sequence of inputs, and ignore all outputs \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.253] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 For the solutions to exercises 7, 8, 9, and 10, please see the Jupyter notebooks avail\u2010 able at https://github.com/ageron/handson-ml. Chapter 14: Recurrent Neural Networks 1. Here are a few RNN applications: \u2022 For a sequence-to-sequence RNN: predicting the weather (or any other time series), machine translation (using an encoder\u2013decoder architecture), video captioning, speech to text, music generation (or other sequence generation), identifying the chords of a song. \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.253] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 end-of-sequence token (EOS token). Any output past the EOS should be ignored (we will discuss this later in this chapter). Okay, now you know how to build an RNN network (or more precisely an RNN net\u2010 work unrolled through time). But how do you train it? Training RNNs To train an RNN, the trick is to unroll it through time (like we just did) and then simply use regular backpropagation (see Figure 14-5). This strategy is called backpro\u2010 pagation through time (BPTT). \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.250] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 from now? In this chapter, we will look at the fundamental concepts underlying RNNs, the main problem they face (namely, vanishing/exploding gradients, discussed in Chapter 11), and the solutions widely used to fight it: LSTM and GRU cells. Along the way, as always, we will show how to implement RNNs using TensorFlow. Finally, we will take a look at the architecture of a machine translation system. 383 \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.247] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 14. Recurrent Neural Networks. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  383 Recurrent Neurons                                                                                                      384 Memory Cells                                                                                                           386 Input and Output Sequences                                                                                  387 \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.229] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 Memory Cells Since the output of a recurrent neuron at time step t is a function of all the inputs from previous time steps, you could say it has a form of memory. A part of a neural network that preserves some state across time steps is called a memory cell (or simply a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell, but later in this chapter we will look at some more complex and powerful types of cells. \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.223] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 RNN (one for each time step). Then we call static_rnn(), giving it the cell factory and the input tensors, and telling it the data type of the inputs (this is used to create the initial state matrix, which by default is full of zeros). The static_rnn() function calls the cell factory\u2019s __call__() function once per input, creating two copies of the cell (each containing a layer of five recurrent neurons), with shared weights and bias \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.218] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 Figure 14-4. Seq to seq (top left), seq to vector (top right), vector to seq (bottom left), delayed seq to seq (bottom right) Sounds promising, so let\u2019s start coding! Basic RNNs in TensorFlow First, let\u2019s implement a very simple RNN model, without using any of TensorFlow\u2019s RNN operations, to better understand what goes on under the hood. We will create an RNN composed of a layer of five recurrent neurons (like the RNN represented in \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.218] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 6 \u201cRecurrent Nets that Time and Count,\u201d F. Gers and J. Schmidhuber (2000). 7 \u201cLearning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation,\u201d K. Cho et al. (2014). Peephole Connections In a basic LSTM cell, the gate controllers can look only at the input x(t) and the previ\u2010 ous short-term state h(t\u20131). It may be a good idea to give them a bit more context by letting them peek at the long-term state as well. This idea was proposed by Felix Gers \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.214] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 producing an output, and sending that output back to itself, as shown in Figure 14-1 (left). At each time step t (also called a frame), this recurrent neuron receives the inputs x(t) as well as its own output from the previous time step, y(t\u20131). We can represent this tiny network against the time axis, as shown in Figure 14-1 (right). This is called unrolling the network through time. Figure 14-1. A recurrent neuron (left), unrolled through time (right) \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.213] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 movie even better. If the RNN gradually forgets the first four words, it will completely misinterpret the review. To solve this problem, various types of cells with long-term 404  |  Chapter 14: Recurrent Neural Networks \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.212] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.211] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True) There are many other variants of the LSTM cell. One particularly popular variant is the GRU cell, which we will look at now. GRU Cell The Gated Recurrent Unit (GRU) cell (see Figure 14-14) was proposed by Kyunghyun Cho et al. in a 2014 paper7 that also introduced the Encoder\u2013Decoder network we mentioned earlier. 408  |  Chapter 14: Recurrent Neural Networks \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict\n- [Score: 0.209] Recurrent Neurons Up to now we have mostly looked at feedforward neural networks, where the activa\u2010 tions flow only in one direction, from the input layer to the output layer (except for a few networks in Appendix E). A recurrent neural network looks very much like a feedforward neural network, except it also has connections pointing backward. Let\u2019s look at the simplest possible RNN, composed of just one neuron receiving inputs, \u2192 pagation through time (BPTT). Figure 14-5. Backpropagation through time Just like in regular backpropagation, there is a first forward pass through the unrolled network (represented by the dashed arrows); then the output sequence is evaluated Training RNNs  |  393 \u2192 CHAPTER 14 Recurrent Neural Networks The batter hits the ball. You immediately start running, anticipating the ball\u2019s trajec\u2010 tory. You track it and adapt your movements, and finally catch it (under a thunder of applause). Predicting the future is what you do all the time, whether you are finishing a friend\u2019s sentence or anticipating the smell of coffee at breakfast. In this chapter, we are going to discuss recurrent neural networks (RNN), a class of nets that can predict"
    }
  },
  {
    "query": "What is an LSTM?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is an LSTM?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 10,
      "prompt": "QUERY: What is an LSTM?\nRELATED EVIDENCE PATHS:\n- [Score: 0.561] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.345] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 to the long-term state (this is why we said it was only \u201cpartially stored\u201d). \u2014 Finally, the output gate (controlled by o(t)) controls which parts of the long- term state should be read and output at this time step (both to h(t)) and y(t). In short, an LSTM cell can learn to recognize an important input (that\u2019s the role of the input gate), store it in the long-term state, learn to preserve it for as long as it is \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.334] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 3 \u201cLong Short-Term Memory,\u201d S. Hochreiter and J. Schmidhuber (1997). 4 \u201cLong Short-Term Memory Recurrent Neural Network Architectures for Large Scale Acoustic Modeling,\u201d H. Sak et al. (2014). 5 \u201cRecurrent Neural Network Regularization,\u201d W. Zaremba et al. (2015). memory have been introduced. They have proved so successful that the basic cells are not much used anymore. Let\u2019s first look at the most popular of these long memory cells: the LSTM cell. LSTM Cell \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.278] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 \u2022 The main layer is the one that outputs g(t). It has the usual role of analyzing the current inputs x(t) and the previous (short-term) state h(t\u20131). In a basic cell, there is nothing else than this layer, and its output goes straight out to y(t) and h(t). In con\u2010 trast, in an LSTM cell this layer\u2019s output does not go straight out, but instead it is partially stored in the long-term state. 406  |  Chapter 14: Recurrent Neural Networks \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.271] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 lstm_cell = tf.contrib.rnn.LSTMCell(num_units=n_neurons, use_peepholes=True) There are many other variants of the LSTM cell. One particularly popular variant is the GRU cell, which we will look at now. GRU Cell The Gated Recurrent Unit (GRU) cell (see Figure 14-14) was proposed by Kyunghyun Cho et al. in a 2014 paper7 that also introduced the Encoder\u2013Decoder network we mentioned earlier. 408  |  Chapter 14: Recurrent Neural Networks \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.254] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 8 A 2015 paper by Klaus Greff et al., \u201cLSTM: A Search Space Odyssey,\u201d seems to show that all LSTM variants perform roughly the same. Figure 14-14. GRU cell The GRU cell is a simplified version of the LSTM cell, and it seems to perform just as well8 (which explains its growing popularity). The main simplifications are: \u2022 Both state vectors are merged into a single vector h(t). \u2022 A single gate controller controls both the forget gate and the input gate. If the \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.245] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 Memory Cells Since the output of a recurrent neuron at time step t is a function of all the inputs from previous time steps, you could say it has a form of memory. A part of a neural network that preserves some state across time steps is called a memory cell (or simply a cell). A single recurrent neuron, or a layer of recurrent neurons, is a very basic cell, but later in this chapter we will look at some more complex and powerful types of cells. \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.242] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 long-term dependencies in the data. In TensorFlow, you can simply use a BasicLSTM Cell instead of a BasicRNNCell: lstm_cell = tf.contrib.rnn.BasicLSTMCell(num_units=n_neurons) LSTM cells manage two state vectors, and for performance reasons they are kept separate by default. You can change this default behavior by setting state_is_tuple=False when creating the BasicLSTMCell. So how does an LSTM cell work? The architecture of a basic LSTM cell is shown in Figure 14-13. LSTM Cell  |  405 \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.207] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 6 \u201cRecurrent Nets that Time and Count,\u201d F. Gers and J. Schmidhuber (2000). 7 \u201cLearning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation,\u201d K. Cho et al. (2014). Peephole Connections In a basic LSTM cell, the gate controllers can look only at the input x(t) and the previ\u2010 ous short-term state h(t\u20131). It may be a good idea to give them a bit more context by letting them peek at the long-term state as well. This idea was proposed by Felix Gers \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state\n- [Score: 0.204] LSTM Cell The Long Short-Term Memory (LSTM) cell was proposed in 19973 by Sepp Hochreiter and J\u00fcrgen Schmidhuber, and it was gradually improved over the years by several researchers, such as Alex Graves, Ha\u015fim Sak,4 Wojciech Zaremba,5 and many more. If you consider the LSTM cell as a black box, it can be used very much like a basic cell, except it will perform much better; training will converge faster and it will detect \u2192 needed (that\u2019s the role of the forget gate), and learn to extract it whenever it is needed. This explains why they have been amazingly successful at capturing long-term pat\u2010 terns in time series, long texts, audio recordings, and more. Equation 14-3 summarizes how to compute the cell\u2019s long-term state, its short-term state, and its output at each time step for a single instance (the equations for a whole mini-batch are very similar). Equation 14-3. LSTM computations i t = \u03c3 Wxi T \u00b7 x t + Whi \u2192 Figure 14-13. LSTM cell If you don\u2019t look at what\u2019s inside the box, the LSTM cell looks exactly like a regular cell, except that its state is split in two vectors: h(t) and c(t) (\u201cc\u201d stands for \u201ccell\u201d). You can think of h(t) as the short-term state and c(t) as the long-term state. Now let\u2019s open the box! The key idea is that the network can learn what to store in the long-term state, what to throw away, and what to read from it. As the long-term state"
    }
  },
  {
    "query": "Explain the vanishing gradient problem.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain the vanishing gradient problem.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: Explain the vanishing gradient problem.\nRELATED EVIDENCE PATHS:\n- [Score: 0.313] 15 You can think of a subgradient vector at a nondifferentiable point as an intermediate vector between the gra\u2010 dient vectors around that point. reaches \u03b81 = 0, then rolls down a gutter until it reaches \u03b82 = 0. On the top-right plot, the contours represent the same cost function plus an \u21131 penalty with \u03b1 = 0.5. The global minimum is on the \u03b82 = 0 axis. BGD first reaches \u03b82 = 0, then rolls down the gutter until it reaches the global minimum. The two bottom plots show the same \u2192 \u2022 It can take on negative values, so the average output of the neurons in any given layer is typically closer to 0 than when using the ReLU activation func\u2010 tion (which never outputs negative values). This helps alleviate the vanishing gradients problem. \u2022 It always has a nonzero derivative, which avoids the dying units issue that can affect ReLU units. \u2022 It is smooth everywhere, whereas the ReLU\u2019s slope abruptly jumps from 0 to 1"
    }
  },
  {
    "query": "What is an embedding in NLP?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is an embedding in NLP?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is an embedding in NLP?\nRELATED EVIDENCE PATHS:\n- [Score: 0.380] Embeddings are also useful for representing categorical attributes that can take on a large number of different values, especially when there are complex similarities between values. For example, con\u2010 sider professions, hobbies, dishes, species, brands, and so on. Natural Language Processing  |  411 \u2192 of the English sentence will be fed last to the encoder, which is useful because that\u2019s generally the first thing that the decoder needs to translate. Each word is initially represented by a simple integer identifier (e.g., 288 for the word \u201cmilk\u201d). Next, an embedding lookup returns the word embedding (as explained ear\u2010 lier, this is a dense, fairly low-dimensional vector). These word embeddings are what is actually fed to the encoder and the decoder."
    }
  },
  {
    "query": "Describe word2vec.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe word2vec.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: Describe word2vec.\nRELATED EVIDENCE PATHS:\n- [Score: 0.460] Word2Vec and Seq2Seq tutorials, so you should definitely check them out. Word Embeddings Before we start, we need to choose a word representation. One option could be to represent each word using a one-hot vector. Suppose your vocabulary contains 50,000 words, then the nth word would be represented as a 50,000-dimensional vector, full of 0s except for a 1 at the nth position. However, with such a large vocabulary, this \u2192 dimensional vectors, then use a sampling technique to estimate the loss without having to compute it over every single word in the target vocabulary. This Sam\u2010 pled Softmax technique was introduced in 2015 by S\u00e9bastien Jean et al.12 In Ten\u2010 sorFlow you can use the sampled_softmax_loss() function. \u2022 Third, the tutorial\u2019s implementation uses an attention mechanism that lets the decoder peek into the input sequence. Attention augmented RNNs are beyond\n- [Score: 0.206] Word2Vec and Seq2Seq tutorials, so you should definitely check them out. Word Embeddings Before we start, we need to choose a word representation. One option could be to represent each word using a one-hot vector. Suppose your vocabulary contains 50,000 words, then the nth word would be represented as a 50,000-dimensional vector, full of 0s except for a 1 at the nth position. However, with such a large vocabulary, this \u2192 drink shoes\u201d is probably not. But how can you come up with such a meaningful rep\u2010 resentation? The most common solution is to represent each word in the vocabulary using a fairly small and dense vector (e.g., 150 dimensions), called an embedding, and just let the neural network learn a good embedding for each word during training. At the begin\u2010 ning of training, embeddings are simply chosen randomly, but during training, back\u2010 410  |  Chapter 14: Recurrent Neural Networks \u2192 dimensional vectors, then use a sampling technique to estimate the loss without having to compute it over every single word in the target vocabulary. This Sam\u2010 pled Softmax technique was introduced in 2015 by S\u00e9bastien Jean et al.12 In Ten\u2010 sorFlow you can use the sampled_softmax_loss() function. \u2022 Third, the tutorial\u2019s implementation uses an attention mechanism that lets the decoder peek into the input sequence. Attention augmented RNNs are beyond"
    }
  },
  {
    "query": "What is transfer learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is transfer learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is transfer learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.257] transfer learning, 289-297 (see also pretrained layers reuse) transform(), 62, 68 transformation pipelines, 67-69 transformers, 62 transformers, custom, 65-66 transpose(), 390 true negative rate (TNR), 93 true positive rate (TPR), 87, 93 truncated backpropagation through time, 404 tuples, 336 tying weights, 423 U underfitting, 28, 70, 155 univariate regression, 37 unstack(), 390 unsupervised learning, 10-12 anomaly detection, 12 association rule learning, 10, 12 clustering, 10 \u2192 Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is"
    }
  },
  {
    "query": "What is fine-tuning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is fine-tuning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: What is fine-tuning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.810] Fine-Tune Your Model  |  77 \u2192 Fine-Tune Your Model  |  73\n- [Score: 0.300] Fine-Tune Your Model  |  77 \u2192 Select and Train a Model                                                                                               69 Training and Evaluating on the Training Set                                                         69 Better Evaluation Using Cross-Validation                                                              71 Fine-Tune Your Model                                                                                                  73 \u2192 Fine-Tune Your Model  |  73\n- [Score: 0.247] Fine-Tune Your Model  |  77 \u2192 Select and Train a Model  |  69 \u2192 Fine-Tune Your Model  |  73"
    }
  },
  {
    "query": "Explain attention mechanisms.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain attention mechanisms.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: Explain attention mechanisms.\nRELATED EVIDENCE PATHS:\n- [Score: 0.236] actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari\u2019s Pong, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play \u2192 1 \u201cSingle Unit Activity in Striate Cortex of Unrestrained Cats,\u201d D. Hubel and T. Wiesel (1958). 2 \u201cReceptive Fields of Single Neurones in the Cat\u2019s Striate Cortex,\u201d D. Hubel and T. Wiesel (1959). 3 \u201cReceptive Fields and Functional Architecture of Monkey Striate Cortex,\u201d D. Hubel and T. Wiesel (1968). 4 \u201cNeocognitron: A Self-organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position,\u201d K. Fukushima (1980)."
    }
  },
  {
    "query": "What is the Transformer architecture?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the Transformer architecture?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is the Transformer architecture?\nRELATED EVIDENCE PATHS:\n- [Score: 0.358] \u2014 Transformers. Some estimators (such as an imputer) can also transform a dataset; these are called transformers. Once again, the API is quite simple: the transformation is performed by the transform() method with the dataset to transform as a parameter. It returns the transformed dataset. This transforma\u2010 tion generally relies on the learned parameters, as is the case for an imputer. All transformers also have a convenience method called fit_transform() \u2192 transfer learning, 289-297 (see also pretrained layers reuse) transform(), 62, 68 transformation pipelines, 67-69 transformers, 62 transformers, custom, 65-66 transpose(), 390 true negative rate (TNR), 93 true positive rate (TPR), 87, 93 truncated backpropagation through time, 404 tuples, 336 tying weights, 423 U underfitting, 28, 70, 155 univariate regression, 37 unstack(), 390 unsupervised learning, 10-12 anomaly detection, 12 association rule learning, 10, 12 clustering, 10"
    }
  },
  {
    "query": "Describe BERT model.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe BERT model.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 17,
      "prompt": "QUERY: Describe BERT model.\nRELATED EVIDENCE PATHS:\n- [Score: 0.482] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.334] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 142  |  Chapter 4: Training Models \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.312] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 390  |  Chapter 14: Recurrent Neural Networks \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.287] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 288  |  Chapter 11: Training Deep Neural Nets \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.272] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 304  |  Chapter 11: Training Deep Neural Nets \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.256] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 44  |  Chapter 2: End-to-End Machine Learning Project \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.242] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 56  |  Chapter 2: End-to-End Machine Learning Project \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.234] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 76  |  Chapter 2: End-to-End Machine Learning Project \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.232] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 60  |  Chapter 2: End-to-End Machine Learning Project \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.220] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 24  |  Chapter 1: The Machine Learning Landscape \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.214] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 the output layer. 274  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.214] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 tion you may need millions of examples (unless you can reuse parts of an existing model). 22  |  Chapter 1: The Machine Learning Landscape \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.211] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 array([ 1.54333232]) 134  |  Chapter 4: Training Models \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.211] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 272  |  Chapter 10: Introduction to Artificial Neural Networks \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.211] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 374  |  Chapter 13: Convolutional Neural Networks \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.204] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 PART I The Fundamentals of Machine Learning \u2192 data. 128  |  Chapter 4: Training Models\n- [Score: 0.204] is a placeholder for the target classes. 394  |  Chapter 14: Recurrent Neural Networks \u2192 PART II Neural Networks and Deep Learning \u2192 data. 128  |  Chapter 4: Training Models"
    }
  },
  {
    "query": "What is GPT?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is GPT?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is GPT?\nRELATED EVIDENCE PATHS:\n- [Score: 0.235] 15 These are estimations of the mean and (uncentered) variance of the gradients. The mean is often called the first moment, while the variance is often called the second moment, hence the name of the algorithm. RMSProp Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm13 fixes this by accumulating only the gradi\u2010 ents from the most recent iterations (as opposed to all the gradients since the begin\u2010 \u2192 training, 260-261 performance measures, 37-40 confusion matrix, 86-88 cross-validation, 85-86 precision and recall, 88-92 ROC (receiver operating characteristic) curve, 93-95 performance scheduling, 305 permutation(), 50 PG algorithms, 454 photo-hosting services, 13 pinning operations, 330 pip, 41 Pipeline constructor, 67-69 pipelines, 36 placeholder nodes, 241 placers (see simple placer; dynamic placer) policy, 446 policy gradients, 447 (see PG algorithms) policy space, 446"
    }
  },
  {
    "query": "What is the difference between classification and regression?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the difference between classification and regression?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: What is the difference between classification and regression?\nRELATED EVIDENCE PATHS:\n- [Score: 0.411] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 classifier\u2019s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput classification system. The line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have\n- [Score: 0.212] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 classifier\u2019s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput classification system. The line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have\n- [Score: 0.211] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 Finally, we will look at two more models that are commonly used for classification tasks: Logistic Regression and Softmax Regression. There will be quite a few math equations in this chapter, using basic notions of linear algebra and calculus. To understand these equa\u2010 tions, you will need to know what vectors and matrices are, how to transpose them, what the dot product is, what matrix inverse is, and what partial derivatives are. If you are unfamiliar with these \u2192 classifier\u2019s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput classification system. The line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have\n- [Score: 0.202] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 multiclass classification (or simply multioutput classification). It is simply a generaliza\u2010 tion of multilabel classification where each label can be multiclass (i.e., it can have more than two possible values). To illustrate this, let\u2019s build a system that removes noise from images. It will take as input a noisy digit image, and it will (hopefully) output a clean digit image, repre\u2010 sented as an array of pixel intensities, just like the MNIST images. Notice that the \u2192 classifier\u2019s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). It is thus an example of a multioutput classification system. The line between classification and regression is sometimes blurry, such as in this example. Arguably, predicting pixel intensity is more akin to regression than to classification. Moreover, multioutput systems are not limited to classification tasks; you could even have"
    }
  },
  {
    "query": "What is the confusion matrix?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the confusion matrix?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is the confusion matrix?\nRELATED EVIDENCE PATHS:\n- [Score: 0.549] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.377] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 Now you are ready to get the confusion matrix using the confusion_matrix() func\u2010 tion. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred): >>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred) array([[53272,  1307],        [ 1077,  4344]]) Each row in a confusion matrix represents an actual class, while each column repre\u2010 sents a predicted class. The first row of this matrix considers non-5 images (the nega\u2010 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.362] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including preci\u2010 sion and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1307) 0.76871350203503808 >>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.80132816823464303 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.243] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 you want to find ways to improve it. One way to do this is to analyze the types of errors it makes. First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier: >>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) >>> conf_mx = confusion_matrix(y_train, y_train_pred) >>> conf_mx array([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4], \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.225] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others). Confusion Matrix A much better way to evaluate the performance of a classifier is to look at the confu\u2010 sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict"
    }
  },
  {
    "query": "Define precision, recall, and F1 score.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Define precision, recall, and F1 score.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Define precision, recall, and F1 score.\nRELATED EVIDENCE PATHS:\n- [Score: 0.494] To compute the F1 score, simply call the f1_score() function: >>> from sklearn.metrics import f1_score >>> f1_score(y_train_5, y_train_pred) 0.78468208092485547 The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other con\u2010 texts you really care about recall. For example, if you trained a classifier to detect vid\u2010 \u2192 treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high. Equation 3-3. F1 score F1 = 2 1 precision + 1 recall = 2 \u00d7 precision \u00d7 recall precision + recall = TP TP + FN + FP 2 88  |  Chapter 3: Classification\n- [Score: 0.219] To compute the F1 score, simply call the f1_score() function: >>> from sklearn.metrics import f1_score >>> f1_score(y_train_5, y_train_pred) 0.78468208092485547 The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other con\u2010 texts you really care about recall. For example, if you trained a classifier to detect vid\u2010 \u2192 If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including preci\u2010 sion and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1307) 0.76871350203503808 >>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.80132816823464303 \u2192 treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high. Equation 3-3. F1 score F1 = 2 1 precision + 1 recall = 2 \u00d7 precision \u00d7 recall precision + recall = TP TP + FN + FP 2 88  |  Chapter 3: Classification\n- [Score: 0.214] To compute the F1 score, simply call the f1_score() function: >>> from sklearn.metrics import f1_score >>> f1_score(y_train_5, y_train_pred) 0.78468208092485547 The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other con\u2010 texts you really care about recall. For example, if you trained a classifier to detect vid\u2010 \u2192 Equation 3-1. Precision precision = TP TP + FP TP is the number of true positives, and FP is the number of false positives. A trivial way to have perfect precision is to make one single positive prediction and ensure it is correct (precision = 1/1 = 100%). This would not be very useful since the classifier would ignore all but one positive instance. So precision is typically used along with another metric named recall, also called sensitivity or true positive rate \u2192 treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F1 score if both recall and precision are high. Equation 3-3. F1 score F1 = 2 1 precision + 1 recall = 2 \u00d7 precision \u00d7 recall precision + recall = TP TP + FN + FP 2 88  |  Chapter 3: Classification"
    }
  },
  {
    "query": "What is ROC curve?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is ROC curve?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: What is ROC curve?\nRELATED EVIDENCE PATHS:\n- [Score: 0.462] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93\n- [Score: 0.265] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 which is the ratio of negative instances that are correctly classified as negative. The TNR is also called specificity. Hence the ROC curve plots sensitivity (recall) versus 1 \u2013 specificity. To plot the ROC curve, you first need to compute the TPR and FPR for various thres\u2010 hold values, using the roc_curve() function: from sklearn.metrics import roc_curve fpr, tpr, thresholds = roc_curve(y_train_5, y_scores) Then you can plot the FPR against the TPR using Matplotlib. This code produces the \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93\n- [Score: 0.264] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC: >>> from sklearn.metrics import roc_auc_score >>> roc_auc_score(y_train_5, y_scores) 0.96244965559671547 Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93\n- [Score: 0.242] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 Once again there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). One way to compare classifiers is to measure the area under the curve (AUC). A per\u2010 fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93\n- [Score: 0.237] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 the false negatives, and the ROC curve otherwise. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement (the curve could be closer to the top- right corner). Let\u2019s train a RandomForestClassifier and compare its ROC curve and ROC AUC \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93\n- [Score: 0.227] The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate, \u2192 y_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest) 94  |  Chapter 3: Classification \u2192 plot in Figure 3-6: def plot_roc_curve(fpr, tpr, label=None):     plt.plot(fpr, tpr, linewidth=2, label=label)     plt.plot([0, 1], [0, 1], 'k--')     plt.axis([0, 1, 0, 1])     plt.xlabel('False Positive Rate')     plt.ylabel('True Positive Rate') plot_roc_curve(fpr, tpr) plt.show() Figure 3-6. ROC curve Performance Measures  |  93"
    }
  },
  {
    "query": "Explain AUC.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain AUC.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: Explain AUC.\nRELATED EVIDENCE PATHS:\n- [Score: 0.427] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,\n- [Score: 0.315] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 have a ROC AUC equal to 0.5. Scikit-Learn provides a function to compute the ROC AUC: >>> from sklearn.metrics import roc_auc_score >>> roc_auc_score(y_train_5, y_scores) 0.96244965559671547 Since the ROC curve is so similar to the precision/recall (or PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,\n- [Score: 0.272] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 Once again there is a tradeoff: the higher the recall (TPR), the more false positives (FPR) the classifier produces. The dotted line represents the ROC curve of a purely random classifier; a good classifier stays as far away from that line as possible (toward the top-left corner). One way to compare classifiers is to measure the area under the curve (AUC). A per\u2010 fect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,\n- [Score: 0.232] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 Precision/Recall Tradeoff                                                                                          89 The ROC Curve                                                                                                          93 Multiclass Classification                                                                                               95 Error Analysis                                                                                                                 98 \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,\n- [Score: 0.231] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 y_scores_forest = y_probas_forest[:, 1]   # score = proba of positive class fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5,y_scores_forest) 94  |  Chapter 3: Classification \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,\n- [Score: 0.226] result, its ROC AUC score is also significantly better: >>> roc_auc_score(y_train_5, y_scores_forest) 0.99312433660038291 Try measuring the precision and recall scores: you should find 98.5% precision and 82.8% recall. Not too bad! Hopefully you now know how to train binary classifiers, choose the appropriate met\u2010 ric for your task, evaluate your classifiers using cross-validation, select the precision/ recall tradeoff that fits your needs, and compare various models using ROC curves \u2192 the false negatives, and the ROC curve otherwise. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement (the curve could be closer to the top- right corner). Let\u2019s train a RandomForestClassifier and compare its ROC curve and ROC AUC \u2192 The ROC Curve The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. It is very similar to the precision/recall curve, but instead of plot\u2010 ting precision versus recall, the ROC curve plots the true positive rate (another name for recall) against the false positive rate. The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate,"
    }
  },
  {
    "query": "What is cross-validation?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is cross-validation?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: What is cross-validation?\nRELATED EVIDENCE PATHS:\n- [Score: 0.491] 19. Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali\u2010 dation set. This saves precious training data. Chapter 2: End-to-End Machine Learning Project See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 3: Classification See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 4: Training Models \u2192 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape\n- [Score: 0.232] 19. Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali\u2010 dation set. This saves precious training data. Chapter 2: End-to-End Machine Learning Project See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 3: Classification See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 4: Training Models \u2192 1 If you draw a straight line between any two points on the curve, the line never crosses the curve. 17. A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. 18. If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). \u2192 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape\n- [Score: 0.208] 19. Cross-validation is a technique that makes it possible to compare models (for model selection and hyperparameter tuning) without the need for a separate vali\u2010 dation set. This saves precious training data. Chapter 2: End-to-End Machine Learning Project See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 3: Classification See the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 4: Training Models \u2192 set, you select the model and hyperparameters that perform best on the validation set, and when you\u2019re happy with your model you run a single final test against the test set to get an estimate of the generalization error. To avoid \u201cwasting\u201d too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated \u2192 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape"
    }
  },
  {
    "query": "What is the curse of dimensionality?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the curse of dimensionality?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 9,
      "prompt": "QUERY: What is the curse of dimensionality?\nRELATED EVIDENCE PATHS:\n- [Score: 0.520] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.293] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 In short, the more dimensions the training set has, the greater the risk of overfitting it. In theory, one solution to the curse of dimensionality could be to increase the size of the training set to reach a sufficient density of training instances. Unfortunately, in practice, the number of training instances required to reach a given density grows exponentially with the number of dimensions. With just 100 features (much less than \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.266] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 (or three) makes it possible to plot a high-dimensional training set on a graph and often gain some important insights by visually detecting patterns, such as clusters. In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques: PCA, Kernel \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.246] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 good technique to reduce dimensionality before running another classification algorithm such as an SVM classifier. Figure 8-13. Reducing the Swiss roll to 2D using various techniques Exercises 1. What are the main motivations for reducing a dataset\u2019s dimensionality? What are the main drawbacks? 2. What is the curse of dimensionality? 3. Once a dataset\u2019s dimensionality has been reduced, is it possible to reverse the operation? If so, how? If not, why? \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.225] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 CHAPTER 8 Dimensionality Reduction Many Machine Learning problems involve thousands or even millions of features for each training instance. Not only does this make training extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often referred to as the curse of dimensionality. Fortunately, in real-world problems, it is often possible to reduce the number of fea\u2010 \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.222] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 Figure 8-2. A 3D dataset lying close to a 2D subspace Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of the high-dimensional (3D) space. Now if we project every training instance perpendicularly onto this subspace (as represented by the short lines con\u2010 necting the instances to the plane), we get the new 2D dataset shown in Figure 8-3. Ta-da! We have just reduced the dataset\u2019s dimensionality from 3D to 2D. Note that \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.222] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 simpler solution; it all depends on the dataset. Hopefully you now have a good sense of what the curse of dimensionality is and how dimensionality reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this chapter will go through some of the most popular algorithms. 212  |  Chapter 8: Dimensionality Reduction \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.219] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 1 Well, four dimensions if you count time, and a few more if you are a string theorist. 2 Watch a rotating tesseract projected into 3D space at http://goo.gl/OM7ktJ. Image by Wikipedia user Nerd\u2010 Boy1392 (Creative Commons BY-SA 3.0). Reproduced from https://en.wikipedia.org/wiki/Tesseract. 3 Fun fact: anyone you know is probably an extremist in at least one dimension (e.g., how much sugar they put in their coffee), if you consider enough dimensions. \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For\n- [Score: 0.208] 2. The curse of dimensionality refers to the fact that many problems that do not exist in low-dimensional space arise in high-dimensional space. In Machine Learning, one common manifestation is the fact that randomly sampled high- dimensional vectors are generally very sparse, increasing the risk of overfitting and making it very difficult to identify patterns in the data without having plenty of training data. 3. Once a dataset\u2019s dimensionality has been reduced using one of the algorithms we \u2192 Learning. Projection In most real-world problems, training instances are not spread out uniformly across all dimensions. Many features are almost constant, while others are highly correlated (as discussed earlier for MNIST). As a result, all training instances actually lie within (or close to) a much lower-dimensional subspace of the high-dimensional space. This sounds very abstract, so let\u2019s look at an example. In Figure 8-2 you can see a 3D data\u2010 set represented by the circles. \u2192 PCA, and LLE. The Curse of Dimensionality We are so used to living in three dimensions1 that our intuition fails us when we try to imagine a high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see Figure 8-1), let alone a 200-dimensional ellipsoid bent in a 1,000-dimensional space. Figure 8-1. Point, segment, square, cube, and tesseract (0D to 4D hypercubes)2 It turns out that many things behave very differently in high-dimensional space. For"
    }
  },
  {
    "query": "What is ensemble learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is ensemble learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 23,
      "prompt": "QUERY: What is ensemble learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.504] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.313] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  183 Voting Classifiers                                                                                                         183 Bagging and Pasting                                                                                                    187 Bagging and Pasting in Scikit-Learn                                                                     188 \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.299] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 best classifier in the ensemble. In fact, even if each classifier is a weak learner (mean\u2010 ing it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse. 184  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.282] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 202  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.281] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 204  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.280] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 and despite its simplicity, this is one of the most powerful Machine Learning algo\u2010 rithms available today. Moreover, as we discussed in Chapter 2, you will often use Ensemble methods near the end of a project, once you have already built a few good predictors, to combine them into an even better predictor. In fact, the winning solutions in Machine Learn\u2010 ing competitions often involve several Ensemble methods (most famously in the Net\u2010 flix Prize competition). \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.278] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 best. The group (or \u201censemble\u201d) will often perform better than the best individual model (just like Random Forests perform better than the individual Decision Trees they rely on), especially if the individual models make very different types of errors. We will cover this topic in more detail in Chapter 7. Analyze the Best Models and Their Errors You will often gain good insights on the problem by inspecting the best models. For \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.275] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 average estimated class probability for each class and picks the class with the highest probability. This gives high-confidence votes more weight and often per\u2010 forms better, but it works only if every classifier is able to estimate class probabil\u2010 ities  (e.g.,  for  the  SVM  classifiers  in  Scikit-Learn  you  must  set probability=True). 3. It is quite possible to speed up training of a bagging ensemble by distributing it \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.274] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 194  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.273] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 196  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.271] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 192  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.270] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 Let\u2019s try one last model now: the RandomForestRegressor. As we will see in Chap\u2010 ter 7, Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algo\u2010 rithms even further. We will skip most of the code since it is essentially the same as for the other models: >>> from sklearn.ensemble import RandomForestRegressor \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.253] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 the ensemble\u2019s accuracy. Voting Classifiers  |  185 \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.238] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 3. Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles? 4. What is the benefit of out-of-bag evaluation? 5. What makes Extra-Trees more random than regular Random Forests? How can this extra randomness help? Are Extra-Trees slower or faster than regular Ran\u2010 dom Forests? 6. If your AdaBoost ensemble underfits the training data, what hyperparameters \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.235] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 Stacking The last Ensemble method we will discuss in this chapter is called stacking (short for stacked generalization).18 It is based on a simple idea: instead of using trivial functions (such as hard voting) to aggregate the predictions of all predictors in an ensemble, why don\u2019t we train a model to perform this aggregation? Figure 7-12 shows such an ensemble performing a regression task on a new instance. Each of the bottom three \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.232] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 Figure 7-1. Training diverse classifiers A very simple way to create an even better classifier is to aggregate the predictions of each classifier and predict the class that gets the most votes. This majority-vote classi\u2010 fier is called a hard voting classifier (see Figure 7-2). Figure 7-2. Hard voting classifier predictions Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.231] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 trained on the moons dataset. As you can see, the ensemble\u2019s predictions will likely generalize much better than the single Decision Tree\u2019s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular). 188  |  Chapter 7: Ensemble Learning and Random Forests \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.227] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 dictors is called an ensemble; thus, this technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method. For example, you can train a group of Decision Tree classifiers, each on a different random subset of the training set. To make predictions, you just obtain the predic\u2010 tions of all individual trees, then predict the class that gets the most votes (see the last exercise in Chapter 6). Such an ensemble of Decision Trees is called a Random Forest, \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.224] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 which is clearly not the case since they are trained on the same data. They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble\u2019s accuracy. Ensemble methods work best when the predictors are as independ\u2010 ent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.220] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara\u2010 meters of a BaggingClassifier to control the ensemble itself.11 The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.218] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.217] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 fier, a Decision Tree classifier, a Logistic Regression classifier, and so on). It is even better if they are trained on different training instances (that\u2019s the whole point of bagging and pasting ensembles), but if not it will still work as long as the models are very different. 2. A hard voting classifier just counts the votes of each classifier in the ensemble and picks the class that gets the most votes. A soft voting classifier computes the \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183\n- [Score: 0.205] CHAPTER 7 Ensemble Learning and Random Forests Suppose you ask a complex question to thousands of random people, then aggregate their answers. In many cases you will find that this aggregated answer is better than an expert\u2019s answer. This is called the wisdom of the crowd. Similarly, if you aggregate the predictions of a group of predictors (such as classifiers or regressors), you will often get better predictions than with the best individual predictor. A group of pre\u2010 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183"
    }
  },
  {
    "query": "Describe bagging.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe bagging.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Describe bagging.\nRELATED EVIDENCE PATHS:\n- [Score: 0.400] Bagging and Pasting  |  189 \u2192 predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be\n- [Score: 0.217] Bagging and Pasting  |  189 \u2192 erally preferred. However, if you have spare time and CPU power you can use cross- validation to evaluate both bagging and pasting and select the one that works best. Out-of-Bag Evaluation With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the \u2192 predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be\n- [Score: 0.207] Bagging and Pasting  |  189 \u2192 This is one of the reasons why bagging and pasting are such popular methods: they scale very well. Bagging and Pasting in Scikit-Learn Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas sifier class (or BaggingRegressor for regression). The following code trains an ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran\u2010 domly sampled from the training set with replacement (this is an example of bagging, \u2192 predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be"
    }
  },
  {
    "query": "Describe boosting.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe boosting.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 8,
      "prompt": "QUERY: Describe boosting.\nRELATED EVIDENCE PATHS:\n- [Score: 0.566] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.296] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 13 \u201cA Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,\u201d Yoav Freund, Robert E. Schapire (1997). 14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they are slow and tend to be unstable with AdaBoost. AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let\u2019s start with Ada\u2010 Boost. AdaBoost One way for a new predictor to correct its predecessor is to pay a bit more attention \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.251] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 boost comes from using a faster optimizer than the regular Gradient Descent opti\u2010 mizer. In this section we will present the most popular ones: Momentum optimiza\u2010 tion, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam optimization. Momentum Optimization Imagine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.241] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 to the training instances that the predecessor underfitted. This results in new predic\u2010 tors focusing more and more on the hard cases. This is the technique used by Ada\u2010 Boost. For example, to build an AdaBoost classifier, a first base classifier (such as a Decision Tree) is trained and used to make predictions on the training set. The relative weight of misclassified training instances is then increased. A second classifier is trained \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.232] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better. Figure 7-8. Decision boundaries of consecutive predictors Once all predictors are trained, the ensemble makes predictions very much like bag\u2010 ging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set. There is one important drawback to this sequential learning techni\u2010 \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.228] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 Let\u2019s go through a simple regression example using Decision Trees as the base predic\u2010 tors (of course Gradient Boosting also works great with regression tasks). This is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT). First, let\u2019s fit a DecisionTreeRegressor to the training set (for example, a noisy quadratic train\u2010 ing set): Boosting  |  197 \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.213] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 Stochastic Gradient Boosting. Boosting  |  201 \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193\n- [Score: 0.207] ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor. \u2192 with TensorFlow, 239-241 Gradient Tree Boosting, 197 GradientDescentOptimizer, 270 gradients(), 240 gradients, vanishing and exploding, 277-288, 404 Batch Normalization, 284-288 Glorot and He initialization, 278-281 gradient clipping, 288 nonsaturating activation functions, 281-283 graphviz, 170 greedy algorithm, 174 grid search, 73-75, 153 group(), 471 GRU (Gated Recurrent Unit) cell, 408-410 H hailstone sequence, 418 hard margin classification, 148-149 hard voting classifiers, 183-186 \u2192 can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its prede\u2010 cessor. There are many boosting methods available, but by far the most popular are Boosting  |  193"
    }
  },
  {
    "query": "What is random forest?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is random forest?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 18,
      "prompt": "QUERY: What is random forest?\nRELATED EVIDENCE PATHS:\n- [Score: 0.625] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.447] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 DecisionTreeClassifier (to control how trees are grown), plus all the hyperpara\u2010 meters of a BaggingClassifier to control the ensemble itself.11 The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. This results in a greater tree diversity, which (once again) trades a higher bias for a lower variance, \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.345] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 9 \u201cRandom Decision Forests,\u201d T. Ho (1995). 10 The BaggingClassifier class remains useful if you want a bag of something other than Decision Trees. 11 There are a few notable exceptions: splitter is absent (forced to \"random\"), presort is absent (forced to False), max_samples is absent (forced to 1.0), and base_estimator is absent (forced to DecisionTreeClassi fier with the provided hyperparameters). Sampling features results in even more predictor diversity, trading a bit more bias for \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.337] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 generally yielding an overall better model. The following BaggingClassifier is roughly equivalent to the previous RandomForestClassifier: bag_clf = BaggingClassifier(     DecisionTreeClassifier(splitter=\"random\", max_leaf_nodes=16),     n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1) Random Forests  |  191 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.331] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 TreesRegressor class has the same API as the RandomForestRegressor class. It is hard to tell in advance whether a RandomForestClassifier will perform better or worse than an ExtraTreesClassifier. Gen\u2010 erally, the only way to know is to try both and compare them using cross-validation (and tuning the hyperparameters using grid search). Feature Importance Yet another great quality of Random Forests is that they make it easy to measure the \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.328] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 feature. This extra randomness acts like a form of regularization: if a Random Forest overfits the training data, Extra-Trees might perform better. Moreover, since Extra-Trees don\u2019t search for the best possible thresholds, they are much 484  |  Appendix A: Exercise Solutions \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.324] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 12 \u201cExtremely randomized trees,\u201d P. Geurts, D. Ernst, L. Wehenkel (2005). Extra-Trees When you are growing a tree in a Random Forest, at each node only a random subset of the features is considered for splitting (as discussed earlier). It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular Decision Trees do). A forest of such extremely random trees is simply called an Extremely Randomized \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.307] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 additional validation set. Thus, you have more instances available for training, and your ensemble can perform slightly better. 5. When you are growing a tree in a Random Forest, only a random subset of the features is considered for splitting at each node. This is true as well for Extra- Trees, but they go one step further: rather than searching for the best possible thresholds, like regular Decision Trees do, they use random thresholds for each \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.267] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 flix Prize competition). In this chapter we will discuss the most popular Ensemble methods, including bag\u2010 ging, boosting, stacking, and a few others. We will also explore Random Forests. Voting Classifiers Suppose you have trained a few classifiers, each one achieving about 80% accuracy. You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier, and perhaps a few more (see Figure 7-1). 183 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.257] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 7. Ensemble Learning and Random Forests. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  183 Voting Classifiers                                                                                                         183 Bagging and Pasting                                                                                                    187 Bagging and Pasting in Scikit-Learn                                                                     188 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.242] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 Let\u2019s try one last model now: the RandomForestRegressor. As we will see in Chap\u2010 ter 7, Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions. Building a model on top of many other models is called Ensemble Learning, and it is often a great way to push ML algo\u2010 rithms even further. We will skip most of the code since it is essentially the same as for the other models: >>> from sklearn.ensemble import RandomForestRegressor \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.226] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 CHAPTER 6 Decision Trees Like SVMs, Decision Trees are versatile Machine Learning algorithms that can per\u2010 form both classification and regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex datasets. For example, in Chap\u2010 ter 2 you trained a DecisionTreeRegressor model on the California housing dataset, fitting it perfectly (actually overfitting it). Decision Trees are also the fundamental components of Random Forests (see Chap\u2010 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.226] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 trained on the moons dataset. As you can see, the ensemble\u2019s predictions will likely generalize much better than the single Decision Tree\u2019s predictions: the ensemble has a comparable bias but a smaller variance (it makes roughly the same number of errors on the training set, but the decision boundary is less irregular). 188  |  Chapter 7: Ensemble Learning and Random Forests \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.225] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 This tree looks very similar to the classification tree you built earlier. The main differ\u2010 ence is that instead of predicting a class in each node, it predicts a value. For example, suppose you want to make a prediction for a new instance with x1 = 0.6. You traverse the tree starting at the root, and you eventually reach the leaf node that predicts value=0.1106. This prediction is simply the average target value of the 110 training \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.214] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 a RandomForestRegressor class for regression tasks). The following code trains a Random Forest classifier with 500 trees (each limited to maximum 16 nodes), using all available CPU cores: from sklearn.ensemble import RandomForestClassifier rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1) rnd_clf.fit(X_train, y_train) y_pred_rf = rnd_clf.predict(X_test) With a few exceptions, a RandomForestClassifier has all the hyperparameters of a \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.208] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 6 As m grows, this ratio approaches 1 \u2013 exp(\u20131) \u2248 63.212%. Figure 7-5. A single Decision Tree versus a bagging ensemble of 500 trees Bootstrapping introduces a bit more diversity in the subsets that each predictor is trained on, so bagging ends up with a slightly higher bias than pasting, but this also means that predictors end up being less correlated so the ensemble\u2019s variance is reduced. Overall, bagging often results in better models, which explains why it is gen\u2010 \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.203] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 bag_clf.fit(X_train, y_train) y_pred = bag_clf.predict(X_test) The BaggingClassifier automatically performs soft voting instead of hard voting if the base classifier can estimate class proba\u2010 bilities (i.e., if it has a predict_proba() method), which is the case with Decision Trees classifiers. Figure 7-5 compares the decision boundary of a single Decision Tree with the deci\u2010 sion boundary of a bagging ensemble of 500 trees (from the preceding code), both \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra\n- [Score: 0.201] a lower variance. Random Forests As we have discussed, a Random Forest9 is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with max_samples set to the size of the training set. Instead of building a BaggingClassifier and pass\u2010 ing it a DecisionTreeClassifier, you can instead use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees10 (similarly, there is \u2192 This is one of the reasons why bagging and pasting are such popular methods: they scale very well. Bagging and Pasting in Scikit-Learn Scikit-Learn offers a simple API for both bagging and pasting with the BaggingClas sifier class (or BaggingRegressor for regression). The following code trains an ensemble of 500 Decision Tree classifiers,5 each trained on 100 training instances ran\u2010 domly sampled from the training set with replacement (this is an example of bagging, \u2192 Trees ensemble12 (or Extra-Trees for short). Once again, this trades more bias for a lower variance. It also makes Extra-Trees much faster to train than regular Random Forests since finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. You can create an Extra-Trees classifier using Scikit-Learn\u2019s ExtraTreesClassifier class. Its API is identical to the RandomForestClassifier class. Similarly, the Extra"
    }
  },
  {
    "query": "What is XGBoost?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is XGBoost?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is XGBoost?\nRELATED EVIDENCE PATHS:\n- [Score: 0.325] gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) gbrt.fit(X, y) 198  |  Chapter 7: Ensemble Learning and Random Forests \u2192 ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor.\n- [Score: 0.208] gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0) gbrt.fit(X, y) 198  |  Chapter 7: Ensemble Learning and Random Forests \u2192 13 \u201cA Decision-Theoretic Generalization of On-Line Learning and an Application to Boosting,\u201d Yoav Freund, Robert E. Schapire (1997). 14 This is just for illustrative purposes. SVMs are generally not good base predictors for AdaBoost, because they are slow and tend to be unstable with AdaBoost. AdaBoost13 (short for Adaptive Boosting) and Gradient Boosting. Let\u2019s start with Ada\u2010 Boost. AdaBoost One way for a new predictor to correct its predecessor is to pay a bit more attention \u2192 ing the base estimator. Gradient Boosting Another very popular Boosting algorithm is Gradient Boosting.17 Just like AdaBoost, Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like AdaBoost does, this method tries to fit the new predictor to the residual errors made by the previous predictor."
    }
  },
  {
    "query": "Explain the difference between parametric and non-parametric models.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain the difference between parametric and non-parametric models.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: Explain the difference between parametric and non-parametric models.\nRELATED EVIDENCE PATHS:\n- [Score: 0.391] model, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting). \u2192 tions. For example, a linear model makes the assumption that the data is fundamentally linear and that the distance between the instances and the straight line is just noise, which can safely be ignored. In a famous 1996 paper,11 David Wolpert demonstrated that if you make absolutely no assumption about the data, then there is no reason to prefer one model over any other. This is called the No Free Lunch (NFL) theorem. For some datasets the best 30  |  Chapter 1: The Machine Learning Landscape"
    }
  },
  {
    "query": "What is a generative adversarial network (GAN)?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a generative adversarial network (GAN)?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is a generative adversarial network (GAN)?\nRELATED EVIDENCE PATHS:\n- [Score: 0.448] each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. 7. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic \u2192 One network, called the \u201cdiscriminator,\u201d is trained to distinguish actual data from fake data produced by a second network, called the \u201cgenerator.\u201d The generator learns to trick the discriminator, while the discriminator learns to avoid the gen\u2010 erator\u2019s tricks. This competition leads to increasingly realistic fake data, and quite robust codings. Adversarial training is a very powerful idea, currently gaining a lot of momentum. Yann Lecun even called it \u201cthe coolest thing since sliced bread.\u201d\n- [Score: 0.280] each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. 7. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic \u2192 unsupervised pretraining of deep neural networks (as we discussed in Chapter 11). Lastly, they are capable of randomly generating new data that looks very similar to the training data; this is called a generative model. For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces. Surprisingly, autoencoders work by simply learning to copy their inputs to their out\u2010 \u2192 One network, called the \u201cdiscriminator,\u201d is trained to distinguish actual data from fake data produced by a second network, called the \u201cgenerator.\u201d The generator learns to trick the discriminator, while the discriminator learns to avoid the gen\u2010 erator\u2019s tricks. This competition leads to increasingly realistic fake data, and quite robust codings. Adversarial training is a very powerful idea, currently gaining a lot of momentum. Yann Lecun even called it \u201cthe coolest thing since sliced bread.\u201d\n- [Score: 0.257] each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. 7. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic \u2192 images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized\u2014for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available \u2192 One network, called the \u201cdiscriminator,\u201d is trained to distinguish actual data from fake data produced by a second network, called the \u201cgenerator.\u201d The generator learns to trick the discriminator, while the discriminator learns to avoid the gen\u2010 erator\u2019s tricks. This competition leads to increasingly realistic fake data, and quite robust codings. Adversarial training is a very powerful idea, currently gaining a lot of momentum. Yann Lecun even called it \u201cthe coolest thing since sliced bread.\u201d\n- [Score: 0.232] each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. 7. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic \u2192 tion. For example, about 75% of the time it would output the (0, 1, 1) triplet. Such a generative model can be used in a variety of ways. For example, if it is trained on images, and you provide an incomplete or noisy image to the network, it will automatically \u201crepair\u201d the image in a reasonable way. You can also use a generative model for classification. Just add a few visible neurons to encode the training image\u2019s \u2192 One network, called the \u201cdiscriminator,\u201d is trained to distinguish actual data from fake data produced by a second network, called the \u201cgenerator.\u201d The generator learns to trick the discriminator, while the discriminator learns to avoid the gen\u2010 erator\u2019s tricks. This competition leads to increasingly realistic fake data, and quite robust codings. Adversarial training is a very powerful idea, currently gaining a lot of momentum. Yann Lecun even called it \u201cthe coolest thing since sliced bread.\u201d\n- [Score: 0.201] each weight vector to the size of an input image (e.g., for MNIST, reshaping a weight vector of shape [784] to [28, 28]). To visualize the features learned by higher layers, one technique is to display the training instances that most activate each neuron. 7. A generative model is a model capable of randomly generating outputs that resemble the training instances. For example, once trained successfully on the MNIST dataset, a generative model can be used to randomly generate realistic \u2192 patterns, and then when they see a new pattern they (hopefully) output the closest learned pattern. This has made them useful in particular for character recognition before they were outperformed by other approaches. You first train the network by showing it examples of character images (each binary pixel maps to one neuron), and then when you show it a new character image, after a few iterations it outputs the closest learned character. \u2192 One network, called the \u201cdiscriminator,\u201d is trained to distinguish actual data from fake data produced by a second network, called the \u201cgenerator.\u201d The generator learns to trick the discriminator, while the discriminator learns to avoid the gen\u2010 erator\u2019s tricks. This competition leads to increasingly realistic fake data, and quite robust codings. Adversarial training is a very powerful idea, currently gaining a lot of momentum. Yann Lecun even called it \u201cthe coolest thing since sliced bread.\u201d"
    }
  },
  {
    "query": "What is reinforcement learning policy?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is reinforcement learning policy?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 36,
      "prompt": "QUERY: What is reinforcement learning policy?\nRELATED EVIDENCE PATHS:\n- [Score: 0.607] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.367] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.364] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.357] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 including a discussion of Markov decision processes (MDP). We will use these techni\u2010 ques to train a model to balance a pole on a moving cart, and another to play Atari games. The same techniques can be used for a wide variety of tasks, from walking robots to self-driving cars. Learning to Optimize Rewards In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.337] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 in the data and use them to make predictions. In Reinforcement Learning, the goal is to find a good policy. \u2022 Unlike in supervised learning, the agent is not explicitly given the \u201cright\u201d answer. It must learn by trial and error. \u2022 Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.335] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 directly try to optimize the policy to increase rewards, the algorithms we will look at now are less direct: the agent learns to estimate the expected sum of discounted future rewards for each state, or the expected sum of discounted future rewards for each action in each state, then uses this knowledge to decide how to act. To understand these algorithms, we must first introduce Markov decision processes (MDP). Markov Decision Processes \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.328] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the \u201cgene pool.\u201d Policy Search The algorithm used by the software agent to determine its actions is called its policy.  For example, the policy could be a neural network taking observations as inputs and outputting the action to take (see Figure 16-2). Figure 16-2. Reinforcement Learning using a neural network policy \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.320] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent\u2019s pieces. 6. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro\u2010 duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.310] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Go player, (d) thermostat, (e) automatic trader5 Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it better find the exit as quickly as possible! There are many other examples of tasks where Rein\u2010 forcement Learning is well suited, such as self-driving cars, placing ads on a web page, or controlling where an image classification system should focus its attention. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.300] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 \u2022 A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don\u2019t need to worry about explora\u2010 tion; they just feed on the training data they are given. \u2022 In supervised and unsupervised learning, training instances are typically inde\u2010 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.299] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Figure 16-3. Four points in policy space and the agent\u2019s corresponding behavior Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regards to the policy parameters, then tweaking these parameters by following the gradient toward higher rewards (gradient ascent). This approach is called policy gradients (PG), which we will discuss in more detail later in this chapter. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.292] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Evaluating Actions: The Credit Assignment Problem If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability and the tar\u2010 get probability. It would just be regular supervised learning. However, in Reinforce\u2010 ment Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.284] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.258] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). 7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally) \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.241] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 The policy can be any algorithm you can think of, and it does not even have to be deterministic. For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability p every second, or randomly rotate left or right with probability 1 \u2013 p. The rotation angle would be a random angle between \u2013r and +r. Since this policy involves some randomness, it is called a stochastic policy. The robot will have an \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.236] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.234] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 wildest dreams. In this chapter we will first explain what Reinforcement Learning is and what it is good at, and then we will present two of the most important techniques in deep Reinforcement Learning: policy gradients and deep Q-networks (DQN), 443 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.232] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 each transition at least once to know the rewards, and it must experience them multi\u2010 ple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.232] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 tive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observa\u2010 tions. 2. Here are a few possible applications of Reinforcement Learning, other than those mentioned in Chapter 16: Music personalization The environment is a user\u2019s personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.231] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 3. What is the discount rate? Can the optimal policy change if you modify the dis\u2010 count rate? 4. How do you measure the performance of a Reinforcement Learning agent? 5. What is the credit assignment problem? When does it occur? How can you allevi\u2010 ate it? Exercises  |  475 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.229] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 reward = R[s, a, sp]     learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)     Q[s, a] = ((1 - learning_rate) * Q[s, a] +                learning_rate * (reward + discount_rate * np.max(Q[sp]))     s = sp # move to next state Given enough iterations, this algorithm will converge to the optimal Q-Values. This is called an off-policy algorithm because the policy being trained is not the one being \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.228] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 exploration policy\u2014for example, a purely random policy\u2014to explore the MDP, and as it progresses the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 16-4). Equation 16-4. TD Learning algorithm Vk + 1 s 1 \u2212\u03b1 Vk s + \u03b1 r + \u03b3 . Vk s\u2032 \u2022 \u03b1 is the learning rate (e.g., 0.01). TD Learning has many similarities with Stochastic Gradient Descent, in particular the fact that it handles one sample at a time. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.225] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1): \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.219] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 rithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 16-5). Equation 16-5. Q-Learning algorithm Qk + 1 s, a 1 \u2212\u03b1 Qk s, a + \u03b1 r + \u03b3 . max a\u2032 Qk s\u2032, a\u2032 For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the rewards it expects to get later. Since the target policy would act optimally, we take the maximum \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.215] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 problems that can be tackled iteratively (in this case finding the action that maximizes the average reward plus the discounted next state value). Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not tell the agent explicitly what to do. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-Values. The optimal Q-Value of the state-action pair (s,a), noted Q*(s,a), is the sum of discounted \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.212] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 books). Note that there are actually two ways the agent can lose the game: either the pole can tilt too much, or the cart can go completely off the screen. With 250 training iterations, the policy learns to balance the pole quite well, but it is not yet good enough at avoiding going off the screen. A few hundred more training iterations will fix that. 458  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.212] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 goo.gl/yTsH6X. CHAPTER 16 Reinforcement Learning Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years,1 in particular in games (e.g., TD-Gammon, a Backgammon playing program) and in machine control, but seldom making the headline news. But a revolution took place in 2013 when researchers from an English \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.212] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well. Here\u2019s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you ran\u2010 domly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn\u2019t increase that probability up to 100%, or else you will \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.211] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 s2 choose action a1 (the only possible action). Interestingly, if you reduce the discount rate to 0.9, the optimal policy changes: in state s1 the best action becomes a0 (stay put; don\u2019t go through the fire). It makes sense because if you value the present much more than the future, then the prospect of future rewards is not worth immediate pain. Temporal Difference Learning and Q-Learning Reinforcement Learning problems with discrete actions can often be modeled as \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.210] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 and the agent\u2019s goal is to find a policy that will maximize rewards over time. For example, the MDP represented in Figure 16-8 has three states and up to three possible discrete actions at each step. If it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses action a1, it just remains in state s0 with cer\u2010 tainty, and without any reward. It can thus decide to stay there forever if it wants. But \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.208] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 \u2022 Getting insights about complex problems and large amounts of data. Types of Machine Learning Systems There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on: \u2022 Whether or not they are trained with human supervision (supervised, unsuper\u2010 vised, semisupervised, and Reinforcement Learning) \u2022 Whether or not they can learn incrementally on the fly (online versus batch learning) \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.207] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.206] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 environments, in reinforcement learning, 444-453, 465, 470 episodes (in RL), 450, 454-455, 457-458, 475 epochs, 120 \u03b5-insensitive, 157 equality contraints, 510 error analysis, 98-101 estimators, 62 Euclidian norm, 39 eval(), 242 evaluating models, 29-31 explained variance, 217 explained variance ratio, 216 exploding gradients, 278 (see also gradients, vanishing and explod\u2010 ing) exploration policies, 465 exponential decay, 287 exponential linear unit (ELU), 282-283 exponential scheduling, 306 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.205] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Learning to Optimize Rewards  |  445 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.205] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should inject as much prior knowledge as possible into the agent, as it will speed up training dramatically. For example, you could add negative rewards propor\u2010 tional to the distance from the center of the screen, and to the pole\u2019s angle. Also, if you already have a reasonably good policy (e.g., \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.205] reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 4. To measure the performance of a Reinforcement Learning agent, you can simply sum up the rewards it gets. In a simulated environment, you can run many epi\u2010 sodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on). 5. The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most"
    }
  },
  {
    "query": "Explain Q-learning.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain Q-learning.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 31,
      "prompt": "QUERY: Explain Q-learning.\nRELATED EVIDENCE PATHS:\n- [Score: 0.587] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.385] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 rithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 16-5). Equation 16-5. Q-Learning algorithm Qk + 1 s, a 1 \u2212\u03b1 Qk s, a + \u03b1 r + \u03b3 . max a\u2032 Qk s\u2032, a\u2032 For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the rewards it expects to get later. Since the target policy would act optimally, we take the maximum \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.363] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 approximate Q-Value to be as close as possible to the reward r that we actually observe after playing action a in state s, plus the discounted value of playing optimally from then on. To estimate this future discounted value, we can simply run the DQN on the next state s\u2032 and for all possible actions a\u2032. We get an approximate future Q- Value for each possible action. We then pick the highest (since we assume we will be \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.342] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Temporal Difference Learning and Q-Learning                                                     463 Exploration Policies                                                                                                 465 Approximate Q-Learning and Deep Q-Learning                                                466 Learning to Play Ms. Pac-Man Using the DQN Algorithm                                  467 x  |  Table of Contents \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.326] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 future rewards the agent can expect on average after it reaches the state s and chooses action a, but before it sees the outcome of this action, assuming it acts optimally after that action. Here is how it works: once again, you start by initializing all the Q-Value estimates to zero, then you update them using the Q-Value Iteration algorithm (see Equation 16-3). Equation 16-3. Q-Value Iteration algorithm Qk + 1 s, a \u2211 s\u2032 T s, a, s\u2032 R s, a, s\u2032 + \u03b3 . max a\u2032 Qk s\u2032, a\u2032 for all s, a \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.323] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 playing optimally), we discount it, and this gives us an estimate of the future discoun\u2010 ted value. By summing the reward r and the future discounted value estimate, we get a target Q-Value y(s, a) for the state-action pair (s, a), as shown in Equation 16-7. 466  |  Chapter 16: Reinforcement Learning \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.314] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 lutely no way you can keep track of an estimate for every single Q-Value. The solution is to find a function Q\u03b8 s, a  that approximates the Q-Value of any state- action pair (s,a) using a manageable number of parameters (given by the parameter vector \u03b8). This is called Approximate Q-Learning. For years it was recommended to use linear combinations of hand-crafted features extracted from the state (e.g., dis\u2010 tance of the closest ghosts, their directions, and so on) to estimate Q-Values, but \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.312] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 while still spending some time visiting unknown regions of the MDP. It is quite com\u2010 mon to start with a high value for \u03b5 (e.g., 1.0) and then gradually reduce it (e.g., down to 0.05). Alternatively, rather than relying on chance for exploration, another approach is to encourage the exploration policy to try actions that it has not tried much before. This can be implemented as a bonus added to the Q-Value estimates, as shown in Equation 16-6. Temporal Difference Learning and Q-Learning  |  465 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.309] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Just like SGD, it can only truly converge if you gradually reduce the learning rate (otherwise it will keep bouncing around the opti\u2010 mum). For each state s, this algorithm simply keeps track of a running average of the imme\u2010 diate rewards the agent gets upon leaving that state, plus the rewards it expects to get later (assuming it acts optimally). Similarly, the Q-Learning algorithm is an adaptation of the Q-Value Iteration algo\u2010 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.306] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 problems that can be tackled iteratively (in this case finding the action that maximizes the average reward plus the discounted next state value). Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not tell the agent explicitly what to do. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-Values. The optimal Q-Value of the state-action pair (s,a), noted Q*(s,a), is the sum of discounted \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.302] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 We first sample a batch of memories and we ask the target DQN to estimate the Q- Values of all possible actions for each memory\u2019s \u201cnext state\u201d, then we apply Equation 16-7 to compute y_val, containing the target Q-Value for each state-action pair. The only tricky part here is that we must multiply the max_next_q_values vector by the continues vector to zero out the future values that correspond to memories where the game ended. Next we run a training operation to improve the online DQN\u2019s abil\u2010 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.300] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 wildest dreams. In this chapter we will first explain what Reinforcement Learning is and what it is good at, and then we will present two of the most important techniques in deep Reinforcement Learning: policy gradients and deep Q-networks (DQN), 443 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.279] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 DeepMind showed that using deep neural networks can work much better, especially for complex problems, and it does not require any feature engineering. A DNN used to estimate Q-Values is called a deep Q-network (DQN), and using a DQN for Approximate Q-Learning is called Deep Q-Learning. Now how can we train a DQN? Well, consider the approximate Q-Value computed by the DQN for a given state-action pair (s,a). Thanks to Bellman, we know we want this \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.269] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 reward = R[s, a, sp]     learning_rate = learning_rate0 / (1 + iteration * learning_rate_decay)     Q[s, a] = ((1 - learning_rate) * Q[s, a] +                learning_rate * (reward + discount_rate * np.max(Q[sp]))     s = sp # move to next state Given enough iterations, this algorithm will converge to the optimal Q-Values. This is called an off-policy algorithm because the policy being trained is not the one being \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.266] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Figure 16-9. Ms. Pac-Man observation, original (left) and after preprocessing (right) Next, let\u2019s create the DQN. It could just take a state-action pair (s,a) as input, and out\u2010 put an estimate of the corresponding Q-Value Q(s,a), but since the actions are dis\u2010 crete it is more convenient and efficient to use a neural network that takes only a state s as input and outputs one Q-Value estimate per action. The DQN will be composed \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.266] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.259] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Equation 16-7. Target Q-Value y s, a = r + \u03b3 . max a\u2032 Q\u03b8 s\u2032, a\u2032 With this target Q-Value, we can run a training iteration using any Gradient Descent algorithm. Specifically, we generally try to minimize the squared error between the estimated Q-Value and the target Q-Value. And that\u2019s all for the basic Deep Q- Learning algorithm! However, in DeepMind\u2019s DQN algorithm, two crucial modifications were introduced: \u2022 Instead of training the DQN based on the latest experiences, DeepMind\u2019s DQN \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.246] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 s2 choose action a1 (the only possible action). Interestingly, if you reduce the discount rate to 0.9, the optimal policy changes: in state s1 the best action becomes a0 (stay put; don\u2019t go through the fire). It makes sense because if you value the present much more than the future, then the prospect of future rewards is not worth immediate pain. Temporal Difference Learning and Q-Learning Reinforcement Learning problems with discrete actions can often be modeled as \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.239] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 executed. It is somewhat surprising that this algorithm is capable of learning the opti\u2010 mal policy by just watching an agent act randomly (imagine learning to play golf when your teacher is a drunken monkey). Can we do better? Exploration Policies Of course Q-Learning can work only if the exploration policy explores the MDP thoroughly enough. Although a purely random policy is guaranteed to eventually visit every state and every transition many times, it may take an extremely long time \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.235] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 to do so. Therefore, a better option is to use the \u03b5-greedy policy: at each step it acts randomly with probability \u03b5, or greedily (choosing the action with the highest Q- Value) with probability 1-\u03b5. The advantage of the \u03b5-greedy policy (compared to a completely random policy) is that it will spend more and more time exploring the interesting parts of the environment, as the Q-Value estimates get better and better, \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.232] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 Now, let\u2019s add the online DQN\u2019s training operations. First, we need to be able to com\u2010 pute its predicted Q-Value for each state-action pair in the memory batch. Since the DQN outputs one Q-Value for every possible action, we need to keep only the Q- Value that corresponds to the action that was actually played. For this, we will convert the action to a one-hot vector (recall that this is a vector full of 0s except for a 1 at the \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.224] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 ity to predict Q-Values. Finally, at regular intervals we copy the online DQN to the target DQN, and we save the model. Unfortunately, training is very slow: if you use your laptop for training, it will take days before Ms. Pac-Man gets any good. You can plot learning curves, for example measuring the average rewards per game, or computing the max Q-Value estimated by the online DQN at every game step and tracking the mean of these \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.218] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.215] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 an environment state as input (in this example, a single preprocessed observation) and outputting an estimated Q-Value for each possible action in that state. Plus we have an operation called copy_online_to_target to copy the values of all the traina\u2010 ble variables of the online DQN to the corresponding target DQN variables. We use TensorFlow\u2019s tf.group() function to group all the assignment operations into a sin\u2010 gle convenient operation. Learning to Play Ms. Pac-Man Using the DQN Algorithm \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.215] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 exploration policy\u2014for example, a purely random policy\u2014to explore the MDP, and as it progresses the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 16-4). Equation 16-4. TD Learning algorithm Vk + 1 s 1 \u2212\u03b1 Vk s + \u03b1 r + \u03b3 . Vk s\u2032 \u2022 \u03b1 is the learning rate (e.g., 0.01). TD Learning has many similarities with Stochastic Gradient Descent, in particular the fact that it handles one sample at a time. \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.213] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 algorithm stores experiences in a large replay memory, and it samples a random training batch from it at each training iteration. This helps reduce the correla\u2010 tions between the experiences in a training batch, which tremendously helps training. \u2022 The algorithm uses two DQNs instead of one: the first one, called the online DQN is the one that plays and learns at each training iteration. The second, called the target DQN is only used to compute the target Q-Values (Equation 16-7). At \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.206] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 a\u2032 Qk s\u2032, a\u2032 for all s, a Once you have the optimal Q-Values, defining the optimal policy, noted \u03c0*(s), is triv\u2010 ial: when the agent is in state s, it should choose the action with the highest Q-Value for that state: \u03c0* s = argmax a Q* s, a . Let\u2019s apply this algorithm to the MDP represented in Figure 16-8. First, we need to define the MDP: nan = np.nan  # represents impossible actions T = np.array([  # shape=[s, a, s']         [[0.7, 0.3, 0.0], [1.0, 0.0, 0.0], [0.8, 0.2, 0.0]], 462  | \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.203] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 given that the agent chose action a. \u2022 \u03b3 is the discount rate. This equation leads directly to an algorithm that can precisely estimate the optimal state value of every possible state: you first initialize all the state value estimates to zero, and then you iteratively update them using the Value Iteration algorithm (see Equation 16-2). A remarkable result is that, given enough time, these estimates are Markov Decision Processes  |  461 \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.202] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 each transition at least once to know the rewards, and it must experience them multi\u2010 ple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.201] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 462  |  Chapter 16: Reinforcement Learning \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning\n- [Score: 0.201] Equation 16-6. Q-Learning using an exploration function Q s, a 1 \u2212\u03b1 Q s, a + \u03b1 r + \u03b3 . max \u03b1\u2032 f Q s\u2032, a\u2032 , N s\u2032, a\u2032 \u2022 N(s\u2032, a\u2032) counts the number of times the action a\u2032 was chosen in state s\u2032. \u2022 f(q, n) is an exploration function, such as f(q, n) = q + K/(1 + n), where K is a curiosity hyperparameter that measures how much the agent is attracted to to the unknown. Approximate Q-Learning and Deep Q-Learning The main problem with Q-Learning is that it does not scale well to large (or even \u2192 max Q-Values over each game. You will notice that these curves are extremely noisy. At some points there may be no apparent progress for a very long time until suddenly the agent learns to survive a reasonable amount of time. As mentioned earlier, one solution is to inject as much prior knowledge as possible into the model (e.g., through preprocessing, rewards, and so on), and you can also try to bootstrap the model by first training it to imitate a basic strategy. In \u2192 of the Q-Value estimates for the next state. Here is how Q-Learning can be implemented: 464  |  Chapter 16: Reinforcement Learning"
    }
  },
  {
    "query": "What is the exploration vs exploitation dilemma?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the exploration vs exploitation dilemma?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 21,
      "prompt": "QUERY: What is the exploration vs exploitation dilemma?\nRELATED EVIDENCE PATHS:\n- [Score: 0.453] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.303] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.293] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent\u2019s pieces. 6. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro\u2010 duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.292] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.264] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 in the data and use them to make predictions. In Reinforcement Learning, the goal is to find a good policy. \u2022 Unlike in supervised learning, the agent is not explicitly given the \u201cright\u201d answer. It must learn by trial and error. \u2022 Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.255] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 Figure 16-3. Four points in policy space and the agent\u2019s corresponding behavior Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regards to the policy parameters, then tweaking these parameters by following the gradient toward higher rewards (gradient ascent). This approach is called policy gradients (PG), which we will discuss in more detail later in this chapter. \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.251] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well. Here\u2019s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you ran\u2010 domly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn\u2019t increase that probability up to 100%, or else you will \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.248] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 including a discussion of Markov decision processes (MDP). We will use these techni\u2010 ques to train a model to balance a pole on a moving cart, and another to play Atari games. The same techniques can be used for a wide variety of tasks, from walking robots to self-driving cars. Learning to Optimize Rewards In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.247] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 \u2022 A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don\u2019t need to worry about explora\u2010 tion; they just feed on the training data they are given. \u2022 In supervised and unsupervised learning, training instances are typically inde\u2010 \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.246] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 Go player, (d) thermostat, (e) automatic trader5 Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it better find the exit as quickly as possible! There are many other examples of tasks where Rein\u2010 forcement Learning is well suited, such as self-driving cars, placing ads on a web page, or controlling where an image classification system should focus its attention. \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.243] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 directly try to optimize the policy to increase rewards, the algorithms we will look at now are less direct: the agent learns to estimate the expected sum of discounted future rewards for each state, or the expected sum of discounted future rewards for each action in each state, then uses this knowledge to decide how to act. To understand these algorithms, we must first introduce Markov decision processes (MDP). Markov Decision Processes \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.234] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.221] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). 7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally) \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.219] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 and the agent\u2019s goal is to find a policy that will maximize rewards over time. For example, the MDP represented in Figure 16-8 has three states and up to three possible discrete actions at each step. If it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses action a1, it just remains in state s0 with cer\u2010 tainty, and without any reward. It can thus decide to stay there forever if it wants. But \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.218] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should inject as much prior knowledge as possible into the agent, as it will speed up training dramatically. For example, you could add negative rewards propor\u2010 tional to the distance from the center of the screen, and to the pole\u2019s angle. Also, if you already have a reasonably good policy (e.g., \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.214] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.213] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 problems that can be tackled iteratively (in this case finding the action that maximizes the average reward plus the discounted next state value). Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not tell the agent explicitly what to do. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-Values. The optimal Q-Value of the state-action pair (s,a), noted Q*(s,a), is the sum of discounted \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.210] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.209] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the \u201cgene pool.\u201d Policy Search The algorithm used by the software agent to determine its actions is called its policy.  For example, the policy could be a neural network taking observations as inputs and outputting the action to take (see Figure 16-2). Figure 16-2. Reinforcement Learning using a neural network policy \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.207] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari\u2019s Pong, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):\n- [Score: 0.201] while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 Evaluating Actions: The Credit Assignment Problem If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability and the tar\u2010 get probability. It would just be regular supervised learning. However, in Reinforce\u2010 ment Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1):"
    }
  },
  {
    "query": "What is the Markov decision process?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the Markov decision process?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: What is the Markov decision process?\nRELATED EVIDENCE PATHS:\n- [Score: 0.582] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory).\n- [Score: 0.274] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory).\n- [Score: 0.263] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459 \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory).\n- [Score: 0.246] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 Figure 16-8. Example of a Markov decision process Bellman found a way to estimate the optimal state value of any state s, noted V*(s), which is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally. He showed that if the agent acts optimally, then the Bellman Optimality Equation applies (see Equation 16-1). This recursive equation says that if the agent acts optimally, then the optimal value of the \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory).\n- [Score: 0.241] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 Figure 16-7 shows an example of a Markov chain with four states. Suppose that the process starts in state s0, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back since no other state points back to s0. If it goes to state s1, it will then most likely go to state s2 (90% probability), then immediately back to state s1 (with 100% probability). It may \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory).\n- [Score: 0.205] 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 The policy can be any algorithm you can think of, and it does not even have to be deterministic. For example, consider a robotic vacuum cleaner whose reward is the amount of dust it picks up in 30 minutes. Its policy could be to move forward with some probability p every second, or randomly rotate left or right with probability 1 \u2013 p. The rotation angle would be a random angle between \u2013r and +r. Since this policy involves some randomness, it is called a stochastic policy. The robot will have an \u2192 Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory)."
    }
  },
  {
    "query": "What are autoencoders?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What are autoencoders?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 42,
      "prompt": "QUERY: What are autoencoders?\nRELATED EVIDENCE PATHS:\n- [Score: 0.655] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.370] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Because the internal representation has a lower dimensionality than the input data (it is 2D instead of 3D), the autoencoder is said to be undercomplete. An undercomplete autoencoder cannot trivially copy its inputs to the codings, yet it must find a way to output a copy of its inputs. It is forced to learn the most important features in the input data (and drop the unimportant ones). Let\u2019s see how to implement a very simple undercomplete autoencoder for dimension\u2010 ality reduction. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.325] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 bread.\u201d Exercises 1. What are the main tasks that autoencoders are used for? 2. Suppose you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances. How can autoencoders help? How would you proceed? 3. If an autoencoder perfectly reconstructs the inputs, is it necessarily a good autoencoder? How can you evaluate the performance of an autoencoder? 4. What are undercomplete and overcomplete autoencoders? What is the main risk \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.322] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Other Autoencoders The amazing successes of supervised learning in image recognition, speech recogni\u2010 tion, text translation, and more have somewhat overshadowed unsupervised learning, but it is actually booming. New architectures for autoencoders and other unsuper\u2010 vised learning algorithms are invented regularly, so much so that we cannot cover them all in this book. Here is a brief (by no means exhaustive) overview of a few more types of autoencoders that you may want to check out: \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.296] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 8 \u201cContractive Auto-Encoders: Explicit Invariance During Feature Extraction,\u201d S. Rifai et al. (2011). Figure 15-12. Images of handwritten digits generated by the variational autoencoder A majority of these digits look pretty convincing, while a few are rather \u201ccreative.\u201d But don\u2019t be too harsh on the autoencoder\u2014it only started learning less than an hour ago. Give it a bit more training time, and those digits will look better and better. Other Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.296] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 internal representation, followed by a decoder (or generative network) that converts the internal representation to the outputs (see Figure 15-1). As you can see, an autoencoder typically has the same architecture as a Multi-Layer Perceptron (MLP; see Chapter 10), except that the number of neurons in the output layer must be equal to the number of inputs. In this example, there is just one hidden 418  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.282] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Looks close enough. So the autoencoder has properly learned to reproduce its inputs, but has it learned useful features? Let\u2019s take a look. Visualizing Features Once your autoencoder has learned some features, you may want to take a look at them. There are various techniques for this. Arguably the simplest technique is to consider each neuron in every hidden layer, and find the training instances that acti\u2010 vate it the most. This is especially useful for the top hidden layers since they often \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.274] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 new samples. Solutions to these exercises are available in Appendix A. 442  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.271] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Training One Autoencoder at a Time                                                                   424 Visualizing the Reconstructions                                                                            427 Visualizing Features                                                                                                 427 Unsupervised Pretraining Using Stacked Autoencoders                                       428 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.271] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 many other kinds of constraints that can be used, including ones that allow the cod\u2010 ing layer to be just as large as the inputs, or even larger, resulting in an overcomplete autoencoder. Let\u2019s look at some of those approaches now. Denoising Autoencoders Another way to force the autoencoder to learn useful features is to add noise to its inputs, training it to recover the original, noise-free inputs. This prevents the autoen\u2010 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.269] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 unsupervised pretraining of deep neural networks (as we discussed in Chapter 11). Lastly, they are capable of randomly generating new data that looks very similar to the training data; this is called a generative model. For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces. Surprisingly, autoencoders work by simply learning to copy their inputs to their out\u2010 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.268] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 puts. This may sound like a trivial task, but we will see that constraining the network in various ways can make it rather difficult. For example, you can limit the size of the internal representation, or you can add noise to the inputs and train the network to recover the original inputs. These constraints prevent the autoencoder from trivially copying the inputs directly to the outputs, which forces it to learn efficient ways of \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.267] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 for a very deep autoencoder to learn to map each training instance to a different coding (e.g., the first instance could be mapped to 0.001, the second to 0.002, the third to 0.003, and so on), and it could learn \u201cby heart\u201d to reconstruct the right training instance for each coding. It would perfectly reconstruct its inputs without really learning any useful pattern in the data. In practice such a mapping is unlikely to happen, but it illustrates the fact that perfect reconstructions are not \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.265] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 coder from trivially copying its inputs to its outputs, so it ends up having to find pat\u2010 terns in the data. The idea of using autoencoders to remove noise has been around since the 1980s (e.g., it is mentioned in Yann LeCun\u2019s 1987 master\u2019s thesis). In a 2008 paper,3 Pascal Vincent et al. showed that autoencoders could also be used for feature extraction. In a 2010 paper,4 Vincent et al. introduced stacked denoising autoencoders. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.260] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 \u2022 Dimensionality reduction \u2022 Generative models \u2022 Anomaly detection (an autoencoder is generally bad at reconstructing outliers) 2. If you want to train a classifier and you have plenty of unlabeled training data, but only a few thousand labeled instances, then you could first train a deep autoencoder on the full dataset (labeled + unlabeled), then reuse its lower half for the classifier (i.e., reuse the layers up to the codings layer, included) and train the \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.255] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 ter 12). For the solutions to exercises 7, 8, and 9, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. Chapter 15: Autoencoders 1. Here are some of the main tasks that autoencoders are used for: \u2022 Feature extraction \u2022 Unsupervised pretraining 498  |  Appendix A: Exercise Solutions \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.253] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 so powerful that it just learns to map each input to a single arbitrary number (and the decoder learns the reverse mapping). Obviously such an autoencoder will reconstruct the training data perfectly, but it will not have learned any useful data representation in the process (and it is unlikely to generalize well to new instances). The architecture of a stacked autoencoder is typically symmetrical with regards to the \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.252] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 reconstruction loss that pushes the autoencoder to reproduce its inputs (we can use cross entropy for this, as discussed earlier). The second is the latent loss that pushes the autoencoder to have codings that look as though they were sampled from a simple Gaussian distribution, for which we use the KL divergence between the target distri\u2010 bution (the Gaussian distribution) and the actual distribution of the codings. The \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.250] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 2 \u201cGreedy Layer-Wise Training of Deep Networks,\u201d Y. Bengio et al. (2007). 3 \u201cExtracting and Composing Robust Features with Denoising Autoencoders,\u201d P. Vincent et al. (2008). 4 \u201cStacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denois\u2010 ing Criterion,\u201d P. Vincent et al. (2010). that (see Appendix E), but in 2007 Yoshua Bengio et al. showed2 that autoencoders worked just as well. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.248] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 classifier using the labeled data. If you have little labeled data, you probably want to freeze the reused layers when training the classifier. 3. The fact that an autoencoder perfectly reconstructs its inputs does not necessarily mean that it is a good autoencoder; perhaps it is simply an overcomplete autoen\u2010 coder that learned to copy its inputs to the codings layer and then to the outputs. In fact, even if the codings layer contained a single neuron, it would be possible \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.247] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 438  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.243] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 \u2022 One neat trick proposed by Salakhutdinov and Hinton is to add Gaussian noise (with zero mean) to the inputs of the coding layer, during training only. In order to preserve a high signal-to-noise ratio, the autoencoder will learn to Exercises  |  441 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.243] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 training and visualizing, 169-171 decoder, 418 deconvolutional layer, 380 deep autoencoders (see stacked autoencoders) Index  |  533 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.241] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Figure 15-8 shows how to use a stacked autoencoder to perform unsupervised pre\u2010 training for a classification neural network. The stacked autoencoder itself is typically trained one autoencoder at a time, as discussed earlier. When training the classifier, if you really don\u2019t have much labeled training data, you may want to freeze the pre\u2010 trained layers (at least the lower ones). Figure 15-8. Unsupervised pretraining using autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.232] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.232] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 a guarantee that the autoencoder learned anything useful. However, if it produces very bad reconstructions, then it is almost guaranteed to be a bad autoencoder. To evaluate the performance of an autoencoder, one option is to measure the reconstruction loss (e.g., compute the MSE, the mean square of the outputs minus the inputs). Again, a high reconstruction loss is a good sign that the autoencoder is bad, but a low reconstruction loss is not a guarantee that it is \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.231] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Figure 15-2. PCA performed by an undercomplete linear autoencoder Stacked Autoencoders Just like other neural networks we have discussed, autoencoders can have multiple hidden layers. In this case they are called stacked autoencoders (or deep autoencoders).  Adding more layers helps the autoencoder learn more complex codings. However, one must be careful not to make the autoencoder too powerful. Imagine an encoder \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.230] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 reconstruct the inputs. The main risk of an overcomplete autoencoder is that it may just copy the inputs to the outputs, without learning any useful feature. 5. To tie the weights of an encoder layer and its corresponding decoder layer, you simply make the decoder weights equal to the transpose of the encoder weights. This reduces the number of parameters in the model by half, often making train\u2010 ing converge faster with less training data, and reducing the risk of overfitting the training set. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.228] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 A generalization of denoising autoencoders, with the added capability to generate data. Winner-take-all (WTA) autoencoder11 During training, after computing the activations of all the neurons in the coding layer, only the top k% activations for each neuron over the training batch are pre\u2010 served, and the rest are set to zero. Naturally this leads to sparse codings. More\u2010 over, a similar WTA approach can be used to produce sparse convolutional autoencoders. Generative Adversarial Network (GAN)12 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.228] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 math is a bit more complex than earlier, in particular because of the Gaussian noise, which limits the amount of information that can be transmitted to the coding layer 436  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.221] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 13 \u201cSemantic Hashing,\u201d R. Salakhutdinov and G. Hinton (2008). 6. What is a common technique to visualize features learned by the lower layer of a stacked autoencoder? What about higher layers? 7. What is a generative model? Can you name a type of generative autoencoder? 8. Let\u2019s use a denoising autoencoder to pretrain an image classifier: \u2022 You can use MNIST (simplest), or another large set of images such as CIFAR10 if you want a bigger challenge. If you choose CIFAR10, you need to write code \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.220] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 images of digits. The output distribution is typically similar to the training data. For example, since MNIST contains many images of each digit, the generative model would output roughly the same number of images of each digit. Some generative models can be parametrized\u2014for example, to generate only some kinds of outputs. An example of a generative autoencoder is the variational autoencoder. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.219] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 several times (performing gradient ascent), the image will gradually turn into the most exciting image (for the neuron). This is a useful technique to visualize the kinds of inputs that a neuron is looking for. Finally, if you are using an autoencoder to perform unsupervised pretraining\u2014for example, for a classification task\u2014a simple way to verify that the features learned by the autoencoder are useful is to measure the performance of the classifier. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.217] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Contractive autoencoder (CAE)8 The autoencoder is constrained during training so that the derivatives of the cod\u2010 ings with regards to the inputs are small. In other words, two similar inputs must have similar codings. Other Autoencoders  |  439 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.211] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 worked just as well. There is nothing special about the TensorFlow implementation: just train an autoen\u2010 coder using all the training data, then reuse its encoder layers to create a new neural network (see Chapter 11 for more details on how to reuse pretrained layers, or check out the code examples in the Jupyter notebooks). Up to now, in order to force the autoencoder to learn interesting features, we have limited the size of the coding layer, making it undercomplete. There are actually \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.210] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 Unsupervised Pretraining Using Stacked Autoencoders As we discussed in Chapter 11, if you are tackling a complex supervised task but you do not have a lot of labeled training data, one solution is to find a neural network that performs a similar task, and then reuse its lower layers. This makes it possible to train 428  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.209] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 training set. 6. To visualize the features learned by the lower layer of a stacked autoencoder, a common technique is simply to plot the weights of each neuron, by reshaping Exercise Solutions  |  499 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.204] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 inputs. During the second phase, the second autoencoder learns to reconstruct the output of the first autoencoder\u2019s hidden layer. Finally, you just build a big sandwich using all these autoencoders, as shown in Figure 15-4 (i.e., you first stack the hidden layers of each autoencoder, then the output layers in reverse order). This gives you the final stacked autoencoder. You could easily train more autoencoders this way, building a very deep stacked autoencoder. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.203] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 n_iterations = 1000 codings = hidden  # the output of the hidden layer provides the codings with tf.Session() as sess:     init.run()     for iteration in range(n_iterations):         training_op.run(feed_dict={X: X_train})  # no labels (unsupervised)     codings_val = codings.eval(feed_dict={X: X_test}) Figure 15-2 shows the original 3D dataset (at the left) and the output of the autoen\u2010 coder\u2019s hidden layer (i.e., the coding layer, at the right). As you can see, the autoen\u2010 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.201] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 2 Some neural network architectures can be unsupervised, such as autoencoders and restricted Boltzmann machines. They can also be semisupervised, such as in deep belief networks and unsupervised pretraining. In Machine Learning an attribute is a data type (e.g., \u201cMileage\u201d), while a feature has several meanings depending on the context, but generally means an attribute plus its value (e.g., \u201cMileage = 15,000\u201d). Many people use the words attribute and feature inter\u2010 changeably, though. \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.200] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 5 \u201cAuto-Encoding Variational Bayes,\u201d D. Kingma and M. Welling (2014). Variational Autoencoders Another important category of autoencoders was introduced in 2014 by Diederik Kingma and Max Welling,5 and has quickly become one of the most popular types of autoencoders: variational autoencoders. They are quite different from all the autoencoders we have discussed so far, in partic\u2010 ular: \u2022 They are probabilistic autoencoders, meaning that their outputs are partly deter\u2010 \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417\n- [Score: 0.200] CHAPTER 15 Autoencoders Autoencoders are artificial neural networks capable of learning efficient representa\u2010 tions of the input data, called codings, without any supervision (i.e., the training set is unlabeled). These codings typically have a much lower dimensionality than the input data, making autoencoders useful for dimensionality reduction (see Chapter 8). More importantly, autoencoders act as powerful feature detectors, and they can be used for \u2192 The noise can be pure Gaussian noise added to the inputs, or it can be randomly switched off inputs, just like in dropout (introduced in Chapter 11). Figure 15-9 shows both options. 430  |  Chapter 15: Autoencoders \u2192 representing the data. In short, the codings are byproducts of the autoencoder\u2019s attempt to learn the identity function under some constraints. In this chapter we will explain in more depth how autoencoders work, what types of constraints can be imposed, and how to implement them using TensorFlow, whether it is for dimensionality reduction, feature extraction, unsupervised pretraining, or as generative models. 417"
    }
  },
  {
    "query": "What is unsupervised pretraining?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is unsupervised pretraining?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 11,
      "prompt": "QUERY: What is unsupervised pretraining?\nRELATED EVIDENCE PATHS:\n- [Score: 0.512] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.386] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Unsupervised Pretraining Using Stacked Autoencoders As we discussed in Chapter 11, if you are tackling a complex supervised task but you do not have a lot of labeled training data, one solution is to find a neural network that performs a similar task, and then reuse its lower layers. This makes it possible to train 428  |  Chapter 15: Autoencoders \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.337] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Figure 15-8 shows how to use a stacked autoencoder to perform unsupervised pre\u2010 training for a classification neural network. The stacked autoencoder itself is typically trained one autoencoder at a time, as discussed earlier. When training the classifier, if you really don\u2019t have much labeled training data, you may want to freeze the pre\u2010 trained layers (at least the lower ones). Figure 15-8. Unsupervised pretraining using autoencoders \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.319] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 This is a rather long and tedious process, but it often works well; in fact, it is this technique that Geoffrey Hinton and his team used in 2006 and which led to the revival of neural networks and the success of Deep Learning. Until 2010, unsuper\u2010 vised pretraining (typically using RBMs) was the norm for deep nets, and it was only after the vanishing gradients problem was alleviated that it became much more com\u2010 mon to train DNNs purely using backpropagation. However, unsupervised pretrain\u2010 \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.270] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Figure 11-5. Unsupervised pretraining Pretraining on an Auxiliary Task One last option is to train a first neural network on an auxiliary task for which you can easily obtain or generate labeled training data, then reuse the lower layers of that network for your actual task. The first neural network\u2019s lower layers will learn feature detectors that will likely be reusable by the second neural network. For example, if you want to build a system to recognize faces, you may only have a \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.228] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Training One Autoencoder at a Time                                                                   424 Visualizing the Reconstructions                                                                            427 Visualizing Features                                                                                                 427 Unsupervised Pretraining Using Stacked Autoencoders                                       428 \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.223] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 unsupervised pretraining of deep neural networks (as we discussed in Chapter 11). Lastly, they are capable of randomly generating new data that looks very similar to the training data; this is called a generative model. For example, you could train an autoencoder on pictures of faces, and it would then be able to generate new faces. Surprisingly, autoencoders work by simply learning to copy their inputs to their out\u2010 \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.219] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 a high-performance model using only little training data because your neural net\u2010 work won\u2019t have to learn all the low-level features; it will just reuse the feature detec\u2010 tors learned by the existing net. Similarly, if you have a large dataset but most of it is unlabeled, you can first train a stacked autoencoder using all the data, then reuse the lower layers to create a neural network for your actual task, and train it using the labeled data. For example, \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.213] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Other Autoencoders The amazing successes of supervised learning in image recognition, speech recogni\u2010 tion, text translation, and more have somewhat overshadowed unsupervised learning, but it is actually booming. New architectures for autoencoders and other unsuper\u2010 vised learning algorithms are invented regularly, so much so that we cannot cover them all in this book. Here is a brief (by no means exhaustive) overview of a few more types of autoencoders that you may want to check out: \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.209] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 Unsupervised learning In unsupervised learning, as you might guess, the training data is unlabeled (Figure 1-7). The system tries to learn without a teacher. Figure 1-7. An unlabeled training set for unsupervised learning Here are some of the most important unsupervised learning algorithms (we will cover dimensionality reduction in Chapter 8): \u2022 Clustering \u2014 k-Means \u2014 Hierarchical Cluster Analysis (HCA) \u2014 Expectation Maximization \u2022 Visualization and dimensionality reduction \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429\n- [Score: 0.205] Unsupervised Pretraining Suppose you want to tackle a complex task for which you don\u2019t have much labeled training data, but unfortunately you cannot find a model trained on a similar task. Don\u2019t lose all hope! First, you should of course try to gather more labeled training data, but if this is too hard or too expensive, you may still be able to perform unsuper\u2010 vised pretraining (see Figure 11-5). That is, if you have plenty of unlabeled training \u2192 and it is able to name everyone in every photo, which is useful for searching photos. Figure 1-11. Semisupervised learning Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsu\u2010 pervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the \u2192 the discovery in 2006 by Geoffrey Hinton et al. that deep neural networks can be pre\u2010 trained in an unsupervised fashion. They used restricted Boltzmann machines for Unsupervised Pretraining Using Stacked Autoencoders  |  429"
    }
  },
  {
    "query": "What is the difference between AI, ML, and Deep Learning?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the difference between AI, ML, and Deep Learning?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 2,
      "prompt": "QUERY: What is the difference between AI, ML, and Deep Learning?\nRELATED EVIDENCE PATHS:\n- [Score: 0.310] \u2022 Machine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules. \u2022 There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based, and so on. \u2022 In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based it tunes some parameters to \u2192 artificial neural networks (ANNs). However, although planes were inspired by birds, they don\u2019t have to flap their wings. Similarly, ANNs have gradually become quite dif\u2010 ferent from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying \u201cunits\u201d rather than \u201cneurons\u201d), lest we restrict our creativity to biologically plausible systems.1 ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\u2010\n- [Score: 0.225] \u2022 Machine Learning is about making machines get better at some task by learning from data, instead of having to explicitly code rules. \u2022 There are many different types of ML systems: supervised or not, batch or online, instance-based or model-based, and so on. \u2022 In a ML project you gather data in a training set, and you feed the training set to a learning algorithm. If the algorithm is model-based it tunes some parameters to \u2192 \u2022 Getting insights about complex problems and large amounts of data. Types of Machine Learning Systems There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on: \u2022 Whether or not they are trained with human supervision (supervised, unsuper\u2010 vised, semisupervised, and Reinforcement Learning) \u2022 Whether or not they can learn incrementally on the fly (online versus batch learning) \u2192 artificial neural networks (ANNs). However, although planes were inspired by birds, they don\u2019t have to flap their wings. Similarly, ANNs have gradually become quite dif\u2010 ferent from their biological cousins. Some researchers even argue that we should drop the biological analogy altogether (e.g., by saying \u201cunits\u201d rather than \u201cneurons\u201d), lest we restrict our creativity to biologically plausible systems.1 ANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\u2010"
    }
  },
  {
    "query": "Explain the concept of feature engineering.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain the concept of feature engineering.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Explain the concept of feature engineering.\nRELATED EVIDENCE PATHS:\n- [Score: 0.527] ing if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves: Main Challenges of Machine Learning  |  25 \u2192 \u2022 Decompose features (e.g., categorical, date/time, etc.). \u2022 Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). \u2022 Aggregate features into promising new features. 4. Feature scaling: standardize or normalize features. Short-List Promising Models Notes: \u2022 If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).\n- [Score: 0.276] ing if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves: Main Challenges of Machine Learning  |  25 \u2192 \u2022 Feature selection: selecting the most useful features to train on among existing features. \u2022 Feature extraction: combining existing features to produce a more useful one (as we saw earlier, dimensionality reduction algorithms can help). \u2022 Creating new features by gathering new data. Now that we have looked at many examples of bad data, let\u2019s look at a couple of exam\u2010 ples of bad algorithms. Overfitting the Training Data \u2192 \u2022 Decompose features (e.g., categorical, date/time, etc.). \u2022 Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). \u2022 Aggregate features into promising new features. 4. Feature scaling: standardize or normalize features. Short-List Promising Models Notes: \u2022 If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests).\n- [Score: 0.269] ing if the training data contains enough relevant features and not too many irrelevant ones. A critical part of the success of a Machine Learning project is coming up with a good set of features to train on. This process, called feature engineering, involves: Main Challenges of Machine Learning  |  25 \u2192 remove noise and redundant features, making the training algorithm per\u2010 form better). \u2014 To visualize the data and gain insights on the most important features. \u2014 Simply to save space (compression). \u2022 The main drawbacks are: \u2014 Some information is lost, possibly degrading the performance of subse\u2010 quent training algorithms. \u2014 It can be computationally intensive. \u2014 It adds some complexity to your Machine Learning pipelines. \u2014 Transformed features are often hard to interpret. \u2192 \u2022 Decompose features (e.g., categorical, date/time, etc.). \u2022 Add promising transformations of features (e.g., log(x), sqrt(x), x2, etc.). \u2022 Aggregate features into promising new features. 4. Feature scaling: standardize or normalize features. Short-List Promising Models Notes: \u2022 If the data is huge, you may want to sample smaller training sets so you can train many different models in a reasonable time (be aware that this penalizes complex models such as large neural nets or Random Forests)."
    }
  },
  {
    "query": "What is a confusion matrix?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a confusion matrix?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is a confusion matrix?\nRELATED EVIDENCE PATHS:\n- [Score: 0.549] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.377] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 Now you are ready to get the confusion matrix using the confusion_matrix() func\u2010 tion. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred): >>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred) array([[53272,  1307],        [ 1077,  4344]]) Each row in a confusion matrix represents an actual class, while each column repre\u2010 sents a predicted class. The first row of this matrix considers non-5 images (the nega\u2010 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.362] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including preci\u2010 sion and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1307) 0.76871350203503808 >>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.80132816823464303 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.243] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 you want to find ways to improve it. One way to do this is to analyze the types of errors it makes. First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier: >>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) >>> conf_mx = confusion_matrix(y_train, y_train_pred) >>> conf_mx array([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4], \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.225] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others). Confusion Matrix A much better way to evaluate the performance of a classifier is to look at the confu\u2010 sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict"
    }
  },
  {
    "query": "What is data augmentation?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is data augmentation?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: What is data augmentation?\nRELATED EVIDENCE PATHS:\n- [Score: 0.470] To reduce overfitting, the authors used two regularization techniques we discussed in previous chapters: first they applied dropout (with a 50% dropout rate) during train\u2010 ing to the outputs of layers F8 and F9. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions. AlexNet also uses a competitive normalization step immediately after the ReLU step \u2192 It is often preferable to generate training instances on the fly during training rather than wasting storage space and network bandwidth. TensorFlow offers several image manipulation operations such as transposing (shifting), rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrast, saturation, and hue (see the API documentation for more details). This makes it easy to implement data aug\u2010 mentation for image datasets.\n- [Score: 0.219] To reduce overfitting, the authors used two regularization techniques we discussed in previous chapters: first they applied dropout (with a 50% dropout rate) during train\u2010 ing to the outputs of layers F8 and F9. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions. AlexNet also uses a competitive normalization step immediately after the ReLU step \u2192 7 \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,\u201d S. Ioffe and C. Szegedy (2015). Batch Normalization Although using He initialization along with ELU (or any variant of ReLU) can signifi\u2010 cantly reduce the vanishing/exploding gradients problems at the beginning of train\u2010 ing, it doesn\u2019t guarantee that they won\u2019t come back during training. In a 2015 paper,7 Sergey Ioffe and Christian Szegedy proposed a technique called \u2192 It is often preferable to generate training instances on the fly during training rather than wasting storage space and network bandwidth. TensorFlow offers several image manipulation operations such as transposing (shifting), rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrast, saturation, and hue (see the API documentation for more details). This makes it easy to implement data aug\u2010 mentation for image datasets.\n- [Score: 0.211] To reduce overfitting, the authors used two regularization techniques we discussed in previous chapters: first they applied dropout (with a 50% dropout rate) during train\u2010 ing to the outputs of layers F8 and F9. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions. AlexNet also uses a competitive normalization step immediately after the ReLU step \u2192 etc.), or alternatively you can just use an existing dataset, such as the flowers dataset or MIT\u2019s places dataset (requires registration, and it is huge). b. Write a preprocessing step that will resize and crop the image to 299 \u00d7 299, with some randomness for data augmentation. c. Using the pretrained Inception v3 model from the previous exercise, freeze all layers up to the bottleneck layer (i.e., the last layer before the output layer), \u2192 It is often preferable to generate training instances on the fly during training rather than wasting storage space and network bandwidth. TensorFlow offers several image manipulation operations such as transposing (shifting), rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrast, saturation, and hue (see the API documentation for more details). This makes it easy to implement data aug\u2010 mentation for image datasets.\n- [Score: 0.211] To reduce overfitting, the authors used two regularization techniques we discussed in previous chapters: first they applied dropout (with a 50% dropout rate) during train\u2010 ing to the outputs of layers F8 and F9. Second, they performed data augmentation by randomly shifting the training images by various offsets, flipping them horizontally, and changing the lighting conditions. AlexNet also uses a competitive normalization step immediately after the ReLU step \u2192 to load batches of images for training. If you want to skip this part, Tensor\u2010 Flow\u2019s model zoo contains tools to do just that. \u2022 Split the dataset into a training set and a test set. Train a deep denoising autoencoder on the full training set. \u2022 Check that the images are fairly well reconstructed, and visualize the low-level features. Visualize the images that most activate each neuron in the coding layer. \u2022 Build a classification deep neural network, reusing the lower layers of the \u2192 It is often preferable to generate training instances on the fly during training rather than wasting storage space and network bandwidth. TensorFlow offers several image manipulation operations such as transposing (shifting), rotating, resizing, flipping, and cropping, as well as adjusting the brightness, contrast, saturation, and hue (see the API documentation for more details). This makes it easy to implement data aug\u2010 mentation for image datasets."
    }
  },
  {
    "query": "What is early stopping?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is early stopping?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: What is early stopping?\nRELATED EVIDENCE PATHS:\n- [Score: 0.538] naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stop\u2010 ping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a \u201cbeautiful free lunch.\u201d \u2192 Early Stopping A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n- [Score: 0.295] naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stop\u2010 ping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a \u201cbeautiful free lunch.\u201d \u2192 tion. So if you immediately stop training when the validation error goes up, you may stop much too early, before the optimum is reached. A better option is to save the model at regular intervals, and when it has not improved for a long time (meaning it will probably never beat the record), you can revert to the best saved model. 7. Stochastic Gradient Descent has the fastest training iteration since it considers \u2192 Early Stopping A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n- [Score: 0.290] naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stop\u2010 ping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a \u201cbeautiful free lunch.\u201d \u2192 \u201cbeautiful free lunch.\u201d Figure 4-20. Early stopping regularization With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it may be hard to know whether you have reached the minimum or not. One solution is to stop only after the validation error has been above the minimum for some time (when you are confident that the model will not do any better), then roll back the model parameters to the point where the validation error was at a minimum. \u2192 Early Stopping A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set\n- [Score: 0.204] naturally goes down, and so does its prediction error on the validation set. However, after a while the validation error stops decreasing and actually starts to go back up. This indicates that the model has started to overfit the training data. With early stop\u2010 ping you just stop training as soon as the validation error reaches the minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a \u201cbeautiful free lunch.\u201d \u2192 them run long enough? 5. Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this? 6. Is it a good idea to stop Mini-batch Gradient Descent immediately when the vali\u2010 dation error goes up? 7. Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How \u2192 Early Stopping A very different way to regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the validation error reaches a minimum. This is called early stopping. Figure 4-20 shows a complex model (in this case a high-degree Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the algorithm learns and its prediction error (RMSE) on the training set"
    }
  },
  {
    "query": "What are embeddings used for?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What are embeddings used for?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What are embeddings used for?\nRELATED EVIDENCE PATHS:\n- [Score: 0.380] Embeddings are also useful for representing categorical attributes that can take on a large number of different values, especially when there are complex similarities between values. For example, con\u2010 sider professions, hobbies, dishes, species, brands, and so on. Natural Language Processing  |  411 \u2192 of the English sentence will be fed last to the encoder, which is useful because that\u2019s generally the first thing that the decoder needs to translate. Each word is initially represented by a simple integer identifier (e.g., 288 for the word \u201cmilk\u201d). Next, an embedding lookup returns the word embedding (as explained ear\u2010 lier, this is a dense, fairly low-dimensional vector). These word embeddings are what is actually fed to the encoder and the decoder."
    }
  },
  {
    "query": "Explain how k-nearest neighbors (KNN) works.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain how k-nearest neighbors (KNN) works.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: Explain how k-nearest neighbors (KNN) works.\nRELATED EVIDENCE PATHS:\n- [Score: 0.456] diction. This simple algorithm is called k-Nearest Neighbors regres\u2010 sion (in this example, k = 3). Replacing the Linear Regression model with k-Nearest Neighbors regression in the previous code is as simple as replacing this line: model = sklearn.linear_model.LinearRegression() with this one: model = sklearn.neighbors.KNeighborsRegressor(n_neighbors=3) Types of Machine Learning Systems  |  21 \u2192 from sklearn.manifold import LocallyLinearEmbedding lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10) X_reduced = lle.fit_transform(X) Figure 8-12. Unrolled Swiss roll using LLE Here\u2019s how LLE works: first, for each training instance x(i), the algorithm identifies its k closest neighbors (in the preceding code k = 10), then tries to reconstruct x(i) as a linear function of these neighbors. More specifically, it finds the weights wi,j such that"
    }
  },
  {
    "query": "What is the role of a validation set?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the role of a validation set?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: What is the role of a validation set?\nRELATED EVIDENCE PATHS:\n- [Score: 0.531] 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape \u2192 set, you select the model and hyperparameters that perform best on the validation set, and when you\u2019re happy with your model you run a single final test against the test set to get an estimate of the generalization error. To avoid \u201cwasting\u201d too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated\n- [Score: 0.318] 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape \u2192 1 If you draw a straight line between any two points on the curve, the line never crosses the curve. 17. A validation set is used to compare models. It makes it possible to select the best model and tune the hyperparameters. 18. If you tune hyperparameters using the test set, you risk overfitting the test set, and the generalization error you measure will be optimistic (you may launch a model that performs worse than you expect). \u2192 set, you select the model and hyperparameters that perform best on the validation set, and when you\u2019re happy with your model you run a single final test against the test set to get an estimate of the generalization error. To avoid \u201cwasting\u201d too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated\n- [Score: 0.204] 17. What is the purpose of a validation set? 18. What can go wrong if you tune hyperparameters using the test set? 19. What is cross-validation and why would you prefer it to a validation set? Solutions to these exercises are available in Appendix A. 32  |  Chapter 1: The Machine Learning Landscape \u2192 lucky on the training data). Possible solutions to overfitting are getting more data, simplifying the model (selecting a simpler algorithm, reducing the number of parameters or features used, or regularizing the model), or reducing the noise in the training data. 16. A test set is used to estimate the generalization error that a model will make on new instances, before the model is launched in production. 478  |  Appendix A: Exercise Solutions \u2192 set, you select the model and hyperparameters that perform best on the validation set, and when you\u2019re happy with your model you run a single final test against the test set to get an estimate of the generalization error. To avoid \u201cwasting\u201d too much training data in validation sets, a common technique is to use cross-validation: the training set is split into complementary subsets, and each model is trained against a different combination of these subsets and validated"
    }
  },
  {
    "query": "What is data leakage?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "message": "[!] No valid paths found. Try lowering prune_thresh or increasing max_hops."
    }
  },
  {
    "query": "What is a hyperparameter?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a hyperparameter?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 6,
      "prompt": "QUERY: What is a hyperparameter?\nRELATED EVIDENCE PATHS:\n- [Score: 0.471] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on.\n- [Score: 0.268] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 to be learned during training (instead of being a hyperparameter, it becomes a parameter that can be modified by backpropagation like any other parameter). This Vanishing/Exploding Gradients Problems  |  281 \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on.\n- [Score: 0.223] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 Figure 1-23. Regularization reduces the risk of overfitting The amount of regularization to apply during learning can be controlled by a hyper\u2010 parameter. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training. If you set the regularization hyper\u2010 parameter to a very large value, you will get an almost flat model (a slope close to \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on.\n- [Score: 0.208] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 number of layers, the number of neurons per layer, the type of activation function to use in each layer, the weight initialization logic, and much more. How do you know what combination of hyperparameters is the best for your task? Of course, you can use grid search with cross-validation to find the right hyperpara\u2010 meters, like you did in previous chapters, but since there are many hyperparameters to tune, and since training a neural network on a large dataset takes a lot of time, you \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on.\n- [Score: 0.203] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 \u2022 Inspection. All the estimator\u2019s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator\u2019s learned parameters are also accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_). \u2022 Nonproliferation of classes. Datasets are represented as NumPy arrays or SciPy sparse matrices, instead of homemade classes. Hyperparameters are just regular Python strings or numbers. \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on.\n- [Score: 0.200] instances and uses them to make predictions. 12. A model has one or more model parameters that determine what it will predict given a new instance (e.g., the slope of a linear model). A learning algorithm tries to find optimal values for these parameters such that the model generalizes well to new instances. A hyperparameter is a parameter of the learning algorithm itself, not of the model (e.g., the amount of regularization to apply). \u2192 found. Having a good sense of what each hyperparameter actually does can also help you search in the right part of the hyperparame\u2010 ter space. Adding Similarity Features Another technique to tackle nonlinear problems is to add features computed using a similarity function that measures how much each instance resembles a particular landmark. For example, let\u2019s take the one-dimensional dataset discussed earlier and \u2192 5 In Chapter 11 we discuss many techniques that introduce additional hyperparameters: type of weight initiali\u2010 zation, activation function hyperparameters (e.g., amount of leak in leaky ReLU), Gradient Clipping thres\u2010 hold, type of optimizer and its hyperparameters (e.g., the momentum hyperparameter when using a MomentumOptimizer), type of regularization for each layer, and the regularization hyperparameters (e.g., drop\u2010 out rate when using dropout) and so on."
    }
  },
  {
    "query": "Explain grid search.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain grid search.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: Explain grid search.\nRELATED EVIDENCE PATHS:\n- [Score: 0.503] Grid Search                                                                                                                 73 Randomized Search                                                                                                   75 Ensemble Methods                                                                                                     76 Analyze the Best Models and Their Errors                                                             76 \u2192 to automatically find the best way to handle outliers, missing fea\u2010 tures, feature selection, and more. Randomized Search The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combi\u2010\n- [Score: 0.245] Grid Search                                                                                                                 73 Randomized Search                                                                                                   75 Ensemble Methods                                                                                                     76 Analyze the Best Models and Their Errors                                                             76 \u2192 \u2022 If you let the randomized search run for, say, 1,000 iterations, this approach will explore 1,000 different values for each hyperparameter (instead of just a few val\u2010 ues per hyperparameter with the grid search approach). \u2022 You have more control over the computing budget you want to allocate to hyper\u2010 parameter search, simply by setting the number of iterations. Ensemble Methods Another way to fine-tune your system is to try to combine the models that perform \u2192 to automatically find the best way to handle outliers, missing fea\u2010 tures, feature selection, and more. Randomized Search The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combi\u2010\n- [Score: 0.224] Grid Search                                                                                                                 73 Randomized Search                                                                                                   75 Ensemble Methods                                                                                                     76 Analyze the Best Models and Their Errors                                                             76 \u2192 search over grid search. If training is very long, you may prefer a Bayesian optimization approach (e.g., using Gaussian process priors, as described by Jasper Snoek, Hugo Larochelle, and Ryan Adams).1 2. Try Ensemble methods. Combining your best models will often perform better than running them individually. 3. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error. \u2192 to automatically find the best way to handle outliers, missing fea\u2010 tures, feature selection, and more. Randomized Search The grid search approach is fine when you are exploring relatively few combinations, like in the previous example, but when the hyperparameter search space is large, it is often preferable to use RandomizedSearchCV instead. This class can be used in much the same way as the GridSearchCV class, but instead of trying out all possible combi\u2010"
    }
  },
  {
    "query": "Explain random search.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain random search.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: Explain random search.\nRELATED EVIDENCE PATHS:\n- [Score: 0.249] 12 You will often see people set the random seed to 42. This number has no special property, other than to be The Answer to the Ultimate Question of Life, the Universe, and Everything. 13 The location information is actually quite coarse, and as a result many districts will have the exact same ID, so they will end up in the same set (test or train). This introduces some unfortunate sampling bias. Well, this works, but it is not perfect: if you run the program again, it will generate a \u2192 Grid Search                                                                                                                 73 Randomized Search                                                                                                   75 Ensemble Methods                                                                                                     76 Analyze the Best Models and Their Errors                                                             76"
    }
  },
  {
    "query": "What is batch size?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is batch size?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 10,
      "prompt": "QUERY: What is batch size?\nRELATED EVIDENCE PATHS:\n- [Score: 0.506] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.248] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 ted layers because we want to apply the activation function after each batch normal\u2010 ization  layer.8  We  create  the  batch  normalization  layers  using  the 286  |  Chapter 11: Training Deep Neural Nets \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.229] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 and shifting the result using two new parameters per layer (one for scaling, the other for shifting). In other words, this operation lets the model learn the optimal scale and mean of the inputs for each layer. In order to zero-center and normalize the inputs, the algorithm needs to estimate the inputs\u2019 mean and standard deviation. It does so by evaluating the mean and standard deviation of the inputs over the current mini-batch (hence the name \u201cBatch Normal\u2010 \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.222] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 instance in the mini-batch (m is the number of instances in the mini-batch and nneurons is the number of neurons). \u2022 X(t) is an m \u00d7 ninputs matrix containing the inputs for all instances (ninputs is the number of input features). \u2022 Wx is an ninputs \u00d7 nneurons matrix containing the connection weights for the inputs of the current time step. \u2022 Wy is an nneurons \u00d7 nneurons matrix containing the connection weights for the out\u2010 puts of the previous time step. \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.219] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 deviation, so instead you simply use the whole training set\u2019s mean and standard devi\u2010 ation. These are typically efficiently computed during training using a moving aver\u2010 age. So, in total, four parameters are learned for each batch-normalized layer: \u03b3 (scale), \u03b2 (offset), \u03bc (mean), and \u03c3 (standard deviation). The authors demonstrated that this technique considerably improved all the deep neural networks they experimented with. The vanishing gradients problem was \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.217] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 T \u00b7 x t + Wy T \u00b7 y t \u22121 + b Just like for feedforward neural networks, we can compute a recurrent layer\u2019s output in one shot for a whole mini-batch by placing all the inputs at time step t in an input matrix X(t) (see Equation 14-2). Equation 14-2. Outputs of a layer of recurrent neurons for all instances in a mini- batch Y t = \u03d5 X t \u00b7 Wx + Y t \u22121 \u00b7 Wy + b = \u03d5 X t Y t \u22121 \u00b7 W + b with W = Wx Wy \u2022 Y(t) is an m \u00d7 nneurons matrix containing the layer\u2019s outputs at time step t for each \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.214] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 at both time steps for all neurons and all instances in the mini-batch: >>> print(Y0_val)  # output at t = 0 [[-0.0664006   0.96257669  0.68105787  0.70918542 -0.89821595]  # instance 0  [ 0.9977755  -0.71978885 -0.99657625  0.9673925  -0.99989718]  # instance 1  [ 0.99999774 -0.99898815 -0.99999893  0.99677622 -0.99999988]  # instance 2  [ 1.         -1.         -1.         -0.99818915  0.99950868]] # instance 3 >>> print(Y1_val)  # output at t = 1 \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.210] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 7 \u201cBatch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,\u201d S. Ioffe and C. Szegedy (2015). Batch Normalization Although using He initialization along with ELU (or any variant of ReLU) can signifi\u2010 cantly reduce the vanishing/exploding gradients problems at the beginning of train\u2010 ing, it doesn\u2019t guarantee that they won\u2019t come back during training. In a 2015 paper,7 Sergey Ioffe and Christian Szegedy proposed a technique called \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.206] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 The Normal Equation                                                                                              110 Computational Complexity                                                                                    112 Gradient Descent                                                                                                         113 Batch Gradient Descent                                                                                           116 \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks\n- [Score: 0.204] ization\u201d). The whole operation is summarized in Equation 11-3. Equation 11-3. Batch Normalization algorithm 1 . \u03bcB = 1 mB \u2211 i = 1 mB x i 2 . \u03c3B 2 = 1 mB \u2211 i = 1 mB x i \u2212\u03bcB 2 3 . x i = x i \u2212\u03bcB \u03c3B 2 + \ufffd 4 . z i = \u03b3x i + \u03b2 \u2022 \u03bcB is the empirical mean, evaluated over the whole mini-batch B. \u2022 \u03c3B is the empirical standard deviation, also evaluated over the whole mini-batch. \u2022 mB is the number of instances in the mini-batch. 284  |  Chapter 11: Training Deep Neural Nets \u2192 Batch Normalization (BN) to address the vanishing/exploding gradients problems, and more generally the problem that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change (which they call the Internal Covariate Shift problem). The technique consists of adding an operation in the model just before the activation function of each layer, simply zero-centering and normalizing the inputs, then scaling \u2192 to [batch_size, n_steps, n_outputs]. These operations are represented in Figure 14-10. 398  |  Chapter 14: Recurrent Neural Networks"
    }
  },
  {
    "query": "What is learning rate?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is learning rate?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 9,
      "prompt": "QUERY: What is learning rate?\nRELATED EVIDENCE PATHS:\n- [Score: 0.534] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.324] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 To find a good learning rate, you can use grid search (see Chapter 2). However, you may want to limit the number of iterations so that grid search can eliminate models that take too long to converge. You may wonder how to set the number of iterations. If it is too low, you will still be far away from the optimal solution when the algorithm stops, but if it is too high, you will waste time while the model parameters do not change anymore. A simple solu\u2010 \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.284] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 Learning Rate Scheduling Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge (as we discussed in Chapter 4). If you set it too low, training will eventually converge to the optimum, but it will take a very long time. If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down (unless you use an adaptive learning rate \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.245] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 the training rate, and now convergence is fast but the network\u2019s accuracy is sub\u2010 optimal), then you can try adding a learning schedule such as exponential decay. \u2022 If your training set is a bit too small, you can implement data augmentation. \u2022 If you need a sparse model, you can add some \u21131 regularization to the mix (and optionally zero out the tiny weights after training). If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with \u21131 reg\u2010 ularization. \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.239] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 this example, a MomentumOptimizer) using this decaying learning rate. Finally, we cre\u2010 ate the training operation by calling the optimizer\u2019s minimize() method; since we pass it the global_step variable, it will kindly take care of incrementing it. That\u2019s it! Since AdaGrad, RMSProp, and Adam optimization automatically reduce the learning rate during training, it is not necessary to add an extra learning schedule. For other \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.239] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 learning schedules (we briefly introduced this concept in Chapter 4), the most com\u2010 mon of which are: Predetermined piecewise constant learning rate For example, set the learning rate to \u03b70 = 0.1 at first, then to \u03b71 = 0.001 after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them. Performance scheduling Measure the validation error every N steps (just like for early stopping) and \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.220] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 than a bad instance\u2019s score by at least some margin. This is called max margin learn\u2010 ing. Faster Optimizers Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training (and reach a better solution): applying a good initiali\u2010 zation strategy for the connection weights, using a good activation function, using Batch Normalization, and reusing parts of a pretrained network. Another huge speed \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.204] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 learning rate \u03b7 comes into play:6 multiply the gradient vector by \u03b7 to determine the size of the downhill step (Equation 4-7). Equation 4-7. Gradient Descent step \u03b8 next step = \u03b8 \u2212\u03b7\u2207\u03b8 MSE \u03b8 Let\u2019s look at a quick implementation of this algorithm: eta = 0.1  # learning rate n_iterations = 1000 m = 100 theta = np.random.randn(2,1)  # random initialization for iteration in range(n_iterations):     gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)     theta = theta - eta * gradients Gradient Descent \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010\n- [Score: 0.203] ing curves. The ideal learning rate will learn quickly and converge to good solution. However, you can do better than a constant learning rate: if you start with a high learning rate and then reduce it once it stops making fast progress, you can reach a good solution faster than with the optimal constant learning rate. There are many dif\u2010 ferent strategies to reduce the learning rate during training. These strategies are called \u2192 attempting to decrease the cost function (e.g., the MSE), until the algorithm converges to a minimum (see Figure 4-3). Figure 4-3. Gradient Descent An important parameter in Gradient Descent is the size of the steps, determined by  the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time (see Figure 4-4). Gradient Descent  |  113 \u2192 optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle). If you have a limited computing budget, you may have to inter\u2010 rupt training before it has converged properly, yielding a suboptimal solution (see Figure 11-8). Figure 11-8. Learning curves for various learning rates \u03b7 You may be able to find a fairly good learning rate by training your network several times during just a few epochs using various learning rates and comparing the learn\u2010"
    }
  },
  {
    "query": "What is an optimizer?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is an optimizer?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is an optimizer?\nRELATED EVIDENCE PATHS:\n- [Score: 0.326] quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. This process is called simulated annealing, because it resembles the process of annealing in metallurgy where molten metal is slowly cooled down. The function that determines the learning rate at each iteration is called the learning schedule. If the learning rate is reduced too quickly, you may get \u2192 acceleration, not as a speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperpara\u2010 Faster Optimizers  |  297"
    }
  },
  {
    "query": "Describe the Adam optimizer.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Describe the Adam optimizer.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 8,
      "prompt": "QUERY: Describe the Adam optimizer.\nRELATED EVIDENCE PATHS:\n- [Score: 0.597] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.344] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 that adaptive optimization methods (i.e., AdaGrad, RMSProp and Adam optimization) can lead to solutions that generalize poorly on some datasets. So you may want to stick to Momentum optimiza\u2010 tion or Nesterov Accelerated Gradient for now, until researchers have a better understanding of this issue. All the optimization techniques discussed so far only rely on the first-order partial derivatives (Jacobians). The optimization literature contains amazing algorithms \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.315] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 Equation 11-8. Adam algorithm 1 . m \u03b21m \u22121 \u2212\u03b21 \u2207\u03b8J \u03b8 2 . s \u03b22s + 1 \u2212\u03b22 \u2207\u03b8J \u03b8 \u2297\u2207\u03b8J \u03b8 3 . m m 1 \u2212\u03b21 t 4 . s s 1 \u2212\u03b22 t 5 . \u03b8 \u03b8 + \u03b7 m \u2298 s + \ufffd \u2022 t represents the iteration number (starting at 1). If you just look at steps 1, 2, and 5, you will notice Adam\u2019s close similarity to both Momentum optimization and RMSProp. The only difference is that step 1 computes an exponentially decaying average rather than an exponentially decaying sum, but \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.247] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 ing decay hyperparameter \u03b22 is often initialized to 0.999. As earlier, the smoothing term \u03f5 is usually initialized to a tiny number such as 10\u20138. These are the default values for TensorFlow\u2019s AdamOptimizer class, so you can simply use: optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) In fact, since Adam is an adaptive learning rate algorithm (like AdaGrad and RMSProp), it requires less tuning of the learning rate hyperparameter \u03b7. You can \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.224] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 16 \u201cThe Marginal Value of Adaptive Gradient Methods in Machine Learning,\u201d A. C. Wilson et al. (2017). 17 \u201cPrimal-Dual Subgradient Methods for Convex Problems,\u201d Yurii Nesterov (2005). 18 \u201cAd Click Prediction: a View from the Trenches,\u201d H. McMahan et al. (2013). This book initially recommended using Adam optimization, because it was generally considered faster and better than other methods. However, a 2017 paper16 by Ashia C. Wilson et al. showed \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.219] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 this example, a MomentumOptimizer) using this decaying learning rate. Finally, we cre\u2010 ate the training operation by calling the optimizer\u2019s minimize() method; since we pass it the global_step variable, it will kindly take care of incrementing it. That\u2019s it! Since AdaGrad, RMSProp, and Adam optimization automatically reduce the learning rate during training, it is not necessary to add an extra learning schedule. For other \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.202] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 tion of the momentum (see Equation 11-5). The only difference from vanilla Momentum optimization is that the gradient is measured at \u03b8 + \u03b2m rather than at \u03b8. Equation 11-5. Nesterov Accelerated Gradient algorithm 1 . m \u03b2m \u2212\u03b7\u2207\u03b8J \u03b8 + \u03b2m 2 . \u03b8 \u03b8 + m This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of\n- [Score: 0.202] often use the default value \u03b7 = 0.001, making Adam even easier to use than Gradient Descent. Faster Optimizers  |  303 \u2192 13 This algorithm was created by Tijmen Tieleman and Geoffrey Hinton in 2012, and presented by Geoffrey Hinton in his Coursera class on neural networks (slides: http://goo.gl/RsQeis; video: https://goo.gl/XUbIyJ). Amusingly, since the authors have not written a paper to describe it, researchers often cite \u201cslide 29 in lecture 6\u201d in their papers. 14 \u201cAdam: A Method for Stochastic Optimization,\u201d D. Kingma, J. Ba (2015). \u2192 momentum=0.9, decay=0.9, epsilon=1e-10) Except on very simple problems, this optimizer almost always performs much better than AdaGrad. In fact, it was the preferred optimization algorithm of many research\u2010 ers until Adam optimization came around. Adam Optimization Adam,14 which stands for adaptive moment estimation, combines the ideas of Momen\u2010 tum optimization and RMSProp: just like Momentum optimization it keeps track of"
    }
  },
  {
    "query": "What is the difference between classification and clustering?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the difference between classification and clustering?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is the difference between classification and clustering?\nRELATED EVIDENCE PATHS:\n- [Score: 0.321] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).\n- [Score: 0.240] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 Figure 1-5. A labeled training set for supervised learning (e.g., spam classification) A typical supervised learning task is classification. The spam filter is a good example of this: it is trained with many example emails along with their class (spam or ham), and it must learn how to classify new emails. Another typical task is to predict a target numeric value, such as the price of a car, given a set of features (mileage, age, brand, etc.) called predictors. This sort of task is \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).\n- [Score: 0.210] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 and it is able to name everyone in every photo, which is useful for searching photos. Figure 1-11. Semisupervised learning Most semisupervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, deep belief networks (DBNs) are based on unsu\u2010 pervised components called restricted Boltzmann machines (RBMs) stacked on top of one another. RBMs are trained sequentially in an unsupervised manner, and then the \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).\n- [Score: 0.208] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 Let\u2019s look at each of these criteria a bit more closely. Supervised/Unsupervised Learning Machine Learning systems can be classified according to the amount and type of supervision they get during training. There are four major categories: supervised learning, unsupervised learning, semisupervised learning, and Reinforcement Learn\u2010 ing. Supervised learning In supervised learning, the training data you feed to the algorithm includes the desired solutions, called labels (Figure 1-5). \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11).\n- [Score: 0.206] changeably, though. Figure 1-6. Regression Note that some regression algorithms can be used for classification as well, and vice versa. For example, Logistic Regression is commonly used for classification, as it can output a value that corresponds to the probability of belonging to a given class (e.g., 20% chance of being spam). Here are some of the most important supervised learning algorithms (covered in this book): \u2022 k-Nearest Neighbors \u2022 Linear Regression \u2022 Logistic Regression \u2192 can feed many examples of each group to a classification algorithm (supervised learning), and it will classify all your customers into these groups. 8. Spam detection is a typical supervised learning problem: the algorithm is fed many emails along with their label (spam or not spam). 9. An online learning system can learn incrementally, as opposed to a batch learn\u2010 ing system. This makes it capable of adapting rapidly to both changing data and \u2192 4 That\u2019s when the system works perfectly. In practice it often creates a few clusters per person, and sometimes mixes up two people who look alike, so you need to provide a few labels per person and manually clean up some clusters. Semisupervised learning Some algorithms can deal with partially labeled training data, usually a lot of unla\u2010 beled data and a little bit of labeled data. This is called semisupervised learning (Figure 1-11)."
    }
  },
  {
    "query": "What is the elbow method in clustering?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the elbow method in clustering?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is the elbow method in clustering?\nRELATED EVIDENCE PATHS:\n- [Score: 0.322] techniques discussed in Chapter 8 is that all instances get mapped to a discrete number of points in the low-dimensional space (one point per neuron). When there are very few neurons, this techni\u2010 que is better described as clustering rather than dimensionality reduction. 528  |  Appendix E: Other Popular ANN Architectures \u2192 cluster specification, 328 clustering algorithms, 10 clusters, 327 coding space, 436 codings, 417 complementary slackness condition, 510 components_, 216 computational complexity, 112, 156, 174 compute_gradients(), 288, 455 concat(), 373 config.gpu_options, 322 ConfigProto, 321 confusion matrix, 86-88, 98-101 connectionism, 262 constrained optimization, 160, 509 Contrastive Divergence, 525 control dependencies, 327 532  |  Index"
    }
  },
  {
    "query": "What is a confusion matrix?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a confusion matrix?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is a confusion matrix?\nRELATED EVIDENCE PATHS:\n- [Score: 0.549] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.377] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 Now you are ready to get the confusion matrix using the confusion_matrix() func\u2010 tion. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred): >>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred) array([[53272,  1307],        [ 1077,  4344]]) Each row in a confusion matrix represents an actual class, while each column repre\u2010 sents a predicted class. The first row of this matrix considers non-5 images (the nega\u2010 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.362] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including preci\u2010 sion and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1307) 0.76871350203503808 >>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.80132816823464303 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.243] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 you want to find ways to improve it. One way to do this is to analyze the types of errors it makes. First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier: >>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) >>> conf_mx = confusion_matrix(y_train, y_train_pred) >>> conf_mx array([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4], \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.225] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others). Confusion Matrix A much better way to evaluate the performance of a classifier is to look at the confu\u2010 sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict"
    }
  },
  {
    "query": "What is meant by model interpretability?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is meant by model interpretability?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is meant by model interpretability?\nRELATED EVIDENCE PATHS:\n- [Score: 0.323] Model Interpretation: White Box Versus Black Box As you can see Decision Trees are fairly intuitive and their decisions are easy to inter\u2010 pret. Such models are often called white box models. In contrast, as we will see, Ran\u2010 dom Forests or neural networks are generally considered black box models. They make great predictions, and you can easily check the calculations that they performed to make these predictions; nevertheless, it is usually hard to explain in simple terms \u2192 model is a linear model, while for other datasets it is a neural network. There is no model that is a priori guaranteed to work better (hence the name of the theorem). The only way to know for sure which model is best is to evaluate them all. Since this is not possible, in practice you make some reasonable assumptions about the data and you evaluate only a few reasonable models. For example, for simple tasks you may evalu\u2010"
    }
  },
  {
    "query": "What is explainable AI (XAI)?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is explainable AI (XAI)?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 3,
      "prompt": "QUERY: What is explainable AI (XAI)?\nRELATED EVIDENCE PATHS:\n- [Score: 0.360] examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 detection algorithms, and genetic algorithms. My greatest hope is that this book will inspire you to build a wonderful ML applica\u2010 tion that will benefit all of us! What will it be? Aur\u00e9lien G\u00e9ron, November 26th, 2016 476  |  Chapter 16: Reinforcement Learning\n- [Score: 0.209] examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 detection algorithms, and genetic algorithms. My greatest hope is that this book will inspire you to build a wonderful ML applica\u2010 tion that will benefit all of us! What will it be? Aur\u00e9lien G\u00e9ron, November 26th, 2016 476  |  Chapter 16: Reinforcement Learning\n- [Score: 0.204] examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 including a discussion of Markov decision processes (MDP). We will use these techni\u2010 ques to train a model to balance a pole on a moving cart, and another to play Atari games. The same techniques can be used for a wide variety of tasks, from walking robots to self-driving cars. Learning to Optimize Rewards In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act \u2192 detection algorithms, and genetic algorithms. My greatest hope is that this book will inspire you to build a wonderful ML applica\u2010 tion that will benefit all of us! What will it be? Aur\u00e9lien G\u00e9ron, November 26th, 2016 476  |  Chapter 16: Reinforcement Learning"
    }
  },
  {
    "query": "What is a time series forecasting?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a time series forecasting?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is a time series forecasting?\nRELATED EVIDENCE PATHS:\n- [Score: 0.316] the future (well, up to a point, of course). They can analyze time series data such as stock prices, and tell you when to buy or sell. In autonomous driving systems, they can anticipate car trajectories and help avoid accidents. More generally, they can work on sequences of arbitrary lengths, rather than on fixed-sized inputs like all the nets we have discussed so far. For example, they can take sentences, documents, or audio \u2192 Input and Output Sequences An RNN can simultaneously take a sequence of inputs and produce a sequence of outputs (see Figure 14-4, top-left network). For example, this type of network is use\u2010 ful for predicting time series such as stock prices: you feed it the prices over the last N days, and it must output the prices shifted by one day into the future (i.e., from N \u2013 1 days ago to tomorrow). Alternatively, you could feed the network a sequence of inputs, and ignore all outputs"
    }
  },
  {
    "query": "Explain the difference between stationary and non-stationary time series.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "message": "[!] No valid paths found. Try lowering prune_thresh or increasing max_hops."
    }
  },
  {
    "query": "What is a Markov chain?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a Markov chain?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 4,
      "prompt": "QUERY: What is a Markov chain?\nRELATED EVIDENCE PATHS:\n- [Score: 0.515] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459\n- [Score: 0.297] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459\n- [Score: 0.296] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 Figure 16-7 shows an example of a Markov chain with four states. Suppose that the process starts in state s0, and there is a 70% chance that it will remain in that state at the next step. Eventually it is bound to leave that state and never come back since no other state points back to s0. If it goes to state s1, it will then most likely go to state s2 (90% probability), then immediately back to state s1 (with 100% probability). It may \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459\n- [Score: 0.239] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459"
    }
  },
  {
    "query": "What is a hidden Markov model?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a hidden Markov model?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 7,
      "prompt": "QUERY: What is a hidden Markov model?\nRELATED EVIDENCE PATHS:\n- [Score: 0.507] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.314] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 11 \u201cA Markovian Decision Process,\u201d R. Bellman (1957). Figure 16-7. Example of a Markov chain Markov decision processes were first described in the 1950s by Richard Bellman.11 They resemble Markov chains but with a twist: at each step, an agent can choose one of several possible actions, and the transition probabilities depend on the chosen action. Moreover, some state transitions return some reward (positive or negative), \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.243] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 alternate a number of times between these two states, but eventually it will fall into state s3 and remain there forever (this is a terminal state). Markov chains can have very different dynamics, and they are heavily used in thermodynamics, chemistry, statistics, and much more. Markov Decision Processes  |  459 \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.231] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 each transition at least once to know the rewards, and it must experience them multi\u2010 ple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.216] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 rithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 16-5). Equation 16-5. Q-Learning algorithm Qk + 1 s, a 1 \u2212\u03b1 Qk s, a + \u03b1 r + \u03b3 . max a\u2032 Qk s\u2032, a\u2032 For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the rewards it expects to get later. Since the target policy would act optimally, we take the maximum \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.216] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 and the agent\u2019s goal is to find a policy that will maximize rewards over time. For example, the MDP represented in Figure 16-8 has three states and up to three possible discrete actions at each step. If it starts in state s0, the agent can choose between actions a0, a1, or a2. If it chooses action a1, it just remains in state s0 with cer\u2010 tainty, and without any reward. It can thus decide to stay there forever if it wants. But \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463\n- [Score: 0.208] Markov Decision Processes In the early 20th century, the mathematician Andrey Markov studied stochastic pro\u2010 cesses with no memory, called Markov chains. Such a process has a fixed number of states, and it randomly evolves from one state to another at each step. The probability for it to evolve from a state s to a state s\u2032 is fixed, and it depends only on the pair (s,s\u2032), not on past states (the system has no memory). \u2192 Figure 16-8. Example of a Markov decision process Bellman found a way to estimate the optimal state value of any state s, noted V*(s), which is the sum of all discounted future rewards the agent can expect on average after it reaches a state s, assuming it acts optimally. He showed that if the agent acts optimally, then the Bellman Optimality Equation applies (see Equation 16-1). This recursive equation says that if the agent acts optimally, then the optimal value of the \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463"
    }
  },
  {
    "query": "What is a probabilistic graphical model?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a probabilistic graphical model?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is a probabilistic graphical model?\nRELATED EVIDENCE PATHS:\n- [Score: 0.294] represents the points where the model estimates a 50% probability: this is the model\u2019s decision boundary. Note that it is a linear boundary.17 Each parallel line represents the 140  |  Chapter 4: Training Models \u2192 representing the ML model and the computations required to train it. The execution phase generally runs a loop that evaluates a training step repeatedly (for example, one step per mini-batch), gradually improving the model parameters. We will go through an example shortly. Managing Graphs Any node you create is automatically added to the default graph: >>> x1 = tf.Variable(1) >>> x1.graph is tf.get_default_graph() True"
    }
  },
  {
    "query": "What is reinforcement learning reward function?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is reinforcement learning reward function?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 33,
      "prompt": "QUERY: What is reinforcement learning reward function?\nRELATED EVIDENCE PATHS:\n- [Score: 0.627] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.379] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Figure 1-12. Reinforcement Learning For example, many robots implement Reinforcement Learning algorithms to learn how to walk. DeepMind\u2019s AlphaGo program is also a good example of Reinforcement Learning: it made the headlines in May 2017 when it beat the world champion Ke Jie at the game of Go. It learned its winning policy by analyzing millions of games, and then playing many games against itself. Note that learning was turned off during the \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.370] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 in the data and use them to make predictions. In Reinforcement Learning, the goal is to find a good policy. \u2022 Unlike in supervised learning, the agent is not explicitly given the \u201cright\u201d answer. It must learn by trial and error. \u2022 Unlike in unsupervised learning, there is a form of supervision, through rewards. We do not tell the agent how to perform the task, but we do tell it when it is making progress or when it is failing. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.363] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 including a discussion of Markov decision processes (MDP). We will use these techni\u2010 ques to train a model to balance a pole on a moving cart, and another to play Atari games. The same techniques can be used for a wide variety of tasks, from walking robots to self-driving cars. Learning to Optimize Rewards In Reinforcement Learning, a software agent makes observations and takes actions within an environment, and in return it receives rewards. Its objective is to learn to act \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.352] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 reward over time. A policy defines what action the agent should choose when it is in a given situation. Types of Machine Learning Systems  |  13 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.351] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Go player, (d) thermostat, (e) automatic trader5 Note that there may not be any positive rewards at all; for example, the agent may move around in a maze, getting a negative reward at every time step, so it better find the exit as quickly as possible! There are many other examples of tasks where Rein\u2010 forcement Learning is well suited, such as self-driving cars, placing ads on a web page, or controlling where an image classification system should focus its attention. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.340] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 chess, instead of giving it a reward only when it wins the game, we could give it a reward every time it captures one of the opponent\u2019s pieces. 6. An agent can often remain in the same region of its environment for a while, so all of its experiences will be very similar for that period of time. This can intro\u2010 duce some bias in the learning algorithm. It may tune its policy for this region of the environment, but it will not perform well as soon as it moves out of this \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.322] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 \u2022 A Reinforcement Learning agent needs to find the right balance between exploring the environment, looking for new ways of getting rewards, and exploiting sources of rewards that it already knows. In contrast, supervised and unsupervised learning systems generally don\u2019t need to worry about explora\u2010 tion; they just feed on the training data they are given. \u2022 In supervised and unsupervised learning, training instances are typically inde\u2010 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.303] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 while the agent follows a different policy. Q-Learning is a good example of such an algorithm. In contrast, an on-policy algorithm learns the value of the policy that the agent actually executes, including both exploration and exploitation. For the solutions to exercises 8, 9, and 10, please see the Jupyter notebooks available at https://github.com/ageron/handson-ml. 502  |  Appendix A: Exercise Solutions \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.302] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 directly try to optimize the policy to increase rewards, the algorithms we will look at now are less direct: the agent learns to estimate the expected sum of discounted future rewards for each state, or the expected sum of discounted future rewards for each action in each state, then uses this knowledge to decide how to act. To understand these algorithms, we must first introduce Markov decision processes (MDP). Markov Decision Processes \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.292] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 in a way that will maximize its expected long-term rewards. If you don\u2019t mind a bit of anthropomorphism, you can think of positive rewards as pleasure, and negative rewards as pain (the term \u201creward\u201d is a bit misleading in this case). In short, the agent acts in the environment and learns by trial and error to maximize its pleasure and minimize its pain. This is quite a broad setting, which can apply to a wide variety of tasks. Here are a few examples (see Figure 16-1): \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.280] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 6 It is often better to give the poor performers a slight chance of survival, to preserve some diversity in the \u201cgene pool.\u201d Policy Search The algorithm used by the software agent to determine its actions is called its policy.  For example, the policy could be a neural network taking observations as inputs and outputting the action to take (see Figure 16-2). Figure 16-2. Reinforcement Learning using a neural network policy \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.277] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Figure 16-3. Four points in policy space and the agent\u2019s corresponding behavior Yet another approach is to use optimization techniques, by evaluating the gradients of the rewards with regards to the policy parameters, then tweaking these parameters by following the gradient toward higher rewards (gradient ascent). This approach is called policy gradients (PG), which we will discuss in more detail later in this chapter. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.267] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Evaluating Actions: The Credit Assignment Problem If we knew what the best action was at each step, we could train the neural network as usual, by minimizing the cross entropy between the estimated probability and the tar\u2010 get probability. It would just be regular supervised learning. However, in Reinforce\u2010 ment Learning the only guidance the agent gets is through rewards, and rewards are typically sparse and delayed. For example, if the agent manages to balance the pole \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.265] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 examples (see Figure 16-1): a. The agent can be the program controlling a walking robot. In this case, the envi\u2010 ronment is the real world, the agent observes the environment through a set of sensors such as cameras and touch sensors, and its actions consist of sending sig\u2010 nals to activate motors. It may be programmed to get positive rewards whenever it approaches the target destination, and negative rewards whenever it wastes time, goes in the wrong direction, or falls down. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.256] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 tive observations will be very correlated. In some cases a replay memory is used to ensure that the training algorithm gets fairly independent observa\u2010 tions. 2. Here are a few possible applications of Reinforcement Learning, other than those mentioned in Chapter 16: Music personalization The environment is a user\u2019s personalized web radio. The agent is the software deciding what song to play next for that user. Its possible actions are to play \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.249] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Markov decision processes, but the agent initially has no idea what the transition probabilities are (it does not know T(s, a, s\u2032)), and it does not know what the rewards are going to be either (it does not know R(s, a, s\u2032)). It must experience each state and Temporal Difference Learning and Q-Learning  |  463 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.242] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 region. To solve this problem, you can use a replay memory; instead of using only the most immediate experiences for learning, the agent will learn based on a buffer of its past experiences, recent and not so recent (perhaps this is why we dream at night: to replay our experiences of the day and better learn from them?). 7. An off-policy RL algorithm learns the value of the optimal policy (i.e., the sum of discounted rewards that can be expected for each state if the agent acts optimally) \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.240] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 wildest dreams. In this chapter we will first explain what Reinforcement Learning is and what it is good at, and then we will present two of the most important techniques in deep Reinforcement Learning: policy gradients and deep Q-networks (DQN), 443 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.229] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Researchers try to find algorithms that work well even when the agent initially knows nothing about the environment. However, unless you are writing a paper, you should inject as much prior knowledge as possible into the agent, as it will speed up training dramatically. For example, you could add negative rewards propor\u2010 tional to the distance from the center of the screen, and to the pole\u2019s angle. Also, if you already have a reasonably good policy (e.g., \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.224] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 each transition at least once to know the rewards, and it must experience them multi\u2010 ple times if it is to have a reasonable estimate of the transition probabilities. The Temporal Difference Learning (TD Learning) algorithm is very similar to the Value Iteration algorithm, but tweaked to take into account the fact that the agent has only partial knowledge of the MDP. In general we assume that the agent initially knows only the possible states and actions, and nothing more. The agent uses an \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.224] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 4. To measure the performance of a Reinforcement Learning agent, you can simply sum up the rewards it gets. In a simulated environment, you can run many epi\u2010 sodes and look at the total rewards it gets on average (and possibly look at the min, max, standard deviation, and so on). 5. The credit assignment problem is the fact that when a Reinforcement Learning agent receives a reward, it has no direct way of knowing which of its previous \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.222] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 Introduction to OpenAI Gym One of the challenges of Reinforcement Learning is that in order to train an agent, you first need to have a working environment. If you want to program an agent that will learn to play an Atari game, you will need an Atari game simulator. If you want to program a walking robot, then the environment is the real world and you can directly train your robot in that environment, but this has its limits: if the robot falls off a cliff, \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.220] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 books). Note that there are actually two ways the agent can lose the game: either the pole can tilt too much, or the cart can go completely off the screen. With 250 training iterations, the policy learns to balance the pole quite well, but it is not yet good enough at avoiding going off the screen. A few hundred more training iterations will fix that. 458  |  Chapter 16: Reinforcement Learning \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.219] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 actions contributed to this reward. It typically occurs when there is a large delay between an action and the resulting rewards (e.g., during a game of Atari\u2019s Pong, there may be a few dozen time steps between the moment the agent hits the ball and the moment it wins the point). One way to alleviate it is to provide the agent with shorter-term rewards, when possible. This usually requires prior knowledge about the task. For example, if we want to build an agent that will learn to play \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.217] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 any case, RL still requires quite a lot of patience and tweaking, but the end result is very exciting. Exercises 1. How would you define Reinforcement Learning? How is it different from regular supervised or unsupervised learning? 2. Can you think of three possible applications of RL that were not mentioned in this chapter? For each of them, what is the environment? What is the agent? What are possible actions? What are the rewards? \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.210] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 problems that can be tackled iteratively (in this case finding the action that maximizes the average reward plus the discounted next state value). Knowing the optimal state values can be useful, in particular to evaluate a policy, but it does not tell the agent explicitly what to do. Luckily, Bellman found a very similar algorithm to estimate the optimal state-action values, generally called Q-Values. The optimal Q-Value of the state-action pair (s,a), noted Q*(s,a), is the sum of discounted \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.208] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 rithm to the situation where the transition probabilities and the rewards are initially unknown (see Equation 16-5). Equation 16-5. Q-Learning algorithm Qk + 1 s, a 1 \u2212\u03b1 Qk s, a + \u03b1 r + \u03b3 . max a\u2032 Qk s\u2032, a\u2032 For each state-action pair (s, a), this algorithm keeps track of a running average of the rewards r the agent gets upon leaving the state s with action a, plus the rewards it expects to get later. Since the target policy would act optimally, we take the maximum \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.206] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 exploration policy\u2014for example, a purely random policy\u2014to explore the MDP, and as it progresses the TD Learning algorithm updates the estimates of the state values based on the transitions and rewards that are actually observed (see Equation 16-4). Equation 16-4. TD Learning algorithm Vk + 1 s 1 \u2212\u03b1 Vk s + \u03b1 r + \u03b3 . Vk s\u2032 \u2022 \u03b1 is the learning rate (e.g., 0.01). TD Learning has many similarities with Stochastic Gradient Descent, in particular the fact that it handles one sample at a time. \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.205] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 approach lets the agent find the right balance between exploring new actions and exploiting the actions that are known to work well. Here\u2019s an analogy: suppose you go to a restaurant for the first time, and all the dishes look equally appealing so you ran\u2010 domly pick one. If it turns out to be good, you can increase the probability to order it next time, but you shouldn\u2019t increase that probability up to 100%, or else you will \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.204] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 goo.gl/yTsH6X. CHAPTER 16 Reinforcement Learning Reinforcement Learning (RL) is one of the most exciting fields of Machine Learning today, and also one of the oldest. It has been around since the 1950s, producing many interesting applications over the years,1 in particular in games (e.g., TD-Gammon, a Backgammon playing program) and in machine control, but seldom making the headline news. But a revolution took place in 2013 when researchers from an English \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.203] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 so on. They would get positive rewards for each product delivered on time, and negative rewards for late deliveries. 3. When estimating the value of an action, Reinforcement Learning algorithms typ\u2010 ically sum all the rewards that this action led to, giving more weight to immediate rewards, and less weight to later rewards (considering that an action has more influence on the near future than on the distant future). To model this, a discount \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most\n- [Score: 0.200] example, it can be a smart thermostat, getting rewards whenever it is close to the target temperature and saves energy, and negative rewards when humans need to tweak the temperature, so the agent must learn to anticipate human needs. e. The agent can observe stock market prices and decide how much to buy or sell every second. Rewards are obviously the monetary gains and losses. 444  |  Chapter 16: Reinforcement Learning \u2192 3. What is the discount rate? Can the optimal policy change if you modify the dis\u2010 count rate? 4. How do you measure the performance of a Reinforcement Learning agent? 5. What is the credit assignment problem? When does it occur? How can you allevi\u2010 ate it? Exercises  |  475 \u2192 whole system is fine-tuned using supervised learning techniques. Reinforcement Learning Reinforcement Learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards, as in Figure 1-12). It must then learn by itself what is the best strategy, called a policy, to get the most"
    }
  },
  {
    "query": "Explain the difference between batch and online learning.",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "Explain the difference between batch and online learning.",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 9,
      "prompt": "QUERY: Explain the difference between batch and online learning.\nRELATED EVIDENCE PATHS:\n- [Score: 0.483] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.322] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 step is fast and cheap, so the system can learn about new data on the fly, as it arrives (see Figure 1-13). Figure 1-13. Online learning Online learning is great for systems that receive data as a continuous flow (e.g., stock prices) and need to adapt to change rapidly or autonomously. It is also a good option Types of Machine Learning Systems  |  15 \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.243] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 resources (e.g., a smartphone application or a rover on Mars), then carrying around large amounts of training data and taking up a lot of resources to train for hours every day is a showstopper. Fortunately, a better option in all these cases is to use algorithms that are capable of learning incrementally. Online learning In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.237] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 if you have limited computing resources: once an online learning system has learned about new data instances, it does not need them anymore, so you can discard them (unless you want to be able to roll back to a previous state and \u201creplay\u201d the data). This can save a huge amount of space. Online learning algorithms can also be used to train systems on huge datasets that cannot fit in one machine\u2019s main memory (this is called out-of-core learning). The \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.227] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 \u2022 Getting insights about complex problems and large amounts of data. Types of Machine Learning Systems There are so many different types of Machine Learning systems that it is useful to classify them in broad categories based on: \u2022 Whether or not they are trained with human supervision (supervised, unsuper\u2010 vised, semisupervised, and Reinforcement Learning) \u2022 Whether or not they can learn incrementally on the fly (online versus batch learning) \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.224] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 that online learning means learning incrementally, typically as new instances arrive). For linear SVM classifiers, one method is to use Gradient Descent (e.g., using SGDClassifier) to minimize the cost function in Equation 5-13, which is derived from the primal problem. Unfortunately it converges much more slowly than the methods based on QP. Equation 5-13. Linear SVM classifier cost function J w, b = 1 2wT \u00b7 w + C \u2211 i = 1 m max 0, 1 \u2212t i wT \u00b7 x i + b \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.213] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 Why Use Machine Learning?                                                                                         4 Types of Machine Learning Systems                                                                             7 Supervised/Unsupervised Learning                                                                           8 Batch and Online Learning                                                                                       14 \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.211] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 A big challenge with online learning is that if bad data is fed to the system, the sys\u2010 tem\u2019s performance will gradually decline. If we are talking about a live system, your clients will notice. For example, bad data could come from a malfunctioning sensor on a robot, or from someone spamming a search engine to try to rank high in search 16  |  Chapter 1: The Machine Learning Landscape \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing\n- [Score: 0.209] algorithm loads part of the data, runs a training step on that data, and repeats the process until it has run on all of the data (see Figure 1-14). This whole process is usually done offline (i.e., not on the live sys\u2010 tem), so online learning can be a confusing name. Think of it as incremental learning. Figure 1-14. Using online learning to handle huge datasets One important parameter of online learning systems is how fast they should adapt to \u2192 autonomous systems, and of training on very large quantities of data. 10. Out-of-core algorithms can handle vast quantities of data that cannot fit in a computer\u2019s main memory. An out-of-core learning algorithm chops the data into mini-batches and uses online learning techniques to learn from these mini- batches. 11. An instance-based learning system learns the training data by heart; then, when given a new instance, it uses a similarity measure to find the most similar learned \u2192 games against the champion; AlphaGo was just applying the policy it had learned. Batch and Online Learning Another criterion used to classify Machine Learning systems is whether or not the system can learn incrementally from a stream of incoming data. Batch learning In batch learning, the system is incapable of learning incrementally: it must be trained using all the available data. This will generally take a lot of time and computing"
    }
  },
  {
    "query": "What is a precision-recall tradeoff?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a precision-recall tradeoff?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is a precision-recall tradeoff?\nRELATED EVIDENCE PATHS:\n- [Score: 0.608] Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop\u2014for example, at around 60% recall. But of course the choice depends on your project. So let\u2019s suppose you decide to aim for 90% precision. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 70,000. To make \u2192 happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the thres\u2010 hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5.\n- [Score: 0.355] Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop\u2014for example, at around 60% recall. But of course the choice depends on your project. So let\u2019s suppose you decide to aim for 90% precision. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 70,000. To make \u2192 plt.xlabel(\"Threshold\")     plt.legend(loc=\"center left\")     plt.ylim([0, 1]) plot_precision_recall_vs_threshold(precisions, recalls, thresholds) plt.show() Figure 3-4. Precision and recall versus the decision threshold You may wonder why the precision curve is bumpier than the recall curve in Figure 3-4. The reason is that precision may sometimes go down when you raise the threshold (although in general it will go up). To understand why, look back at Figure 3-3 and notice what \u2192 happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the thres\u2010 hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5.\n- [Score: 0.298] Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop\u2014for example, at around 60% recall. But of course the choice depends on your project. So let\u2019s suppose you decide to aim for 90% precision. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 70,000. To make \u2192 from sklearn.metrics import precision_recall_curve precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores) Finally, you can plot precision and recall as functions of the threshold value using Matplotlib (Figure 3-4): def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):     plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")     plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")     plt.xlabel(\"Threshold\") \u2192 happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the thres\u2010 hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5.\n- [Score: 0.284] Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop\u2014for example, at around 60% recall. But of course the choice depends on your project. So let\u2019s suppose you decide to aim for 90% precision. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 70,000. To make \u2192 true positives (actual 5s) on the right of that threshold, and one false positive (actually a 6). Therefore, with that threshold, the precision is 80% (4 out of 5). But out of 6 actual 5s, the classifier only detects 4, so the recall is 67% (4 out of 6). Now if you raise the threshold (move it to the arrow on the right), the false positive (the 6) becomes a true negative, thereby increasing precision (up to 100% in this case), but \u2192 happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the thres\u2010 hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5.\n- [Score: 0.231] Figure 3-5. Precision versus recall You can see that precision really starts to fall sharply around 80% recall. You will probably want to select a precision/recall tradeoff just before that drop\u2014for example, at around 60% recall. But of course the choice depends on your project. So let\u2019s suppose you decide to aim for 90% precision. You look up the first plot (zooming in a bit) and find that you need to use a threshold of about 70,000. To make \u2192 one true positive becomes a false negative, decreasing recall down to 50%. Conversely, lowering the threshold increases recall and reduces precision. Performance Measures  |  89 \u2192 happens when you start from the central threshold and move it just one digit to the right: precision goes from 4/5 (80%) down to 3/4 (75%). On the other hand, recall can only go down when the thres\u2010 hold is increased, which explains why its curve looks smooth. Now you can simply select the threshold value that gives you the best precision/recall tradeoff for your task. Another way to select a good precision/recall tradeoff is to plot precision directly against recall, as shown in Figure 3-5."
    }
  },
  {
    "query": "What is the difference between sample and population?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is the difference between sample and population?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 1,
      "prompt": "QUERY: What is the difference between sample and population?\nRELATED EVIDENCE PATHS:\n- [Score: 0.398] is not, you run the risk of introducing a significant sampling bias. When a survey company decides to call 1,000 people to ask them a few questions, they don\u2019t just pick 1,000 people randomly in a phone booth. They try to ensure that these 1,000 people are representative of the whole population. For example, the US population is com\u2010 posed of 51.3% female and 48.7% male, so a well-conducted survey in the US would \u2192 samples can be nonrepresentative if the sampling method is flawed. This is called sampling bias. A Famous Example of Sampling Bias Perhaps the most famous example of sampling bias happened during the US presi\u2010 dential election in 1936, which pitted Landon against Roosevelt: the Literary Digest conducted a very large poll, sending mail to about 10 million people. It got 2.4 million answers, and predicted with high confidence that Landon would get 57% of the votes. 24  |"
    }
  },
  {
    "query": "What is hypothesis testing?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "message": "[!] No valid paths found. Try lowering prune_thresh or increasing max_hops."
    }
  },
  {
    "query": "What is p-value?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "message": "[!] No valid paths found. Try lowering prune_thresh or increasing max_hops."
    }
  },
  {
    "query": "What is bootstrapping?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is bootstrapping?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is bootstrapping?\nRELATED EVIDENCE PATHS:\n- [Score: 0.529] predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be \u2192 the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\n- [Score: 0.293] predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be \u2192 sampled several times for the same predictor. This sampling and training process is represented in Figure 7-4. Figure 7-4. Pasting/bagging training set sampling and training Once all predictors are trained, the ensemble can make a prediction for a new instance by simply aggregating the predictions of all predictors. The aggregation function is typically the statistical mode (i.e., the most frequent prediction, just like a \u2192 the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\n- [Score: 0.236] predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be \u2192 1 \u201cBagging Predictors,\u201d L. Breiman (1996). 2 In statistics, resampling with replacement is called bootstrapping. 3 \u201cPasting small votes for classification in large databases and on-line,\u201d L. Breiman (1999). 4 Bias and variance were introduced in Chapter 4. Bagging and Pasting One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every \u2192 the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\n- [Score: 0.226] predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be \u2192 ples=1.0) but sampling features (i.e., bootstrap_features=True and/or max_fea tures smaller than 1.0) is called the Random Subspaces method.8 190  |  Chapter 7: Ensemble Learning and Random Forests \u2192 the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam\n- [Score: 0.213] predictor, but to train them on different random subsets of the training set. When sampling is performed with replacement, this method is called bagging1 (short for bootstrap aggregating2). When sampling is performed without replacement, it is called pasting.3 In other words, both bagging and pasting allow training instances to be sampled sev\u2010 eral times across multiple predictors, but only bagging allows training instances to be \u2192 erally preferred. However, if you have spare time and CPU power you can use cross- validation to evaluate both bagging and pasting and select the one that works best. Out-of-Bag Evaluation With bagging, some instances may be sampled several times for any given predictor, while others may not be sampled at all. By default a BaggingClassifier samples m training instances with replacement (bootstrap=True), where m is the size of the \u2192 the same way as max_samples and bootstrap, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. This is particularly useful when you are dealing with high-dimensional inputs (such as images). Sampling both training instances and features is called the Random Patches method.7 Keeping all training instances (i.e., bootstrap=False and max_sam"
    }
  },
  {
    "query": "What is a confusion matrix?",
    "top_k": 2,
    "max_hop": 2,
    "response": {
      "query": "What is a confusion matrix?",
      "top_k": 2,
      "max_hop": 2,
      "num_paths": 5,
      "prompt": "QUERY: What is a confusion matrix?\nRELATED EVIDENCE PATHS:\n- [Score: 0.549] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.377] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 Now you are ready to get the confusion matrix using the confusion_matrix() func\u2010 tion. Just pass it the target classes (y_train_5) and the predicted classes (y_train_pred): >>> from sklearn.metrics import confusion_matrix >>> confusion_matrix(y_train_5, y_train_pred) array([[53272,  1307],        [ 1077,  4344]]) Each row in a confusion matrix represents an actual class, while each column repre\u2010 sents a predicted class. The first row of this matrix considers non-5 images (the nega\u2010 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.362] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 If you are confused about the confusion matrix, Figure 3-2 may help. Figure 3-2. An illustrated confusion matrix Precision and Recall Scikit-Learn provides several functions to compute classifier metrics, including preci\u2010 sion and recall: >>> from sklearn.metrics import precision_score, recall_score >>> precision_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1307) 0.76871350203503808 >>> recall_score(y_train_5, y_train_pred) # == 4344 / (4344 + 1077) 0.80132816823464303 \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.243] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 you want to find ways to improve it. One way to do this is to analyze the types of errors it makes. First, you can look at the confusion matrix. You need to make predictions using the cross_val_predict() function, then call the confusion_matrix() function, just like you did earlier: >>> y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=3) >>> conf_mx = confusion_matrix(y_train, y_train_pred) >>> conf_mx array([[5725,    3,   24,    9,   10,   49,   50,   10,   39,    4], \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict\n- [Score: 0.225] negatives, so its confusion matrix would have nonzero values only on its main diago\u2010 nal (top left to bottom right): >>> confusion_matrix(y_train_5, y_train_perfect_predictions) array([[54579,    0],        [    0, 5421]]) The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the accuracy of the positive pre\u2010 dictions; this is called the precision of the classifier (Equation 3-1). Equation 3-1. Precision \u2192 for classifiers, especially when you are dealing with skewed datasets (i.e., when some classes are much more frequent than others). Confusion Matrix A much better way to evaluate the performance of a classifier is to look at the confu\u2010 sion matrix. The general idea is to count the number of times instances of class A are classified as class B. For example, to know the number of times the classifier confused images of 5s with 3s, you would look in the 5th row and 3rd column of the confusion \u2192 matrix. To compute the confusion matrix, you first need to have a set of predictions, so they can be compared to the actual targets. You could make predictions on the test set, but let\u2019s keep it untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the cross_val_predict() function: from sklearn.model_selection import cross_val_predict"
    }
  }
]